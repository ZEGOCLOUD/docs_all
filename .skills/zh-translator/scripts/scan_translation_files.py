#!/usr/bin/env python3
"""
æ‰«ææŒ‡å®šç›®å½•ä¸‹æ‰€æœ‰éœ€è¦ç¿»è¯‘çš„æ–‡ä»¶ï¼Œå¹¶ç”Ÿæˆç¿»è¯‘è®¡åˆ’
æ”¯æŒ Markdown å’Œ YAML æ–‡ä»¶ï¼Œè‡ªåŠ¨è¯†åˆ« API æ–‡ä»¶ï¼ˆmdx + yaml å¯¹ï¼‰
"""

import sys
import json
from pathlib import Path
from typing import List, Dict, Any
from collections import defaultdict


def scan_directory(directory: str, file_types: List[str] = None) -> List[Dict[str, Any]]:
    """
    é€’å½’æ‰«æç›®å½•ä¸‹æ‰€æœ‰æŒ‡å®šç±»å‹çš„æ–‡ä»¶

    Args:
        directory: è¦æ‰«æçš„ç›®å½•è·¯å¾„
        file_types: è¦åŒ…å«çš„æ–‡ä»¶æ‰©å±•ååˆ—è¡¨ï¼Œå¦‚ ['.md', '.yaml', '.mdx']

    Returns:
        list: æ–‡ä»¶ä¿¡æ¯åˆ—è¡¨
    """
    if file_types is None:
        file_types = ['.md', '.yaml', '.mdx']

    dir_path = Path(directory)
    if not dir_path.exists():
        print(f"Error: Directory '{directory}' does not exist", file=sys.stderr)
        return []

    files = []
    for file_path in dir_path.rglob('*'):
        if file_path.is_file() and file_path.suffix in file_types:
            # åªå¤„ç†ä¸­æ–‡æ–‡æ¡£ï¼ˆè·¯å¾„ä¸­åŒ…å« /zh/ çš„ï¼‰
            if '/zh/' in str(file_path):
                # è®¡ç®—æ–‡ä»¶è¡Œæ•°
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        line_count = sum(1 for _ in f)
                except Exception:
                    line_count = 0

                files.append({
                    'path': str(file_path),
                    'relative_path': str(file_path.relative_to(dir_path)),
                    'suffix': file_path.suffix,
                    'line_count': line_count,
                    'size_kb': file_path.stat().st_size / 1024
                })

    return files


def identify_api_files(files: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
    """
    è¯†åˆ« API æ–‡ä»¶å¯¹ï¼ˆåŒå mdx å’Œ yamlï¼‰

    è§„åˆ™ï¼š
    - åœ¨ server å¹³å°çš„ api-reference ç›®å½•ä¸‹
    - å­˜åœ¨åŒåçš„ .mdx å’Œ .yaml æ–‡ä»¶
    - è¿™ç±»æ–‡ä»¶åªéœ€è¦ç¿»è¯‘ .yamlï¼Œ.mdx æ˜¯è‡ªåŠ¨ç”Ÿæˆçš„

    Returns:
        dict: {
            'api_only_yaml': [...],  # åªéœ€è¦ç¿»è¯‘ yaml çš„æ–‡ä»¶
            'regular_files': [...]   # æ™®é€šæ–‡ä»¶
        }
    """
    api_pairs = defaultdict(list)
    result = {
        'api_only_yaml': [],
        'regular_files': []
    }

    # æŒ‰ basename åˆ†ç»„
    for file_info in files:
        file_path = file_info['path']
        file_path_obj = Path(file_path)

        # æ£€æŸ¥æ˜¯å¦åœ¨ server å¹³å°çš„ api-reference ç›®å½•ä¸‹
        if '/server/' in file_path and '/api-reference/' in file_path:
            basename = file_path_obj.stem
            api_pairs[basename].append(file_info)
        else:
            result['regular_files'].append(file_info)

    # å¯¹äºæœ‰åŒå mdx å’Œ yaml çš„æ–‡ä»¶ï¼Œåªä¿ç•™ yaml
    for basename, file_list in api_pairs.items():
        yaml_files = [f for f in file_list if f['suffix'] == '.yaml']
        mdx_files = [f for f in file_list if f['suffix'] == '.mdx']

        if yaml_files and mdx_files:
            # æœ‰æˆå¯¹çš„æ–‡ä»¶ï¼Œåªä¿ç•™ yaml
            result['api_only_yaml'].extend(yaml_files)
        elif yaml_files:
            # åªæœ‰ yamlï¼ŒæŒ‰æ™®é€šæ–‡ä»¶å¤„ç†
            result['regular_files'].extend(yaml_files)
        else:
            # åªæœ‰ mdx æˆ–å…¶ä»–æƒ…å†µï¼ŒæŒ‰æ™®é€šæ–‡ä»¶å¤„ç†
            result['regular_files'].extend(file_list)

    return result


def create_translation_batches(files: List[Dict[str, Any]], lines_per_batch: int = 500) -> List[Dict[str, Any]]:
    """
    å°†æ–‡ä»¶ç»„ç»‡æˆç¿»è¯‘æ‰¹æ¬¡

    ç­–ç•¥ï¼š
    - å°æ–‡ä»¶ï¼ˆ< 100 è¡Œï¼‰ï¼šæŒ‰æ–‡ä»¶åˆ†ç»„ï¼Œæ¯æ‰¹å¤šä¸ªæ–‡ä»¶
    - ä¸­ç­‰æ–‡ä»¶ï¼ˆ100-500 è¡Œï¼‰ï¼šæ¯æ‰¹ 1-2 ä¸ªæ–‡ä»¶
    - å¤§æ–‡ä»¶ï¼ˆ> 500 è¡Œï¼‰ï¼šå•ç‹¬æˆæ‰¹ï¼Œå¯èƒ½éœ€è¦åˆ†æ®µç¿»è¯‘

    Args:
        files: æ–‡ä»¶åˆ—è¡¨
        lines_per_batch: æ¯æ‰¹å¤§è‡´çš„ç›®æ ‡è¡Œæ•°

    Returns:
        list: æ‰¹æ¬¡åˆ—è¡¨
    """
    # æŒ‰æ–‡ä»¶å¤§å°æ’åºï¼ˆå°æ–‡ä»¶ä¼˜å…ˆï¼‰
    sorted_files = sorted(files, key=lambda x: x['line_count'])

    batches = []
    current_batch = {
        'files': [],
        'total_lines': 0,
        'file_count': 0
    }

    for file_info in sorted_files:
        line_count = file_info['line_count']
        file_size_category = 'small' if line_count < 100 else 'medium' if line_count < 500 else 'large'

        file_info['size_category'] = file_size_category

        # å¤§æ–‡ä»¶ï¼ˆ> 500 è¡Œï¼‰å•ç‹¬æˆæ‰¹
        if file_size_category == 'large':
            # ä¿å­˜å½“å‰æ‰¹æ¬¡
            if current_batch['files']:
                batches.append(current_batch)
                current_batch = {'files': [], 'total_lines': 0, 'file_count': 0}

            # å¤§æ–‡ä»¶å•ç‹¬æˆæ‰¹
            batches.append({
                'files': [file_info],
                'total_lines': line_count,
                'file_count': 1,
                'needs_segmentation': True
            })
        elif file_size_category == 'medium':
            # ä¸­ç­‰æ–‡ä»¶ï¼Œæ£€æŸ¥å½“å‰æ‰¹æ¬¡
            if current_batch['file_count'] >= 2 or current_batch['total_lines'] + line_count > lines_per_batch:
                batches.append(current_batch)
                current_batch = {'files': [], 'total_lines': 0, 'file_count': 0}

            current_batch['files'].append(file_info)
            current_batch['total_lines'] += line_count
            current_batch['file_count'] += 1
        else:
            # å°æ–‡ä»¶ï¼Œå¯ä»¥å¤šä¸ªä¸€æ‰¹
            if current_batch['total_lines'] + line_count > lines_per_batch * 1.5:
                batches.append(current_batch)
                current_batch = {'files': [], 'total_lines': 0, 'file_count': 0}

            current_batch['files'].append(file_info)
            current_batch['total_lines'] += line_count
            current_batch['file_count'] += 1

    # ä¿å­˜æœ€åä¸€ä¸ªæ‰¹æ¬¡
    if current_batch['files']:
        batches.append(current_batch)

    return batches


def print_summary(result: Dict[str, Any]):
    """æ‰“å°æ‰«æç»“æœæ‘˜è¦"""
    print("\n" + "="*70)
    print("ğŸ“Š ç¿»è¯‘æ‰«æç»“æœ")
    print("="*70)

    # API æ–‡ä»¶ç»Ÿè®¡
    api_files = result['api_only_yaml']
    print(f"\nğŸ”§ API æ–‡ä»¶ï¼ˆåªéœ€ç¿»è¯‘ YAMLï¼ŒMDX è‡ªåŠ¨ç”Ÿæˆï¼‰ï¼š{len(api_files)} ä¸ª")
    if api_files:
        api_lines = sum(f['line_count'] for f in api_files)
        print(f"   æ€»è¡Œæ•°ï¼š{api_lines} è¡Œ")

    # æ™®é€šæ–‡ä»¶ç»Ÿè®¡
    regular_files = result['regular_files']
    md_files = [f for f in regular_files if f['suffix'] == '.md']
    yaml_files = [f for f in regular_files if f['suffix'] == '.yaml']

    print(f"\nğŸ“„ Markdown æ–‡ä»¶ï¼š{len(md_files)} ä¸ª")
    if md_files:
        md_lines = sum(f['line_count'] for f in md_files)
        print(f"   æ€»è¡Œæ•°ï¼š{md_lines} è¡Œ")

    print(f"\nğŸ“‹ YAML æ–‡ä»¶ï¼ˆé APIï¼‰ï¼š{len(yaml_files)} ä¸ª")
    if yaml_files:
        yaml_lines = sum(f['line_count'] for f in yaml_files)
        print(f"   æ€»è¡Œæ•°ï¼š{yaml_lines} è¡Œ")

    # æ‰¹æ¬¡ç»Ÿè®¡
    batches = result['batches']
    print(f"\nğŸ“¦ ç¿»è¯‘æ‰¹æ¬¡ï¼š{len(batches)} æ‰¹")
    print(f"   æ€»æ–‡ä»¶æ•°ï¼š{result['total_files']}")
    print(f"   æ€»è¡Œæ•°ï¼š{result['total_lines']}")
    print(f"   å¹³å‡æ¯æ‰¹ï¼š{result['total_lines'] // len(batches) if batches else 0} è¡Œ")

    # å¤§æ–‡ä»¶æç¤º
    large_files = [f for f in result['all_files'] if f.get('size_category') == 'large']
    if large_files:
        print(f"\nâš ï¸  å¤§æ–‡ä»¶ï¼ˆ>500 è¡Œï¼‰ï¼š{len(large_files)} ä¸ª")
        for f in large_files[:5]:  # åªæ˜¾ç¤ºå‰ 5 ä¸ª
            print(f"   - {f['relative_path']} ({f['line_count']} è¡Œ)")
        if len(large_files) > 5:
            print(f"   ... è¿˜æœ‰ {len(large_files) - 5} ä¸ªå¤§æ–‡ä»¶")


def print_batch_plan(batches: List[Dict[str, Any]], show_details: bool = True):
    """æ‰“å°ç¿»è¯‘æ‰¹æ¬¡è®¡åˆ’"""
    print("\n" + "="*70)
    print("ğŸ“‹ ç¿»è¯‘æ‰¹æ¬¡è®¡åˆ’")
    print("="*70)

    for i, batch in enumerate(batches, 1):
        print(f"\næ‰¹æ¬¡ {i}/{len(batches)} ({batch['total_lines']} è¡Œ, {batch['file_count']} ä¸ªæ–‡ä»¶)")

        if show_details or batch.get('needs_segmentation'):
            for j, file_info in enumerate(batch['files'], 1):
                flag = ""
                if file_info.get('size_category') == 'large':
                    flag = " [å¤§æ–‡ä»¶-éœ€åˆ†æ®µ]"
                elif file_info.get('size_category') == 'medium':
                    flag = " [ä¸­æ–‡ä»¶]"

                print(f"  {j}. {file_info['relative_path']}{flag}")
                print(f"     â†’ {file_info['line_count']} è¡Œ, {file_info['size_kb']:.1f} KB")
        else:
            # ç®€ç•¥æ˜¾ç¤º
            file_names = [f['relative_path'].split('/')[-1] for f in batch['files']]
            print(f"  æ–‡ä»¶ï¼š{', '.join(file_names)}")

        if batch.get('needs_segmentation'):
            print(f"  âš ï¸  æ­¤æ‰¹æ¬¡åŒ…å«å¤§æ–‡ä»¶ï¼Œç¿»è¯‘æ—¶å¯èƒ½éœ€è¦åˆ†æ®µå¤„ç†")


def main():
    if len(sys.argv) < 2:
        print("Usage: scan_translation_files.py <directory> [lines_per_batch]", file=sys.stderr)
        print("Example: scan_translation_files.py docs/zh/product 500", file=sys.stderr)
        print("Example: scan_translation_files.py docs/zh/server/api-reference", file=sys.stderr)
        sys.exit(1)

    directory = sys.argv[1]
    lines_per_batch = int(sys.argv[2]) if len(sys.argv) > 2 else 500

    print(f"ğŸ” æ‰«æç›®å½•ï¼š{directory}")
    print(f"âš™ï¸  æ¯æ‰¹ç›®æ ‡è¡Œæ•°ï¼š{lines_per_batch}")

    # æ‰«ææ–‡ä»¶
    files = scan_directory(directory)

    if not files:
        print("\nâŒ æœªæ‰¾åˆ°ä»»ä½•éœ€è¦ç¿»è¯‘çš„æ–‡ä»¶")
        print("æç¤ºï¼šç¡®ä¿ç›®å½•è·¯å¾„æ­£ç¡®ï¼Œä¸”åŒ…å« /zh/ å­ç›®å½•çš„æ–‡ä»¶")
        sys.exit(0)

    # è¯†åˆ« API æ–‡ä»¶
    categorized = identify_api_files(files)

    # åˆå¹¶æ‰€æœ‰éœ€è¦ç¿»è¯‘çš„æ–‡ä»¶
    all_files = categorized['api_only_yaml'] + categorized['regular_files']

    # åˆ›å»ºç¿»è¯‘æ‰¹æ¬¡
    batches = create_translation_batches(all_files, lines_per_batch)

    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    total_lines = sum(f['line_count'] for f in all_files)

    # æ„å»ºç»“æœ
    result = {
        'directory': directory,
        'total_files': len(all_files),
        'total_lines': total_lines,
        'api_only_yaml': categorized['api_only_yaml'],
        'regular_files': categorized['regular_files'],
        'all_files': all_files,
        'batches': batches,
        'lines_per_batch': lines_per_batch
    }

    # æ‰“å°ç»“æœ
    print_summary(result)
    print_batch_plan(batches, show_details=True)

    # è¾“å‡º JSON æ ¼å¼ä¾›è„šæœ¬ä½¿ç”¨
    print("\n" + "="*70)
    print("ğŸ“„ JSON è¾“å‡ºï¼ˆä¾›åç»­å¤„ç†ï¼‰")
    print("="*70)
    print(json.dumps(result, indent=2, ensure_ascii=False))


if __name__ == '__main__':
    main()
