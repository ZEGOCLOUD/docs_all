#!/usr/bin/env python3
"""
æ‰¹é‡ç¿»è¯‘æ–°äº§å“æ–‡æ¡£çš„ä¸»ç¼–æ’è„šæœ¬

æ•´åˆæ‰€æœ‰æ­¥éª¤ï¼š
1. æ‰«æå’Œåˆ†ç±»æ–‡æ¡£
2. å‡†å¤‡ç›®æ ‡ç›®å½•
3. é¢„å¤„ç†å…¨å¤ç”¨æ–‡æ¡£
4. ç”Ÿæˆç¿»è¯‘æ‰¹æ¬¡è®¡åˆ’
5. è¾“å‡ºè¿›åº¦è®°å½•æ–‡ä»¶æ¨¡æ¿
"""

import sys
import json
from pathlib import Path
from datetime import datetime


def calculate_target_directory(source_dir: str) -> str:
    """
    è®¡ç®—ç›®æ ‡ç›®å½•è·¯å¾„ï¼ˆå°† /zh/ æ›¿æ¢ä¸º /en/ï¼‰

    Args:
        source_dir: æºç›®å½•è·¯å¾„

    Returns:
        str: ç›®æ ‡ç›®å½•è·¯å¾„
    """
    return source_dir.replace('/zh/', '/en/')


def prepare_target_directory(source_dir: str, target_dir: str) -> dict:
    """
    å‡†å¤‡ç›®æ ‡ç›®å½•

    å¦‚æœç›®æ ‡ç›®å½•ä¸å­˜åœ¨ï¼Œåˆ™æ‹·è´æºç›®å½•åˆ°ç›®æ ‡ç›®å½•
    å¦‚æœç›®æ ‡ç›®å½•å·²å­˜åœ¨ï¼Œåˆ™ç›´æ¥ä½¿ç”¨

    Args:
        source_dir: æºç›®å½•è·¯å¾„
        target_dir: ç›®æ ‡ç›®å½•è·¯å¾„

    Returns:
        dict: å‡†å¤‡ç»“æœ
    """
    source_path = Path(source_dir)
    target_path = Path(target_dir)

    result = {
        'source_dir': source_dir,
        'target_dir': target_dir,
        'existed': target_path.exists(),
        'copied': False
    }

    if target_path.exists():
        print(f"âœ… ç›®æ ‡ç›®å½•å·²å­˜åœ¨ï¼š{target_dir}")
    else:
        print(f"\nğŸ“ ç›®æ ‡ç›®å½•ä¸å­˜åœ¨ï¼Œå¼€å§‹æ‹·è´...")
        print(f"   æºç›®å½•ï¼š{source_dir}")
        print(f"   ç›®æ ‡ç›®å½•ï¼š{target_dir}")

        # è¿™é‡Œåªæ˜¯ç”Ÿæˆå‘½ä»¤ï¼Œå®é™…æ‹·è´ç”±ç”¨æˆ·æ‰§è¡Œ
        copy_command = f"cp -r '{source_dir}' '{target_dir}'"
        print(f"\nğŸ’¡ è¯·æ‰§è¡Œä»¥ä¸‹å‘½ä»¤æ‹·è´ç›®å½•ï¼š")
        print(f"   {copy_command}")
        print(f"\nâš ï¸  æ‹·è´å®Œæˆåè¯·é‡æ–°è¿è¡Œæ­¤è„šæœ¬")
        sys.exit(0)

    return result


def create_progress_file(scan_result: dict, prep_result: dict, preprocess_results: list = None) -> dict:
    """
    åˆ›å»ºè¿›åº¦è®°å½•æ–‡ä»¶

    Args:
        scan_result: æ‰«æç»“æœ
        prep_result: å‡†å¤‡ç»“æœ
        preprocess_results: é¢„å¤„ç†ç»“æœï¼ˆå¯é€‰ï¼‰

    Returns:
        dict: è¿›åº¦è®°å½•
    """
    now = datetime.utcnow().isoformat() + 'Z'

    # æå–æ‰¹æ¬¡ä¿¡æ¯
    batches = scan_result.get('batches', [])

    # æ„å»ºè·³è¿‡çš„æ–‡ä»¶åˆ—è¡¨
    skipped_files = []

    # API æ–‡æ¡£
    for file_info in scan_result.get('files', []):
        if file_info.get('has_doctype_api', False):
            skipped_files.append({
                'path': file_info['path'],
                'reason': 'docType: API',
                'line_count': file_info['line_count']
            })

    # YAML ç”Ÿæˆçš„ MDX
    # è¿™äº›ä¿¡æ¯åœ¨ scan_result ä¸­æ²¡æœ‰ç›´æ¥æ ‡è®°ï¼Œéœ€è¦ä» yaml_pairs ä¸­æå–
    # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…ä½¿ç”¨æ—¶å¯ä»¥ä» scan_result ä¸­æå–

    # å…¨å¤ç”¨æ–‡æ¡£ï¼ˆå·²è§£å†³çš„ï¼‰
    if preprocess_results:
        for result in preprocess_results:
            if result['status'] == 'resolved':
                skipped_files.append({
                    'path': result['source'],
                    'reason': 'reuse_doc_resolved',
                    'line_count': result.get('line_count', 0),
                    'resolved_to': result.get('resolved_to')
                })

    progress = {
        'source_directory': scan_result['directory'],
        'target_directory': prep_result['target_dir'],
        'started_at': now,
        'last_updated': now,
        'status': 'in_progress',
        'total_files': scan_result['summary']['total_files'],
        'completed_files': 0,
        'skipped_files': len(skipped_files),
        'total_lines': scan_result['summary']['total_lines'],
        'translated_lines': 0,
        'current_batch': 1,
        'total_batches': len(batches),
        'scan_summary': scan_result['summary'],
        'batches': [],
        'skipped_files': skipped_files,
        'failed_files': []
    }

    # æ·»åŠ æ‰¹æ¬¡ä¿¡æ¯
    for batch in batches:
        progress['batches'].append({
            'batch_number': batch['batch_number'],
            'status': 'pending',
            'file_count': batch['file_count'],
            'total_lines': batch['total_lines'],
            'files': [
                {
                    'source': f['path'],
                    'target': f['target_path'],
                    'lines': f['line_count'],
                    'status': 'pending'
                }
                for f in batch['files']
            ]
        })

    return progress


def save_progress_file(progress: dict, target_dir: str):
    """
    ä¿å­˜è¿›åº¦æ–‡ä»¶åˆ°ç›®æ ‡ç›®å½•

    Args:
        progress: è¿›åº¦è®°å½•
        target_dir: ç›®æ ‡ç›®å½•
    """
    progress_file_path = Path(target_dir) / '.translation-progress.json'

    with open(progress_file_path, 'w', encoding='utf-8') as f:
        json.dump(progress, f, indent=2, ensure_ascii=False)

    print(f"\nâœ… è¿›åº¦æ–‡ä»¶å·²åˆ›å»ºï¼š{progress_file_path}")


def main():
    if len(sys.argv) < 2:
        print("Usage: batch_translate.py <source_directory>", file=sys.stderr)
        print("Example: batch_translate.py core_products/real-time-voice-video/zh/flutter", file=sys.stderr)
        sys.exit(1)

    source_dir = sys.argv[1]
    target_dir = calculate_target_directory(source_dir)

    print("="*70)
    print("ğŸš€ æ‰¹é‡ç¿»è¯‘æ–°äº§å“æ–‡æ¡£")
    print("="*70)

    # ç¬¬ä¸€æ­¥ï¼šå‡†å¤‡ç›®æ ‡ç›®å½•
    print("\nğŸ“ ç¬¬ä¸€æ­¥ï¼šå‡†å¤‡ç›®æ ‡ç›®å½•")
    prep_result = prepare_target_directory(source_dir, target_dir)

    # ç¬¬äºŒæ­¥ï¼šæ‰«ææ–‡æ¡£ï¼ˆéœ€è¦è°ƒç”¨ scan_batch_translation.pyï¼‰
    print("\nğŸ” ç¬¬äºŒæ­¥ï¼šæ‰«ææ–‡æ¡£")
    print("ğŸ’¡ è¯·å…ˆè¿è¡Œæ‰«æè„šæœ¬ç”Ÿæˆæ‰«æç»“æœï¼š")
    print(f"   python3 .claude/skills/trans-batch/scripts/scan_batch_translation.py {source_dir}")
    print(f"   å°†è¾“å‡ºä¿å­˜ä¸º scan_result.json")
    print("\nâš ï¸  æ‰«æå®Œæˆåè¯·é‡æ–°è¿è¡Œæ­¤è„šæœ¬ï¼Œå¹¶æŒ‡å®šæ‰«æç»“æœæ–‡ä»¶ï¼š")
    print(f"   python3 .claude/skills/trans-batch/scripts/batch_translate.py {source_dir} scan_result.json")

    if len(sys.argv) < 3:
        sys.exit(0)

    # å¦‚æœæä¾›äº†æ‰«æç»“æœæ–‡ä»¶ï¼Œç»§ç»­å¤„ç†
    scan_result_file = sys.argv[2]

    try:
        with open(scan_result_file, 'r', encoding='utf-8') as f:
            scan_result = json.load(f)
    except Exception as e:
        print(f"âŒ é”™è¯¯ï¼šæ— æ³•è¯»å–æ‰«æç»“æœæ–‡ä»¶ï¼š{e}", file=sys.stderr)
        sys.exit(1)

    print(f"âœ… å·²åŠ è½½æ‰«æç»“æœï¼š{scan_result_file}")

    # ç¬¬ä¸‰æ­¥ï¼šé¢„å¤„ç†å…¨å¤ç”¨æ–‡æ¡£
    print("\nğŸ”„ ç¬¬ä¸‰æ­¥ï¼šé¢„å¤„ç†å…¨å¤ç”¨æ–‡æ¡£")
    reuse_docs = [f for f in scan_result.get('files', []) if f.get('is_reuse_doc', False)]

    if reuse_docs:
        print(f"å‘ç° {len(reuse_docs)} ä¸ªå…¨å¤ç”¨æ–‡æ¡£")
        print("ğŸ’¡ è¯·è¿è¡Œé¢„å¤„ç†è„šæœ¬ï¼š")
        print(f"   python3 .claude/skills/trans-batch/scripts/preprocess_reuse_docs.py {scan_result_file}")
        print(f"   å°†è¾“å‡ºä¿å­˜ä¸º preprocess_result.json")
        print("\nâš ï¸  é¢„å¤„ç†å®Œæˆåè¯·é‡æ–°è¿è¡Œæ­¤è„šæœ¬ï¼Œå¹¶æŒ‡å®šé¢„å¤„ç†ç»“æœæ–‡ä»¶ï¼š")
        print(f"   python3 .claude/skills/trans-batch/scripts/batch_translate.py {source_dir} {scan_result_file} preprocess_result.json")

        if len(sys.argv) < 4:
            sys.exit(0)

        preprocess_result_file = sys.argv[3]

        try:
            with open(preprocess_result_file, 'r', encoding='utf-8') as f:
                preprocess_results = json.load(f)
        except Exception as e:
            print(f"âŒ é”™è¯¯ï¼šæ— æ³•è¯»å–é¢„å¤„ç†ç»“æœæ–‡ä»¶ï¼š{e}", file=sys.stderr)
            sys.exit(1)

        print(f"âœ… å·²åŠ è½½é¢„å¤„ç†ç»“æœï¼š{preprocess_result_file}")
    else:
        print("âœ… æ²¡æœ‰éœ€è¦é¢„å¤„ç†çš„å…¨å¤ç”¨æ–‡æ¡£")
        preprocess_results = None

    # ç¬¬å››æ­¥ï¼šåˆ›å»ºè¿›åº¦æ–‡ä»¶
    print("\nğŸ“Š ç¬¬å››æ­¥ï¼šåˆ›å»ºè¿›åº¦è®°å½•æ–‡ä»¶")
    progress = create_progress_file(scan_result, prep_result, preprocess_results)
    save_progress_file(progress, target_dir)

    # ç¬¬äº”æ­¥ï¼šè¾“å‡ºç¿»è¯‘è®¡åˆ’
    print("\nğŸ“‹ ç¬¬äº”æ­¥ï¼šç¿»è¯‘æ‰¹æ¬¡è®¡åˆ’")
    print("="*70)

    batches = scan_result.get('batches', [])
    for i, batch in enumerate(batches, 1):
        print(f"\næ‰¹æ¬¡ {batch['batch_number']}/{len(batches)} ({batch['total_lines']} è¡Œ, {batch['file_count']} ä¸ªæ–‡ä»¶)")
        for j, file_info in enumerate(batch['files'], 1):
            size_flag = ""
            if file_info.get('size_category') == 'large':
                size_flag = " [å¤§æ–‡ä»¶]"
            elif file_info.get('size_category') == 'medium':
                size_flag = " [ä¸­ç­‰]"

            print(f"  {j}. {file_info['relative_path']}{size_flag}")
            print(f"     â†’ {file_info['target_path']}")
            print(f"     {file_info['line_count']} è¡Œ")

        if batch.get('needs_segmentation'):
            print(f"  âš ï¸  æ­¤æ‰¹æ¬¡åŒ…å«è¶…å¤§æ–‡ä»¶ï¼Œç¿»è¯‘æ—¶éœ€è¦åˆ†æ®µå¤„ç†ï¼ˆæ¯æ®µä¸è¶…è¿‡ 2000 è¡Œï¼‰")

    # è¾“å‡ºæ€»ç»“
    print("\n" + "="*70)
    print("ğŸ“Š å‡†å¤‡å·¥ä½œå®Œæˆ")
    print("="*70)
    print(f"æºç›®å½•ï¼š{source_dir}")
    print(f"ç›®æ ‡ç›®å½•ï¼š{target_dir}")
    print(f"æ€»æ–‡ä»¶æ•°ï¼š{scan_result['summary']['total_files']}")
    print(f"æ€»è¡Œæ•°ï¼š{scan_result['summary']['total_lines']}")
    print(f"æ€»æ‰¹æ¬¡æ•°ï¼š{len(batches)}")
    print(f"\nè·³è¿‡æ–‡ä»¶æ•°ï¼š{progress['skipped_files']}")
    print(f"   - API æ–‡æ¡£ï¼š{scan_result['summary']['skipped_api_files']}")
    print(f"   - YAML ç”Ÿæˆ MDXï¼š{scan_result['summary']['skipped_mdx_files']}")
    if preprocess_results:
        resolved_count = sum(1 for r in preprocess_results if r['status'] == 'resolved')
        print(f"   - å…¨å¤ç”¨æ–‡æ¡£ï¼ˆå·²è§£å†³ï¼‰ï¼š{resolved_count}")

    print(f"\nâœ… å‡†å¤‡å·¥ä½œå®Œæˆï¼å¯ä»¥å¼€å§‹é€æ‰¹ç¿»è¯‘äº†")
    print(f"ğŸ“ è¿›åº¦æ–‡ä»¶ï¼š{Path(target_dir) / '.translation-progress.json'}")
    print(f"\nğŸ’¡ æç¤ºï¼š")
    print(f"   1. åŠ è½½æœ¯è¯­å¯¹ç…§è¡¨ï¼š.translate/common-terminology.csv")
    print(f"   2. å¼€å§‹ç¿»è¯‘ç¬¬ä¸€æ‰¹ï¼Œæ¯æ‰¹å®Œæˆåæ›´æ–°è¿›åº¦æ–‡ä»¶")
    print(f"   3. å¤§æ–‡ä»¶éœ€è¦åˆ†æ®µç¿»è¯‘ï¼ˆæ¯æ®µä¸è¶…è¿‡ 2000 è¡Œï¼‰")


if __name__ == '__main__':
    main()
