#!/usr/bin/env python3
"""
æ‰«æå¾…ç¿»è¯‘çš„ä¸­æ–‡ç›®å½•ï¼Œæ™ºèƒ½åˆ†ç±»æ–‡æ¡£å¹¶ç”Ÿæˆç¿»è¯‘è®¡åˆ’

åˆ†ç±»è§„åˆ™ï¼š
1. API æ–‡æ¡£ï¼šdocType ä¸º API çš„ MD æ–‡æ¡£ï¼ˆè‡ªåŠ¨è·³è¿‡ï¼‰
2. YAML ç”Ÿæˆçš„ MDXï¼šåŒä¸€ç›®å½•ä¸‹å­˜åœ¨åŒå mdx å’Œ yaml æ–‡ä»¶ï¼ˆåªç¿»è¯‘ yamlï¼‰
3. å…¨å¤ç”¨æ–‡æ¡£ï¼šé™¤äº† frontmatter å¤–åªæœ‰ä¸¤è¡Œéç©ºå†…å®¹ï¼ˆä¸€è¡Œ import Contentï¼Œä¸€è¡Œ <Contentï¼‰
4. æ™®é€šæ–‡æ¡£ï¼šæŒ‰è¡Œæ•°åˆ†ç±»ï¼ˆ<50è¡Œå°æ–‡ä»¶ï¼Œ50-300è¡Œä¸­ç­‰æ–‡ä»¶ï¼Œ>300è¡Œå¤§æ–‡ä»¶ï¼‰
"""

import sys
import json
import re
from pathlib import Path
from typing import List, Dict, Any, Tuple
from collections import defaultdict


def is_reuse_doc(file_path: Path) -> bool:
    """
    åˆ¤æ–­æ˜¯å¦ä¸ºå…¨å¤ç”¨æ–‡æ¡£

    å…¨å¤ç”¨æ–‡æ¡£ç‰¹å¾ï¼š
    - é™¤äº† frontmatter å¤–åªæœ‰ä¸¤è¡Œéç©ºå†…å®¹
    - ä¸€è¡Œä»¥ import Content from å¼€å¤´
    - å¦ä¸€è¡Œä»¥ <Content å¼€å¤´
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()

        # è·³è¿‡ frontmatterï¼ˆåœ¨ --- ä¹‹é—´çš„å†…å®¹ï¼‰
        frontmatter_end = -1
        if lines[0].strip() == '---':
            for i, line in enumerate(lines[1:], 1):
                if line.strip() == '---':
                    frontmatter_end = i
                    break

        # æå– frontmatter ä¹‹åçš„å†…å®¹
        content_lines = lines[frontmatter_end + 1:] if frontmatter_end > 0 else lines

        # è¿‡æ»¤ç©ºè¡Œå’Œçº¯ç©ºæ ¼è¡Œ
        non_empty_lines = [line for line in content_lines if line.strip()]

        # æ£€æŸ¥æ˜¯å¦åªæœ‰ä¸¤è¡Œéç©ºå†…å®¹
        if len(non_empty_lines) != 2:
            return False

        # æ£€æŸ¥ç¬¬ä¸€è¡Œæ˜¯å¦åŒ…å« import Content from
        line1 = non_empty_lines[0].strip()
        line2 = non_empty_lines[1].strip()

        has_import = 'import Content from' in line1
        has_content_tag = line2.startswith('<Content')

        return has_import and has_content_tag

    except Exception as e:
        print(f"Warning: Failed to check file {file_path}: {e}", file=sys.stderr)
        return False


def has_doctype_api(file_path: Path) -> bool:
    """æ£€æŸ¥æ–‡ä»¶ frontmatter ä¸­æ˜¯å¦åŒ…å« docType: API"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()

        # åªæ£€æŸ¥å‰ 20 è¡Œ
        for line in lines[:20]:
            if line.strip() == 'docType: API':
                return True

        return False

    except Exception:
        return False


def scan_directory(directory: str) -> List[Dict[str, Any]]:
    """
    é€’å½’æ‰«æç›®å½•ä¸‹æ‰€æœ‰ .md å’Œ .mdx æ–‡ä»¶

    Args:
        directory: è¦æ‰«æçš„ç›®å½•è·¯å¾„

    Returns:
        list: æ–‡ä»¶ä¿¡æ¯åˆ—è¡¨
    """
    dir_path = Path(directory)
    if not dir_path.exists():
        print(f"Error: Directory '{directory}' does not exist", file=sys.stderr)
        return []

    files = []
    for file_path in dir_path.rglob('*'):
        # åªå¤„ç† .md å’Œ .mdx æ–‡ä»¶
        if file_path.is_file() and file_path.suffix in ['.md', '.mdx']:
            # è®¡ç®—æ–‡ä»¶è¡Œæ•°
            line_count = 0
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    line_count = len(lines)
            except Exception:
                line_count = 0

            files.append({
                'path': str(file_path),
                'relative_path': str(file_path.relative_to(dir_path)),
                'suffix': file_path.suffix,
                'line_count': line_count,
                'size_kb': file_path.stat().st_size / 1024,
                'has_doctype_api': has_doctype_api(file_path),
                'is_reuse_doc': is_reuse_doc(file_path)
            })

    return files


def categorize_files(files: List[Dict[str, Any]], base_dir: Path) -> Dict[str, Any]:
    """
    åˆ†ç±»æ–‡ä»¶

    åˆ†ç±»è§„åˆ™ï¼š
    1. API æ–‡æ¡£ï¼šhas_doctype_api = Trueï¼ˆè‡ªåŠ¨è·³è¿‡ï¼‰
    2. YAML ç”Ÿæˆçš„ MDXï¼šåŒä¸€ç›®å½•ä¸‹å­˜åœ¨åŒå mdx å’Œ yaml æ–‡ä»¶
    3. å…¨å¤ç”¨æ–‡æ¡£ï¼šis_reuse_doc = True
    4. æ™®é€šæ–‡æ¡£ï¼šæŒ‰è¡Œæ•°åˆ†ç±»

    Returns:
        dict: åˆ†ç±»ç»“æœ
    """
    result = {
        'api_docs': [],           # API æ–‡æ¡£ï¼ˆè·³è¿‡ï¼‰
        'yaml_pairs': {           # YAML+MDX å¯¹
            'yaml_files': [],     # éœ€è¦ç¿»è¯‘çš„ yaml
            'mdx_files': []       # è·³è¿‡çš„ mdx
        },
        'reuse_docs': [],         # å…¨å¤ç”¨æ–‡æ¡£ï¼ˆéœ€è¦é¢„å¤„ç†ï¼‰
        'regular_docs': {         # æ™®é€šæ–‡æ¡£
            'small': [],          # < 50 è¡Œ
            'medium': [],         # 50-300 è¡Œ
            'large': []           # > 300 è¡Œ
        }
    }

    # ç¬¬ä¸€æ­¥ï¼šåˆ†ç¦» API æ–‡æ¡£å’Œé API æ–‡æ¡£
    non_api_files = []
    for file_info in files:
        if file_info.get('has_doctype_api', False):
            result['api_docs'].append(file_info)
        else:
            non_api_files.append(file_info)

    # ç¬¬äºŒæ­¥ï¼šæ£€æŸ¥ YAML+MDX æ–‡ä»¶å¯¹
    file_map = defaultdict(lambda: {'.mdx': None, '.md': None, '.yaml': None})

    for file_info in non_api_files:
        file_path_obj = Path(file_info['path'])
        parent_dir = file_path_obj.parent
        basename = file_path_obj.stem  # ä¸å¸¦æ‰©å±•åçš„æ–‡ä»¶å
        suffix = file_info['suffix']

        if suffix in ['.md', '.mdx', '.yaml']:
            key = (parent_dir, basename)
            file_map[key][suffix] = file_info

    # ç¬¬ä¸‰æ­¥ï¼šåˆ†ç±»æ–‡ä»¶
    for (parent_dir, basename), file_dict in file_map.items():
        md_file = file_dict['.md']
        mdx_file = file_dict['.mdx']
        yaml_file = file_dict['.yaml']

        # ä¼˜å…ˆå¤„ç† YAML+MDX å¯¹
        if yaml_file and (md_file or mdx_file):
            # è¿™æ˜¯ä¸€ä¸ª YAML ç”Ÿæˆçš„ MDX æ–‡ä»¶å¯¹
            result['yaml_pairs']['yaml_files'].append(yaml_file)
            if mdx_file:
                result['yaml_pairs']['mdx_files'].append(mdx_file)
            if md_file:
                result['yaml_pairs']['mdx_files'].append(md_file)
            continue

        # å¤„ç†å…¨å¤ç”¨æ–‡æ¡£
        if mdx_file and mdx_file.get('is_reuse_doc', False):
            result['reuse_docs'].append(mdx_file)
            continue

        if md_file and md_file.get('is_reuse_doc', False):
            result['reuse_docs'].append(md_file)
            continue

        # å¤„ç†æ™®é€šæ–‡æ¡£
        regular_files = [f for f in [md_file, mdx_file, yaml_file] if f]
        for file_info in regular_files:
            line_count = file_info['line_count']
            if line_count < 50:
                file_info['size_category'] = 'small'
                result['regular_docs']['small'].append(file_info)
            elif line_count <= 300:
                file_info['size_category'] = 'medium'
                result['regular_docs']['medium'].append(file_info)
            else:
                file_info['size_category'] = 'large'
                result['regular_docs']['large'].append(file_info)

    return result


def calculate_target_path(source_path: str, base_dir: Path) -> str:
    """
    è®¡ç®—ç›®æ ‡æ–‡ä»¶è·¯å¾„ï¼ˆå°† /zh/ æ›¿æ¢ä¸º /en/ï¼‰
    """
    path_obj = Path(source_path)
    parts = path_obj.parts

    # æ‰¾åˆ° /zh/ çš„ä½ç½®å¹¶æ›¿æ¢ä¸º /en/
    new_parts = []
    for part in parts:
        if part == 'zh':
            new_parts.append('en')
        else:
            new_parts.append(part)

    return str(Path(*new_parts))


def create_batches(categorized: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    åˆ›å»ºç¿»è¯‘æ‰¹æ¬¡

    æ‰¹æ¬¡ç­–ç•¥ï¼š
    - å°æ–‡ä»¶ï¼ˆ< 50 è¡Œï¼‰ï¼šæ¯æ‰¹ 10-20 ä¸ªæ–‡ä»¶ï¼Œæ€»è¡Œæ•°æ§åˆ¶åœ¨ 1000 è¡Œä»¥å†…
    - ä¸­ç­‰æ–‡ä»¶ï¼ˆ50-300 è¡Œï¼‰ï¼šæ¯æ‰¹ 2-5 ä¸ªæ–‡ä»¶ï¼Œæ€»è¡Œæ•°æ§åˆ¶åœ¨ 1500 è¡Œä»¥å†…
    - å¤§æ–‡ä»¶ï¼ˆ> 300 è¡Œï¼‰ï¼šå•ç‹¬æˆæ‰¹

    Args:
        categorized: åˆ†ç±»åçš„æ–‡ä»¶ä¿¡æ¯

    Returns:
        list: æ‰¹æ¬¡åˆ—è¡¨
    """
    batches = []
    batch_number = 1

    # å¤„ç†å°æ–‡ä»¶
    small_files = categorized['regular_docs']['small']
    if small_files:
        current_batch = {
            'batch_number': batch_number,
            'files': [],
            'total_lines': 0,
            'file_count': 0
        }

        for file_info in small_files:
            if current_batch['file_count'] >= 20 or current_batch['total_lines'] + file_info['line_count'] > 1000:
                batches.append(current_batch)
                batch_number += 1
                current_batch = {
                    'batch_number': batch_number,
                    'files': [],
                    'total_lines': 0,
                    'file_count': 0
                }

            current_batch['files'].append(file_info)
            current_batch['total_lines'] += file_info['line_count']
            current_batch['file_count'] += 1

        if current_batch['files']:
            batches.append(current_batch)
            batch_number += 1

    # å¤„ç†ä¸­ç­‰æ–‡ä»¶
    medium_files = categorized['regular_docs']['medium']
    if medium_files:
        for file_info in medium_files:
            # æ¯ 2-5 ä¸ªæ–‡ä»¶ä¸€æ‰¹ï¼Œæˆ–æ€»è¡Œæ•°è¶…è¿‡ 1500
            batch = {
                'batch_number': batch_number,
                'files': [file_info],
                'total_lines': file_info['line_count'],
                'file_count': 1
            }

            # å°è¯•æ·»åŠ æ›´å¤šæ–‡ä»¶
            for other_file in medium_files[medium_files.index(file_info) + 1:]:
                if batch['file_count'] >= 5 or batch['total_lines'] + other_file['line_count'] > 1500:
                    break
                batch['files'].append(other_file)
                batch['total_lines'] += other_file['line_count']
                batch['file_count'] += 1

            batches.append(batch)
            batch_number += 1

    # å¤„ç†å¤§æ–‡ä»¶
    large_files = categorized['regular_docs']['large']
    for file_info in large_files:
        batches.append({
            'batch_number': batch_number,
            'files': [file_info],
            'total_lines': file_info['line_count'],
            'file_count': 1,
            'needs_segmentation': file_info['line_count'] > 2000
        })
        batch_number += 1

    return batches


def print_summary(categorized: Dict[str, Any], base_dir: Path):
    """æ‰“å°æ‰«æç»“æœæ‘˜è¦"""
    print("\n" + "="*70)
    print("ğŸ“Š æ‰¹é‡ç¿»è¯‘æ‰«æç»“æœ")
    print("="*70)

    # API æ–‡æ¡£ç»Ÿè®¡
    api_docs = categorized['api_docs']
    print(f"\nâ­ï¸  API æ–‡æ¡£ï¼ˆdocType: APIï¼Œè‡ªåŠ¨è·³è¿‡ï¼‰ï¼š{len(api_docs)} ä¸ª")
    if api_docs:
        api_lines = sum(f['line_count'] for f in api_docs)
        print(f"   æ€»è¡Œæ•°ï¼š{api_lines} è¡Œ")
        for f in api_docs[:3]:
            print(f"   - {f['relative_path']} ({f['line_count']} è¡Œ)")
        if len(api_docs) > 3:
            print(f"   ... è¿˜æœ‰ {len(api_docs) - 3} ä¸ªæ–‡ä»¶")

    # YAML+MDX æ–‡ä»¶å¯¹ç»Ÿè®¡
    yaml_pairs = categorized['yaml_pairs']
    yaml_files = yaml_pairs['yaml_files']
    mdx_files = yaml_pairs['mdx_files']

    if yaml_files:
        print(f"\nğŸ”— YAML+MDX æ–‡ä»¶å¯¹ï¼š{len(yaml_files)} å¯¹")
        yaml_lines = sum(f['line_count'] for f in yaml_files)
        mdx_lines = sum(f['line_count'] for f in mdx_files)
        print(f"   âœ… éœ€ç¿»è¯‘ YAMLï¼š{len(yaml_files)} ä¸ª ({yaml_lines} è¡Œ)")
        print(f"   â­ï¸  è·³è¿‡ MDX/MDï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼‰ï¼š{len(mdx_files)} ä¸ª ({mdx_lines} è¡Œ)")
        for i, (yaml_file, mdx_file) in enumerate(zip(yaml_files[:3], mdx_files[:3])):
            print(f"   {i+1}. {yaml_file['relative_path']} â†’ {yaml_file['line_count']} è¡Œ")
            print(f"      {mdx_file['relative_path']} â†’ è·³è¿‡")
        if len(yaml_files) > 3:
            print(f"   ... è¿˜æœ‰ {len(yaml_files) - 3} å¯¹æ–‡ä»¶")

    # å…¨å¤ç”¨æ–‡æ¡£ç»Ÿè®¡
    reuse_docs = categorized['reuse_docs']
    print(f"\nğŸ”„ å…¨å¤ç”¨æ–‡æ¡£ï¼ˆéœ€è¦é¢„å¤„ç†ï¼‰ï¼š{len(reuse_docs)} ä¸ª")
    if reuse_docs:
        reuse_lines = sum(f['line_count'] for f in reuse_docs)
        print(f"   æ€»è¡Œæ•°ï¼š{reuse_lines} è¡Œ")
        for f in reuse_docs[:3]:
            print(f"   - {f['relative_path']} ({f['line_count']} è¡Œ)")
        if len(reuse_docs) > 3:
            print(f"   ... è¿˜æœ‰ {len(reuse_docs) - 3} ä¸ªæ–‡ä»¶")

    # æ™®é€šæ–‡æ¡£ç»Ÿè®¡
    regular_docs = categorized['regular_docs']
    small_files = regular_docs['small']
    medium_files = regular_docs['medium']
    large_files = regular_docs['large']

    print(f"\nğŸ“„ æ™®é€šæ–‡æ¡£åˆ†ç±»ï¼š")
    print(f"   å°æ–‡ä»¶ï¼ˆ< 50 è¡Œï¼‰ï¼š{len(small_files)} ä¸ª")
    if small_files:
        small_lines = sum(f['line_count'] for f in small_files)
        print(f"      æ€»è¡Œæ•°ï¼š{small_lines} è¡Œ")

    print(f"   ä¸­ç­‰æ–‡ä»¶ï¼ˆ50-300 è¡Œï¼‰ï¼š{len(medium_files)} ä¸ª")
    if medium_files:
        medium_lines = sum(f['line_count'] for f in medium_files)
        print(f"      æ€»è¡Œæ•°ï¼š{medium_lines} è¡Œ")

    print(f"   å¤§æ–‡ä»¶ï¼ˆ> 300 è¡Œï¼‰ï¼š{len(large_files)} ä¸ª")
    if large_files:
        large_lines = sum(f['line_count'] for f in large_files)
        print(f"      æ€»è¡Œæ•°ï¼š{large_lines} è¡Œ")
        print(f"   âš ï¸  å¤§æ–‡ä»¶åˆ—è¡¨ï¼š")
        for f in large_files[:5]:
            print(f"      - {f['relative_path']} ({f['line_count']} è¡Œ)")
        if len(large_files) > 5:
            print(f"      ... è¿˜æœ‰ {len(large_files) - 5} ä¸ªå¤§æ–‡ä»¶")


def print_batch_plan(batches: List[Dict[str, Any]]):
    """æ‰“å°ç¿»è¯‘æ‰¹æ¬¡è®¡åˆ’"""
    print("\n" + "="*70)
    print("ğŸ“‹ ç¿»è¯‘æ‰¹æ¬¡è®¡åˆ’")
    print("="*70)

    for i, batch in enumerate(batches, 1):
        print(f"\næ‰¹æ¬¡ {batch['batch_number']} ({batch['total_lines']} è¡Œ, {batch['file_count']} ä¸ªæ–‡ä»¶)")

        for j, file_info in enumerate(batch['files'], 1):
            size_flag = ""
            if file_info.get('size_category') == 'large':
                size_flag = " [å¤§æ–‡ä»¶]"
            elif file_info.get('size_category') == 'medium':
                size_flag = " [ä¸­ç­‰]"

            print(f"  {j}. {file_info['relative_path']}{size_flag}")
            print(f"     â†’ {file_info['line_count']} è¡Œ")

        if batch.get('needs_segmentation'):
            print(f"  âš ï¸  æ­¤æ‰¹æ¬¡åŒ…å«è¶…å¤§æ–‡ä»¶ï¼Œç¿»è¯‘æ—¶éœ€è¦åˆ†æ®µå¤„ç†ï¼ˆæ¯æ®µä¸è¶…è¿‡ 2000 è¡Œï¼‰")


def main():
    if len(sys.argv) < 2:
        print("Usage: scan_batch_translation.py <directory>", file=sys.stderr)
        print("Example: scan_batch_translation.py core_products/real-time-voice-video/zh/flutter", file=sys.stderr)
        sys.exit(1)

    directory = sys.argv[1]
    base_dir = Path(directory)

    print(f"ğŸ” æ‰«æç›®å½•ï¼š{directory}")

    # æ‰«ææ–‡ä»¶
    files = scan_directory(directory)

    if not files:
        print("\nâŒ æœªæ‰¾åˆ°ä»»ä½•æ–‡ä»¶")
        sys.exit(0)

    # åˆ†ç±»æ–‡ä»¶
    categorized = categorize_files(files, base_dir)

    # è®¡ç®—ç›®æ ‡è·¯å¾„å¹¶æ·»åŠ åˆ°æ–‡ä»¶ä¿¡æ¯ä¸­
    all_files = (
        categorized['api_docs'] +
        categorized['yaml_pairs']['yaml_files'] +
        categorized['yaml_pairs']['mdx_files'] +
        categorized['reuse_docs'] +
        categorized['regular_docs']['small'] +
        categorized['regular_docs']['medium'] +
        categorized['regular_docs']['large']
    )

    for file_info in all_files:
        file_info['target_path'] = calculate_target_path(file_info['path'], base_dir)

    # åˆ›å»ºç¿»è¯‘æ‰¹æ¬¡ï¼ˆåªåŒ…å«éœ€è¦ç¿»è¯‘çš„æ–‡ä»¶ï¼‰
    files_to_translate = (
        categorized['yaml_pairs']['yaml_files'] +
        categorized['reuse_docs'] +
        categorized['regular_docs']['small'] +
        categorized['regular_docs']['medium'] +
        categorized['regular_docs']['large']
    )

    batches = create_batches(categorized)

    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    total_files = len(files_to_translate)
    total_lines = sum(f['line_count'] for f in files_to_translate)

    # æ‰“å°ç»“æœ
    print_summary(categorized, base_dir)
    print_batch_plan(batches)

    print(f"\nğŸ“Š ç¿»è¯‘ä»»åŠ¡ç»Ÿè®¡ï¼š")
    print(f"   éœ€ç¿»è¯‘æ–‡ä»¶æ•°ï¼š{total_files}")
    print(f"   éœ€ç¿»è¯‘æ€»è¡Œæ•°ï¼š{total_lines}")
    print(f"   ç¿»è¯‘æ‰¹æ¬¡æ•°ï¼š{len(batches)}")

    # è¾“å‡º JSON æ ¼å¼ä¾›åç»­å¤„ç†
    print("\n" + "="*70)
    print("ğŸ“„ JSON è¾“å‡º")
    print("="*70)

    result = {
        'directory': directory,
        'summary': {
            'total_files': total_files,
            'total_lines': total_lines,
            'skipped_api_files': len(categorized['api_docs']),
            'skipped_mdx_files': len(categorized['yaml_pairs']['mdx_files']),
            'yaml_files_to_translate': len(categorized['yaml_pairs']['yaml_files']),
            'reuse_docs': len(categorized['reuse_docs']),
            'small_files': len(categorized['regular_docs']['small']),
            'medium_files': len(categorized['regular_docs']['medium']),
            'large_files': len(categorized['regular_docs']['large'])
        },
        'files': all_files,
        'batches': batches
    }

    print(json.dumps(result, indent=2, ensure_ascii=False))


if __name__ == '__main__':
    main()
