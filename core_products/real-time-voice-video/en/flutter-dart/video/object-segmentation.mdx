---
articleID: 17917
date: "2024-09-06"
---
# Object Segmentation

- - -

## Feature Overview


Object Segmentation is a value-added capability provided by Express SDK. It uses AI algorithms to recognize content in video frames and sets `transparency information` for each pixel. Pixels of the main subject are set to "opaque", while pixels outside the main subject are set to "transparent". Developers can use the transparency information of these pixels to process the subject and non-subject parts of the image differently, thereby implementing different features.

<Warning title="Note">
- The current official SDK does not include "Object Segmentation" related features. If needed, please contact ZEGOCLOUD Technical Support for a special build and provide your AppID to enable relevant permissions.
- "Object Segmentation" is a paid feature. For trial or formal pricing inquiries, please contact ZEGOCLOUD sales personnel.
</Warning>



### Object Segmentation Types

For users in different environments, ZEGO provides two segmentation capabilities: "Green Screen Background Segmentation" and "Arbitrary Background Segmentation".

<table>

<tbody><tr>
<th>Segmentation Type</th>
<th>Green Screen Background Segmentation</th>
<th>Arbitrary Background Segmentation</th>
</tr>
<tr>
<th>Capability Description</th>
<td><p>When users set up a green screen themselves, the subject in the non-green screen area can be retained.</p><p>Suitable for e-commerce live streaming, online exams, and other scenarios.</p></td>
<td><p>Most users do not have the conditions to set up a green screen. Through ZEGO's arbitrary background segmentation capability, the subject in the image can be recognized without a green screen.</p><p>Suitable for online education, video conferences, and other scenarios.</p></td>
</tr>
<tr>
<th>Illustration</th>
<td><Frame width="auto" height="256" caption=""><img src="https://doc-media.zego.im/sdk-doc/Pics/subject_segmentation/people_with_greenscreen.png" /></Frame></td>
<td><Frame width="auto" height="256" caption=""><img src="https://doc-media.zego.im/sdk-doc/Pics/subject_segmentation/people_with_realenvironment.jpg" /></Frame></td>
</tr>
</tbody></table>

### Feature Scenarios

Based on the object segmentation capability, developers can implement business scenarios such as background blurring, virtual backgrounds, presenter mode, and multi-user real-time interaction, creating more diverse interactive experiences.

<table>

<tbody><tr>
<th>Feature</th>
<th>Background Blur</th>
<th>Virtual Background</th>
<th>Transparent Background</th>
<th>Subject Segmentation and Transmission</th>
</tr>
<tr>
<th>Feature Description</th>
<td>Blur the area outside the subject.</td>
<td>Replace the area outside the subject with custom images, videos, or colors.</td>
<td><p>Render the subject's image over other video content locally.</p><p>For example, implement features such as presenter mode over screen sharing or playing video content.</p></td>
<td>Combined with the Alpha channel data transmission capability provided by Express SDK, the segmented subject is transmitted to the playing end and rendered at the playing end, achieving the visual effect of multiple people in different locations appearing in the same scene in real time</td>
</tr>
<tr>
<th>Illustration</th>
<td><Frame width="auto" height="auto" caption=""><img src="https://media-resource.spreading.io/docuo/workspace740/af061ebc6eaf0f12ae9e7f72235bd04e/6364d2bc42.png" /></Frame></td>
<td><Frame width="auto" height="auto" caption=""><img src="https://media-resource.spreading.io/docuo/workspace740/af061ebc6eaf0f12ae9e7f72235bd04e/de1e9cf26f.png" /></Frame></td>
<td><Frame width="auto" height="auto" caption=""><img src="https://media-resource.spreading.io/docuo/workspace740/af061ebc6eaf0f12ae9e7f72235bd04e/5bc5b6b70d.png" /></Frame></td>
<td><Frame width="auto" height="auto" caption=""><img src="https://doc-media.zego.im/sdk-doc/Pics/subject_segmentation/multiplayer.jpeg" /></Frame></td>
</tr>
</tbody></table>


### Hardware Compatibility

<table>

<tbody><tr>
<th>Platform</th>
<th>Hardware Requirements</th>
</tr>
<tr>
<td>Android</td>
<td><ul><li><p>Snapdragon Chips:</p><ul><li>Snapdragon 6 Series: Snapdragon 675 and above</li><li>Snapdragon 7 Series: Snapdragon 730 and above</li><li>Snapdragon 8 Series: Snapdragon 835 and above</li></ul></li><li><p>HiSilicon Kirin Chips:</p><ul><li>Kirin 8 Series: Kirin 820 and above</li><li>Kirin 9 Series: Kirin 980 and above</li></ul></li><li><p>MediaTek Chips:</p><ul><li>Helio P Series: Helio P60 and above</li><li>Dimensity Series: Dimensity 820 and above</li></ul></li><li>Samsung Chips: Exynos 1080 and above</li></ul></td>
</tr>
<tr>
<td>iOS</td>
<td>A Series Chips: Apple A9 and above, e.g., iPhone 6s</td>
</tr>
<tr>
<td>macOS</td>
<td>M Series Chips: Apple M1 and above</td>
</tr>
<tr>
<td>Windows</td>
<td>Intel Core i5 and above</td>
</tr>
</tbody></table>

**Please note, when using Flutter SDK on the above platforms, hardware compatibility is as above.**

## Sample Source Code

Please refer to [Download Sample Source Code](/real-time-video-flutter/quick-start/run-example-code) to get the source code.

For related source code, please check files in the "lib/topics/OtherFunctions/screen_sharing" directory.

## Prerequisites

Before using the object segmentation feature, ensure:

- You have contacted ZEGOCLOUD Technical Support for a special build.

- You have created a project in the [ZEGOCLOUD Console](https://console.zegocloud.com) and applied for a valid AppID and AppSign. For details, please refer to [Console - Project Information](/console/project-info).
- You have integrated ZEGO Express SDK in your project and implemented basic audio and video publishing and playing features. For details, please refer to [Quick Start - Integrating SDK](/real-time-video-flutter/quick-start/integrating-sdk) and [Quick Start - Implementing Video Call](/real-time-video-flutter/quick-start/implementing-video-call).


## Implementation

<Warning title="Note">

- Enabling object segmentation will consume additional system resources. To ensure user experience, currently only one channel's publishing stream can have object segmentation enabled.
- If using third-party filters through custom preprocessing, ensure that the third-party filters support Alpha channel passthrough.

</Warning>



<Frame width="512" height="auto" caption=""><img src="https://doc-media.zego.im/sdk-doc/Pics/subject_segmentation/subject_segmentation_flutter.png" /></Frame>

Please note, developers can choose whether to implement the **(Optional)** steps in the above diagram according to their business scenario needs. If implementation is needed, please refer to the specific instructions below.

### Initialization and Room Login

For the specific process of initialization and room login, please refer to "[Create Engine](/real-time-video-flutter/quick-start/implementing-video-call)" and "[Login Room](/real-time-video-flutter/quick-start/implementing-video-call)" in the Implementing Video Call documentation.


### Listen for Object Segmentation State Callback

Call the [onVideoObjectSegmentationStateChanged](https://www.zegocloud.com/docs/unique-api/express-video-sdk/en/dart_flutter/zego_express_engine/ZegoExpressEngine/onVideoObjectSegmentationStateChanged.html) interface to listen for object segmentation state callbacks.

<Warning title="Note">
The object segmentation state callback depends on enabling preview or publishing. That is, to listen to the [onVideoObjectSegmentationStateChanged](https://www.zegocloud.com/docs/unique-api/express-video-sdk/en/dart_flutter/zego_express_engine/ZegoExpressEngine/onVideoObjectSegmentationStateChanged.html) callback, you need to call preview [startPreview](https://www.zegocloud.com/docs/unique-api/express-video-sdk/en/dart_flutter/zego_express_engine/ZegoExpressEnginePublisher/startPreview.html) or publishing [startPublishingStream](https://www.zegocloud.com/docs/unique-api/express-video-sdk/en/dart_flutter/zego_express_engine/ZegoExpressEnginePublisher/startPublishingStream.html).
</Warning>



```dart
ZegoExpressEngine.onVideoObjectSegmentationStateChanged = (state, channel, errorCode) {
  ZegoLog().addLog(
      'Video object segmentation state changed. state: $state, errorCode: $errorCode, channel: $channel');
};
```


### Use Object Segmentation to Implement Different Business Features

<Warning title="Note">
If developers need to update the object segmentation type or background processing type, they need to modify the [ZegoObjectSegmentationConfig](https://www.zegocloud.com/docs/unique-api/express-video-sdk/en/dart_flutter/zego_express_engine/ZegoObjectSegmentationConfig-class.html) configuration and call the [enableVideoObjectSegmentation](https://www.zegocloud.com/docs/unique-api/express-video-sdk/en/dart_flutter/zego_express_engine/ZegoExpressEnginePublisher/enableVideoObjectSegmentation.html) interface again to enable object segmentation to update the object segmentation effect. The update result will be notified to developers through the [onVideoObjectSegmentationStateChanged](https://www.zegocloud.com/docs/unique-api/express-video-sdk/en/dart_flutter/zego_express_engine/ZegoExpressEngine/onVideoObjectSegmentationStateChanged.html) callback.
</Warning>



<h4>Background Blur</h4>

<Accordion title="Use Object Segmentation to Implement Background Blur" defaultOpen="false">
Call the [enableVideoObjectSegmentation](https://www.zegocloud.com/docs/unique-api/express-video-sdk/en/dart_flutter/zego_express_engine/ZegoExpressEnginePublisher/enableVideoObjectSegmentation.html) interface to enable object segmentation and set the background processing type to "Blur".

```dart
ZegoObjectSegmentationConfig config = ZegoObjectSegmentationConfig.defaultConfig();
config.objectSegmentationType = ZegoObjectSegmentationType.AnyBackground;//Select the object segmentation type to enable based on actual situation
config.backgroundConfig.processType = ZegoBackgroundProcessType.Blur;//Set background processing method to blur
config.backgroundConfig.blurLevel = ZegoBackgroundBlurLevel.Medium;//Set background blur level to medium
engine.enableVideoObjectSegmentation(enable, config, ZegoPublishChannel.Main);//Enable object segmentation
```
</Accordion>

<h4>Virtual Background</h4>

<Accordion title="Use Object Segmentation to Implement Virtual Background" defaultOpen="false">
Virtual background supports two types of materials:

- Images.
  Currently supports "PNG" and "JPEG" image formats, i.e., image files with ".png", ".jpg", and ".jpeg" extensions.
- Videos, with the following restrictions:
    - Video format: MP4, FLV, MKV, AVI.
    - Video source: Local video.
    - Video playback method: Loop playback.
    - Resolution: Maximum not exceeding 4096 px, recommended within 1920 px.
    - Video duration: Maximum not exceeding 30 seconds, recommended within 15 seconds.
    - Video size: Maximum not exceeding 50 MB, recommended within 10 MB.

<Warning title="Warning">


When using this feature, developers should pay attention to the aspect ratio of custom images and video materials, otherwise parts exceeding the view will be cropped.

</Warning>



Call the [enableVideoObjectSegmentation](https://www.zegocloud.com/docs/unique-api/express-video-sdk/en/dart_flutter/zego_express_engine/ZegoExpressEnginePublisher/enableVideoObjectSegmentation.html) interface to enable object segmentation and set the background processing type to "Image" or "Video".

```dart
ZegoObjectSegmentationConfig config = ZegoObjectSegmentationConfig.defaultConfig();
config.objectSegmentationType = ZegoObjectSegmentationType.AnyBackground;//Select the object segmentation type to enable based on actual situation

//Set background processing method to Image
config.backgroundConfig.processType = ZegoBackgroundProcessType.Image;
config.backgroundConfig.imageURL = "<image_path>";//Set background image path
engine.enableVideoObjectSegmentation(enable, config, ZegoPublishChannel.Main);//Enable object segmentation

//Set background processing method to Video
config.backgroundConfig.processType = ZegoBackgroundProcessType.Video;
config.backgroundConfig.videoURL = "<video_path>";//Set background video path
engine.enableVideoObjectSegmentation(enable, config, ZegoPublishChannel.Main);//Enable object segmentation
```
</Accordion>

<h4>Transparent Background</h4>

<a name="enableAlphaChannelVideoEncoder"></a>

<Accordion title="Use Object Segmentation to Implement Transparent Background" defaultOpen="false">
<Warning title="Warning">


If developers need to implement business functions similar to "presenter mode", they need to mix the "subject image" with "video source content to be mixed" into one video stream on the business side.


</Warning>



Call the [enableVideoObjectSegmentation](https://www.zegocloud.com/docs/unique-api/express-video-sdk/en/dart_flutter/zego_express_engine/ZegoExpressEnginePublisher/enableVideoObjectSegmentation.html) interface to enable object segmentation and set the background processing type to "Transparent".

```dart
ZegoObjectSegmentationConfig config = ZegoObjectSegmentationConfig.defaultConfig();
config.objectSegmentationType = ZegoObjectSegmentationType.AnyBackground;//Select the object segmentation type to enable based on actual situation
config.backgroundConfig.processType = ZegoBackgroundProcessType.Transparent;//Set background processing method to transparent
engine.enableVideoObjectSegmentation(enable, config, ZegoPublishChannel.Main);//Enable object segmentation
```
</Accordion>

### (Optional) Use Alpha Channel to Transmit Segmented Subject

<Accordion title="Use Alpha Channel to Transmit Segmented Subject" defaultOpen="false">
If the publishing end needs to transmit the segmented subject image to the playing end through the Alpha channel and render the subject at the playing end, you need to first call the [enableAlphaChannelVideoEncoder](https://www.zegocloud.com/docs/unique-api/express-video-sdk/en/dart_flutter/zego_express_engine/ZegoExpressEnginePublisher/enableAlphaChannelVideoEncoder.html) interface to set the encoder to support the alpha channel, then call the [startPublishingStream](https://www.zegocloud.com/docs/unique-api/express-video-sdk/en/dart_flutter/zego_express_engine/ZegoExpressEnginePublisher/startPublishingStream.html) interface to publish the stream to transmit it smoothly to the playing end.

<Warning title="Warning">


Currently only supports alpha channel data arranged below RGB or YUV data.

</Warning>



- Enable Alpha channel data transmission:

    ```dart
    ZegoAlphaLayoutType layoutType = ZegoAlphaLayoutType.Bottom; // Alpha channel data arranged below RGB or YUV data
    engine.enableAlphaChannelVideoEncoder(true, layoutType, ZegoPublishChannel.Main); // Enable encoder to support alpha channel
    ```

- Disable Alpha channel data transmission:

    ```dart
    ZegoAlphaLayoutType layoutType = ZegoAlphaLayoutType.Bottom; // Alpha channel data arranged below RGB or YUV data
    engine.enableAlphaChannelVideoEncoder(false, layoutType, ZegoPublishChannel.Main); // Disable encoder to support alpha channel
    ```
</Accordion>

### Start Preview and Publishing

After enabling the object segmentation feature through the [enableVideoObjectSegmentation](https://www.zegocloud.com/docs/unique-api/express-video-sdk/en/dart_flutter/zego_express_engine/ZegoExpressEnginePublisher/enableVideoObjectSegmentation.html) interface, you can preview.

<Note title="Note">


Developers can also enable preview first, then enable object segmentation. This document introduces enabling object segmentation first, then previewing.


</Note>



```dart
ZegoCanvas canvas = ZegoCanvas(previewView);
canvas.alphaBlend = true;//Enable internal rendering Alpha blending. After enabling, it supports Alpha blending between the segmented subject and background layer
engine.startPreview(canvas);
engine.startPublishingStream(streamID);
```

### (Optional) Set Alpha Channel Rendering at Playing End and Start Playing

<Accordion title="Set Alpha Channel Rendering at Playing End and Start Playing" defaultOpen="false">
<Warning title="Warning">


Alpha channel rendering needs to be enabled at the playing end only when the publishing end has enabled Alpha channel transmission.


</Warning>



```dart
ZegoCanvas canvas = ZegoCanvas(playView);
canvas.alphaBlend = true;//Enable internal rendering Alpha blending. After enabling, it supports Alpha blending between the segmented subject and background layer
engine.startPlayingStream(streamID, canvas);
```
</Accordion>

### Disable Object Segmentation

Call the [enableVideoObjectSegmentation](https://www.zegocloud.com/docs/unique-api/express-video-sdk/en/dart_flutter/zego_express_engine/ZegoExpressEnginePublisher/enableVideoObjectSegmentation.html) interface to disable object segmentation.

```dart
ZegoObjectSegmentationType objectType = ZegoObjectSegmentationType.AnyBackground;//Select the object segmentation type to disable based on actual situation
engine.enableVideoObjectSegmentation(false, objectType, ZegoPublishChannel.Main);//Disable object segmentation
```

### Destroy Engine

Call the [destroyEngine](https://www.zegocloud.com/docs/unique-api/express-video-sdk/en/dart_flutter/zego_express_engine/ZegoExpressEngine/destroyEngine.html) interface to destroy the engine.

```dart
ZegoExpressEngine.destroyEngine();
```
