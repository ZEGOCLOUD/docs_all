---
articleID: 11256
date: "2025-07-30"
---
# Integration with AI-Effects

---

## Usage Guide

### Introduction

Video Call is a real-time audio/video interaction service product from ZEGO. Developers can build audio/video applications through its flexible and easy-to-use API. At the same time, another product from ZEGO - AI-Effects, based on leading AI algorithms, provides features such as beauty effects, body reshaping, makeup, stickers, etc. Combining the two can easily achieve the combination of audio/video interaction and beauty effects, creating real-time beauty applications.

The combination of the two can be widely used in live streaming scenarios such as entertainment live streaming, game live streaming, video conferences, etc.

<video poster="https://doc-media.zego.im/sdk-doc/Pics/ZegoEffects/AI_cover.png" src="https://doc-media.zego.im/sdk-doc/doc/video/AI/AI.mp4" width="65%" muted="true" loop="true" autoplay="autoplay" preload="auto" nocontrols></video>

### Concept Explanation

- ZEGO Express SDK: ZEGO real-time audio/video SDK, providing basic real-time audio/video functionality, including live streaming publishing and playing, live streaming co-hosting, etc. Hereinafter referred to as ZEGO Express SDK.
- ZEGO Effects SDK: ZEGO AI beauty effects SDK, providing multiple intelligent image rendering and algorithm capabilities, including intelligent beauty effects, AR effects, image segmentation, etc. Hereinafter referred to as ZEGO Effects SDK.


## Example Source Code

To facilitate developers in implementing the combination of the two, ZEGO provides example code. Please refer to [AI-Effects - Run Sample Code](https://www.zegocloud.com/docs/ai-effects-ios-objc/quick-starts/run-sample-codes).


## Prerequisites

- You have created a project in the [ZEGO Console](https://console.zego.im) and applied for a valid AppID and AppSign. For details, see "Project Information" in [Console - Project Management](/console/project-info); and contacted ZEGO technical support to enable ZEGO Effects related package service permissions.
- You have integrated ZEGO Express SDK into your project and implemented basic audio/video publishing and playing functionality. For details, see [Quick Start - Integration](/real-time-video-ios-oc/quick-start/integrating-sdk) and [Quick Start - Implementing Video Call](/real-time-video-ios-oc/quick-start/implementing-video-call).
- You have integrated ZEGO Effects SDK into your project. For details, see "AI-Effects" [Quick Start - Integration](/ai-effects-ios-objc/quick-starts/import-the-sdk).
- You have obtained the unique authentication file for ZEGO Effects SDK. For details, see "AI-Effects" [Quick Start - Online Authentication](/ai-effects-ios-objc/quick-starts/online-authentication).


## Usage Principles and Steps

The principle of using ZEGO Effects SDK and ZEGO Express SDK together to perform real-time AI beauty effects processing on video data is as follows:
<Frame width="512" height="auto" caption="">
  <img src="https://doc-media.zego.im/sdk-doc/Pics/Common/ZegoExpressEngine/video_pre_processing.png" />
</Frame>

Through the above process, the specific implementation steps are as follows:
<Frame width="512" height="auto" caption="">
  <img src="https://doc-media.zego.im/sdk-doc/Pics/ZegoEffects/Apple/RTC_AI_bestPractise3.png" />
</Frame>

1. Initialize ZEGO Effects SDK and ZEGO Express SDK. There is no timing restriction on initialization.
2. Obtain original video data, which can be obtained through [Custom Video Capture](/real-time-video-ios-oc/video/custom-video-capture) or [Custom Video Preprocessing](/real-time-video-ios-oc/video/custom-video-preprocessing) of ZEGO Express SDK.
3. Pass the captured original video data to ZEGO Effects SDK for AI beauty effects processing.
4. Pass the processed data to ZEGO Express SDK for publishing. If you need to adjust AI beauty effects during the publishing and playing process, you can use the related functions of ZEGO Effects SDK to make real-time changes.
5. Remote users play the processed data by pulling it through ZEGO Express SDK.


### 1 Initialize ZEGO Effects/Express SDK

For the initialization of the two SDKs, there is no timing restriction. The following steps take "initializing ZEGO Effects SDK first, then initializing ZEGO Express SDK" as an example.

#### Initialize ZEGO Effects SDK

1. Import Effects models and resources.

    When using AI-related features of ZEGO Effects SDK, you must first import AI models and resources.

    ```objc
    // Pass in the absolute path of the face recognition model. Face detection, big eyes, and face slimming features all require this
    NSString *faceDetectionModelPath = [[NSBundle mainBundle] pathForResource:@"FaceDetectionModel" ofType:@"model"];
    // Pass in the absolute path of the portrait segmentation model. AI portrait segmentation feature requires this
    NSString *segmentationModelPath = [[NSBundle mainBundle] pathForResource:@"SegmentationModel" ofType:@"model"];

    // Pass in the absolute path of beauty effects and body reshaping common resources
    NSString *commonBundlePath = [[NSBundle mainBundle] pathForResource:@"CommonResources" ofType:@"bundle"];
    // Pass in the absolute path of pendant resources.
    NSString *pendantBundlePath = [[NSBundle mainBundle] pathForResource:@"PendantResources" ofType:@"bundle"];
    // Pass in the absolute path of whitening resources.
    NSString *whitenBundlePath = [[NSBundle mainBundle] pathForResource:@"FaceWhiteningResources" ofType:@"bundle"];

    // Pass in the list of resource or model paths, must be called before create
    [ZegoEffects setResources:@[faceDetectionModelPath, SegmentationModel, commonBundlePath, pendantBundlePath, whitenBundlePath]];
    ```

    For all resources and models supported by ZEGO Effects SDK, please refer to "AI-Effects" [Quick Start - Import Resources and Models](/ai-effects-ios-objc/quick-starts/import-resources-and-models).

2. Create Effects object.

<Tabs>
<Tab title="ZegoEffects SDK 2.1.0 and above">
Directly pass the AppID and AppSign applied for in [Prerequisites](#prerequisites) into the [create](https://www.zegocloud.com/docs/article/api?doc=Effects_SDK_API~objectivec_ios~class~ZegoEffects#create-appid-app-sign-callback-1) interface. After internal authentication by the SDK, create the Effects object and return the related [error code](/real-time-video-ios-oc/client-sdk/error-code).

```objc
ZegoEffects *customEffects = nil;
NSInteger appId = *******;
NSString *appSign = @"********";
[ZegoEffects create:appid appSign:appSign callback:^(ZegoEffects * _Nonnull effects, NSInteger errorCode)]{
    customEffects = effects;
    // Execute custom logic
}
```
</Tab>
<Tab title="ZegoEffects SDK below 2.1.0">
Call the [create](https://www.zegocloud.com/docs/article/api?doc=effects-sdk_API~objectivec_ios~class~ZegoEffects#create-license) interface, pass in the authentication file obtained in [Prerequisites](#prerequisites), and create the Effects object.
```objc
// Please refer to the actual obtained file for authentication content
ZegoEffects *effects = [ZegoEffects create:@"ABCDEFG"];
// Save effects instance
self.effects = effects;
```
</Tab>
</Tabs>


3. Initialize Effects object.

    Call the [initEnv](https://www.zegocloud.com/docs/article/api?doc=Effects_SDK_API~objective-c_ios~class~ZegoEffects#init-env-resolution) interface to initialize the Effects object, which requires passing in the width and height of the video image data to be processed.

    Taking processing 1280 Ã— 720 video images as an example:

    ```objc
    // Initialize the Effects object and pass in the width and height of the original image to be processed. Manage the lifecycle yourself. When stopping image capture, call [self.effects uninitEnv]; interface to uninitialize, otherwise it will cause memory leaks.
    [self.effects initEnv:CGSizeMake(1280, 720)];
    ```

#### Initialize ZEGO Express SDK

Call the [createEngineWithProfile](https://www.zegocloud.com/docs/article/api?doc=Express_Video_SDK_API~ObjectiveC_ios~class~zego-express-engine#create-engine-with-profile-event-handler) interface to initialize ZEGO Express SDK.

```objc
ZegoEngineProfile *profile = [[ZegoEngineProfile alloc] init];
profile.appID = [KeyCenter appID];
profile.appSign = [KeyCenter appSign];
profile.scenario = ZegoScenarioDefault;
[ZegoExpressEngine createEngineWithProfile:profile eventHandler:self];
```

### 2 Obtain Original Video Data

ZEGO Express SDK can obtain original video data through [Custom Video Preprocessing](/real-time-video-ios-oc/best-practice/integration-with-zego-effects-sdk) and [Custom Video Capture](/real-time-video-ios-oc/best-practice/integration-with-zego-effects-sdk).

The differences between the two obtaining methods are as follows. Developers can choose as needed based on actual situations.

<table>

  <tbody><tr>
    <th>Data Acquisition Method</th>
    <th>Video Data Capture Method</th>
    <th>Advantage</th>
  </tr>
  <tr>
    <td>Custom video preprocessing</td>
    <td>Video data is captured internally by ZEGO Express SDK, and original video data is obtained through callbacks.</td>
    <td>Extremely simple combination of ZEGO Express SDK and ZEGO Effects SDK. Developers do not need to manage device input sources, only need to operate on the original data thrown by ZEGO Express SDK, and then pass it back to ZEGO Express SDK.</td>
  </tr>
  <tr>
    <td>Custom video capture</td>
    <td>Video data is captured by the developer themselves and provided to ZEGO Express SDK.</td>
    <td>When integrating with multiple manufacturers, business implementation is more flexible, and there is more room for performance optimization.</td>
  </tr>
</tbody></table>

<a id="Customvideopreprocessing"></a>

- **Method 1: Custom Video Preprocessing**

    Taking obtaining [CVPixelBufferRef](https://www.zegocloud.com/docs/article/api?doc=Express_Video_SDK_API~objectivec_ios~enum~ZegoVideoBufferType#zego-video-buffer-type-cv-pixel-buffer) type original video data as an example.

    Developers call the [enableCustomVideoProcessing](https://www.zegocloud.com/docs/article/api?doc=Express_Video_SDK_API~ObjectiveC_ios~class~zego-express-engine#enable-custom-video-processing-config) interface to enable custom video preprocessing; after enabling, ZEGO Express SDK will internally capture video data; after capture is complete, the captured original video data can be obtained through the [onCapturedUnprocessedCVPixelBuffer](https://www.zegocloud.com/docs/article/api?doc=Express_Video_SDK_API~ObjectiveC_ios~protocol~zego-custom-video-process-handler&jumpType=route#on-captured-unprocessed-cv-pixel-buffer-timestamp-channel) callback interface.

    ```objc
    ZegoCustomVideoProcessConfig *processConfig = [[ZegoCustomVideoProcessConfig alloc] init];
    // Select CVPixelBuffer type video frame data
    processConfig.bufferType = ZegoVideoBufferTypeCVPixelBuffer;

    // Enable custom preprocessing
    [[ZegoExpressEngine sharedEngine] enableCustomVideoProcessing:YES config:processConfig channel:ZegoPublishChannelMain];

    // Set self as the custom video preprocessing callback object
    [[ZegoExpressEngine sharedEngine] setCustomVideoProcessHandler:self];
    ```

    For specific principles, please refer to [Custom Video Preprocessing](/real-time-video-ios-oc/video/custom-video-preprocessing) in "Video Call".

<a id="CustomCollection"></a>

- **Method 2: Custom Video Capture**

    Custom video capture mainly relies on developers capturing video data themselves. For specific methods, please refer to [Custom Video Capture](/real-time-video-ios-oc/video/custom-video-capture) in "Video Call".


### 3 Perform AI Beauty Effects Processing

After obtaining the original video data, pass the data to ZEGO Effects SDK to start AI beauty effects processing on the video (e.g., beauty effects, makeup, background segmentation, etc.).

- **Method 1: Custom Video Preprocessing**

    In the [onCapturedUnprocessedCVPixelBuffer](https://www.zegocloud.com/docs/article/api?doc=Express_Video_SDK_API~ObjectiveC_ios~protocol~zego-custom-video-process-handler&jumpType=route#on-captured-unprocessed-cv-pixel-buffer-timestamp-channel) callback, after obtaining the original video data, call the related interfaces of ZEGO Effects SDK to perform AI beauty effects processing (please refer to [Beauty Effects](/ai-effects-ios-objc/guides/face-beautification), [Body Reshaping](/ai-effects-ios-objc/guides/shape-retouch), [Background Segmentation](/ai-effects-ios-objc/guides/background-segmentation), [Face Detection](/ai-effects-ios-objc/guides/face-detection), [Stickers](https://www.zegocloud.com/docs/faq/AIEffect_Stickers?product=Effects&platform=ios), [Filters](/ai-effects-ios-objc/guides/filters)), and return the processed data to ZEGO Express SDK.

    ```objc
    // Taking custom preprocessing as an example
    // Callback method to obtain original data
    - (void)onCapturedUnprocessedCVPixelBuffer:(CVPixelBufferRef)buffer timestamp:(CMTime)timestamp channel:(ZegoPublishChannel)channel {
        ...
        // Custom preprocessing: Use ZEGO Effects SDK here
        [self.effects processImageBuffer:buffer];

        // Send the processed buffer back to ZEGO Express SDK
        [[ZegoExpressEngine sharedEngine] sendCustomVideoProcessedCVPixelBuffer:output timestamp:timestamp channel:channel];
        ...
    }
    ```

- **Method 2: Custom Video Capture**

    After receiving the [onStart](@onStart) callback for custom capture, developers obtain video data through custom capture, then call the related interfaces of ZEGO Effects SDK to perform AI beauty effects processing (please refer to [Beauty Effects](/ai-effects-ios-objc/guides/face-beautification), [Body Reshaping](/ai-effects-ios-objc/guides/shape-retouch), [Background Segmentation](/ai-effects-ios-objc/guides/background-segmentation), [Face Detection](/ai-effects-ios-objc/guides/face-detection), [Stickers](https://www.zegocloud.com/docs/faq/AIEffect_Stickers?product=Effects&platform=ios), [Filters](/ai-effects-ios-objc/guides/filters)), and return the processed data to ZEGO Express SDK (please refer to "3 Send Video Frame Data to SDK" in [Custom Video Capture](/real-time-video-ios-oc/video/custom-video-capture#3-send-video-frame-data-to-sdk)).


### 4 Publish Processed Data

After processing is completed by ZEGO Effects SDK, return the processed data to ZEGO Express SDK.

ZEGO Express SDK calls the [startPublishingStream](https://www.zegocloud.com/docs/article/api?doc=Express_Video_SDK_API~ObjectiveC~class~zego-express-engine#start-publishing-stream) interface, passes in the processed data stream streamID, starts publishing, and sends to the cloud server.

```objc
// Start publishing stream
[[ZegoExpressEngine sharedEngine] startPublishingStream:@"streamID"];
```

### 5 Pull Processed Data for Playback

After ZEGO Express SDK starts publishing, remote users can call the [startPlayingStream](https://www.zegocloud.com/docs/article/api?doc=Express_Video_SDK_API~ObjectiveC~class~zego-express-engine#start-playing-stream-canvas) interface, pass in the processed data stream streamID, pull video data, and play.

```objc
// Pull real-time stream
[[ZegoExpressEngine sharedEngine] startPlayingStream:@"streamID" canvas:[ZegoCanvas canvasWithView:self.view]];
```

At this point, developers can fully implement real-time adjustment of AI beauty effects while publishing and playing audio/video streams.
