# Display Subtitles

---

This article introduces how to display subtitles during a voice call between a user and an AI agent. As follows:

- User's speech: Stream the user's spoken content as it is being recognized by ASR in real time.
- AI agent's speech: Stream the AI agent's output content as it is being generated by LLM in real time.

<Frame width="50%" height="50%" >
  <img src="https://media-resource.spreading.io/docuo/workspace740/af061ebc6eaf0f12ae9e7f72235bd04e/a0ae8a9fad.png" alt="image.png"/>
</Frame>

## Prerequisites

You should have already integrated the ZEGO Express SDK and the ZEGOCLOUD AI Agent, and implemented a basic voice-call feature following the [Quick Start](./../Quick%20start.mdx) doc.

## Quick Implementation

During voice conversations between users and AI agents, the ZEGOCLOUD AI Agent server sends ASR recognition text and LLM response text via custom messages in the RTC room to the client. By listening for these custom messages, the client can parse the status events and render the UI.

The processing flowchart for RTC room custom messages is as follows:
```mermaid
flowchart TD
    Start([Start]) --> Init[Implement onRecvExperimentalAPI callback and initialize subtitle UI component]
    Init --> ParseMessage[Parse RTC room custom messages]
    ParseMessage --> |Cmd=3| ProcessASR[Process ASR text]
    ParseMessage --> |Cmd=4| ProcessLLM[Process LLM text]
    ProcessASR --> UpdateSubtitles1[Update user subtitles]
    ProcessLLM --> UpdateSubtitles2[Update AI agent subtitles]
    UpdateSubtitles1 --> HandleEndFlags[Clear message cache after message ends]
    UpdateSubtitles2 --> HandleEndFlags[Clear message cache after message ends]
    HandleEndFlags --> End([End])
```

### Listening to Custom Room Messages

:::if{props.platform=undefined}
By listening to the `onRecvExperimentalAPI` callback, the client can obtain custom room messages with `method` as `liveroom.room.on_recive_room_channel_message`. Below is an example of the listener callback code:

```java {1,3,14}
ZegoExpressEngine.getEngine().setEventHandler(new IZegoEventHandler() {
    @Override
    public void onRecvExperimentalAPI(String content) {
        super.onRecvExperimentalAPI(content);
        try {
            // Step 1: Parse the content into a JSONObject
            JSONObject json = new JSONObject(content);

            // Step 2: Check the value of the method field
            if (json.has("method") && json.getString("method")
                .equals("liveroom.room.on_recive_room_channel_message")) {
                // Step 3: Get and parse params
                JSONObject paramsObject = json.getJSONObject("params");
                String msgContent = paramsObject.getString("msg_content");

                // JSON string example: "{\"Timestamp\":1745224717,\"SeqId\":1467995418,\"Round\":2132219714,\"Cmd\":3,\"Data\":{\"MessageId\":\"2135894567\",\"Text\":\"你\",\"EndFlag\":false}}"
                // Parse the JSON string into an AudioChatMessage object
                AudioChatMessage chatMessage = gson.fromJson(msgContent, AudioChatMessage.class);
                if (chatMessage.cmd == 3) {
                    updateASRChatMessage(chatMessage);
                } else if (chatMessage.cmd == 4) {
                    addOrUpdateLLMChatMessage(chatMessage);
                }
            }
        } catch (JSONException e) {
            e.printStackTrace();
        }
    }
});

/**
 * Voice chat UI, structure of chat messages within the room sent by the backend server
 */
public static class AudioChatMessage {
    @SerializedName("Timestamp")
    public long timestamp;
    @SerializedName("SeqId")
    public int seqId;
    @SerializedName("Round")
    public int round;
    @SerializedName("Cmd")
    public int cmd;
    @SerializedName("Data")
    public Data data;
    public static class Data {
        @SerializedName("SpeakStatus")
        public int speakStatus;
        @SerializedName("Text")
        public String text;
        @SerializedName("MessageId")
        public String messageId;
        @SerializedName("EndFlag")
        public boolean endFlag;
    }
}
```
:::
:::if{props.platform="iOS"}
By implementing the `ZegoEventHandler` protocol and listening to the `onRecvExperimentalAPI` callback, the client can obtain room custom messages with `method` as `liveroom.room.on_recive_room_channel_message`. Below is an example of the callback listener code:

<CodeGroup>
```swift YourViewController.h
// Implement ZegoEventHandler protocol
@interface YourService () <ZegoEventHandler>
@property (nonatomic, strong) YourViewController *youViewController;
@end

@implementation YourService

// Handle messages received from express onRecvExperimentalAPI
- (void)onRecvExperimentalAPI:(NSString *)content {
    // Forward to view for message content parsing
    [self.youViewController handleExpressExperimentalAPIContent:content];
}

@end // YourService implementation
```

```swift YourViewController.m
// Implement ZegoEventHandler protocol in the header file
@interface YourViewController () <ZegoEventHandler>

@end

@implementation YourViewController

// Parse custom signaling messages
- (void)handleExpressExperimentalAPIContent:(NSString *)content {
    // Parse JSON content
    NSError *error;
    NSData *jsonData = [content dataUsingEncoding:NSUTF8StringEncoding];
    NSDictionary *contentDict = [NSJSONSerialization JSONObjectWithData:jsonData 
                                                        options:NSJSONReadingMutableContainers 
                                                          error:&error];
    if (error || !contentDict) {
        NSLog(@"JSON parsing failed: %@", error);
        return;
    }
    // Check if it's a room message
    NSString *method = contentDict[@"method"];
    if (![method isEqualToString:@"liveroom.room.on_recive_room_channel_message"]) {
        return;
    }
    // Get message parameters
    NSDictionary *params = contentDict[@"params"];
    if (!params) {
        return;
    }
    NSString *msgContent = params[@"msg_content"];
    NSString *sendIdName = params[@"send_idname"];
    NSString *sendNickname = params[@"send_nickname"];
    NSString *roomId = params[@"roomid"];
    if (!msgContent || !sendIdName || !roomId) {
         NSLog(@"parseExperimentalAPIContent Parameters incomplete: msgContent=%@, sendIdName=%@, roomId=%@",
                msgContent, sendIdName, roomId);
        return;
    }
    
    // JSON string example: "{\"Timestamp\":1745224717,\"SeqId\":1467995418,\"Round\":2132219714,\"Cmd\":3,\"Data\":{\"MessageId\":\"2135894567\",\"Text\":\"你\",\"EndFlag\":false}}"
    // Parse message content
    [self handleMessageContent:msgContent userID:sendIdName userName:sendNickname ?: @""];
}

// Handle message content
- (void)handleMessageContent:(NSString *)command userID:(NSString *)userID userName:(NSString *)userName{
    NSDictionary* msgDict = [self dictFromJson:command];
    if (!msgDict) {
        return;
    }
  
    // Parse basic information
    int cmd = [msgDict[@"Cmd"] intValue];
    int64_t seqId = [msgDict[@"SeqId"] longLongValue];
    int64_t round = [msgDict[@"Round"] longLongValue];
    int64_t timestamp = [msgDict[@"Timestamp"] longLongValue];
    NSDictionary *data = msgDict[@"Data"];
  
    // Handle messages based on command type
    switch (cmd) {
        case 3: // ASR text
            [self handleAsrText:data seqId:seqId round:round timestamp:timestamp];
            break;
        case 4: // LLM text
            [self handleLlmText:data seqId:seqId round:round timestamp:timestamp];
            break;
    }
}

@end // YourViewController implementation
```
</CodeGroup>
:::
:::if{props.platform="Web"}
By listening to the `recvExperimentalAPI` callback, the client can obtain room custom messages with `method` as `onRecvRoomChannelMessage`. Below is an example of the callback listener code:
```javascript {1,3,14}
zg.on("recvExperimentalAPI", (result) => {
  const { method, content } = result;
  if (method === "onRecvRoomChannelMessage") {
    try {
      // Parse the message
      const recvMsg = JSON.parse(content.msgContent);
      const { Cmd, SeqId, Data, Round } = recvMsg;
    } catch (error) {
      console.error("Failed to parse the message:", error);
    }
  }
});
// Enable the experimental API for onRecvRoomChannelMessage
zg.callExperimentalAPI({ method: "onRecvRoomChannelMessage", params: {} });
```
:::
### Room Custom Message Protocol

The fields of the room custom message are described as follows:

| Field | Type | Description |
| --- | --- | --- |
| Timestamp | Number | Timestamp, at the second level |
| SeqId | Number | Packet sequence number, may be out of order. Please sort the messages according to the sequence number. In extreme cases, the Id may not be continuous. |
| Round | Number | Dialogue turn, increases each time the user starts speaking |
| Cmd | Number | <ul><li>3: ASR text.</li><li>4: LLM text.</li></ul> |
| Data | Object | Specific content, different Cmds correspond to different Data |

Data varies depending on the Cmd as follows:

<Tabs>
<Tab title="Cmd is 3">

| Field | Type | Description |
| --- | --- | --- |
| Text | String |  ASR text of user speech.<br />Each issuance is the full text, supporting text correction. |
| MessageId | String | Message ID. It is unique for each turn of ASR text message. |
| EndFlag | Bool | End flag, true indicates that the ASR text of this turn has been processed. |

</Tab>
<Tab title="Cmd is 4">

| Field | Type | Description |
| --- | --- | --- |
| Text | String | LLM text.<br/>Each issuance is incremental text. |
| MessageId | String | Message ID. It is unique for each turn of LLM text message. |
| EndFlag | Bool | End flag, true indicates that the LLM text of this turn has been processed. |

</Tab>
</Tabs>
### Processing Logic

Determine the message type based on the Cmd field, and obtain the message content from the Data field.
<Tabs>
<Tab title="Cmd is 3, ASR Text">

:::if{props.platform="Web"}
```javascript
 // Handle user message
 function handleUserMessage(data, seqId, round) {
    if (data.EndFlag) {
      // User has finished speaking
    }
    const content = data.Text;
    if (content) {
      // Use the ASR text corresponding to the latest seqId as the latest speech recognition result and update the UI
    }
 }
```
:::
:::if{props.platform="iOS"}
```swift
- (void)handleAsrText:(NSDictionary *)data seqId:(int64_t)seqId round:(int64_t)round timestamp:(int64_t)timestamp {
    NSString *content = data[@"Text"];
    NSString *messageId = data[@"MessageId"];
    BOOL endFlag = [data[@"EndFlag"] boolValue];
  
    if (content && content.length > 0) {
        // Process ASR message and update UI
    }
}
```
:::

The corresponding message processing flow is shown in the figure below:

<Frame width="auto" height="auto" >
  <img src="https://media-resource.spreading.io/docuo/workspace740/af061ebc6eaf0f12ae9e7f72235bd04e/5b7718691f.png" alt="image.png"/>
</Frame>

</Tab>
<Tab title="Cmd is 4, LLM Text">
:::if{props.platform="Web"}
```javascript
// Handle agent message
function handleAgentMessage(data, seqId, round) {
  const llmEndFlag = data.EndFlag;
  if (llmEndFlag) {
    // Agent has finished responding
  }
  const llmText = data.Text;
  if (llmText) {
      // Process agent message
  }
}
```
:::
:::if{props.platform="iOS"}
```swift
- (void)handleLlmText:(NSDictionary *)data seqId:(int64_t)seqId round:(int64_t)round timestamp:(int64_t)timestamp {
    NSString *content = data[@"Text"];
    NSString *messageId = data[@"MessageId"];
    BOOL endFlag = [data[@"EndFlag"] boolValue];
  
    if (content && content.length > 0) {
        // Process LLM message and update UI
    }
}
```
:::


The corresponding message processing flow is shown in the figure below. Among them:
- AI message cache: A HashMap, with key as MessageId and value as the new message.
- UI message list: An array containing user messages and AI messages, storing all messages displayed on the UI.

<Frame width="auto" height="auto" >
  <img src="https://media-resource.spreading.io/docuo/workspace740/af061ebc6eaf0f12ae9e7f72235bd04e/e2b7eb3ce5.png" alt="image copy.png"/>
</Frame>

</Tab>
</Tabs>


### Use the subtitle component

:::if{props.platform=undefined}
You can also download the [subtitle component source code](https://github.com/ZEGOCLOUD/ai_agent_quick_start/blob/master/android/QuickStart/app/src/main/java/im/zego/aiagent/express/quickstart/AudioChatMessageParser.java) to your project for use.
<Accordion title="Subtitle Component Usage Example" defaultOpen="false">
```java
private AudioChatMessageParser audioChatMessageParser = new AudioChatMessageParser();

ZegoExpressEngine.getEngine().setEventHandler(new IZegoEventHandler() {
    @Override
    public void onRecvExperimentalAPI(String content) {
        super.onRecvExperimentalAPI(content);
        try {
            // Step 1: Parse content into a JSONObject
            JSONObject json = new JSONObject(content);

            // Step 2: Check the value of the method field
            if (json.has("method") && json.getString("method")
                .equals("liveroom.room.on_recive_room_channel_message")) {
                // Step 3: Get params and parse them
                JSONObject paramsObject = json.getJSONObject("params");
                String msgContent = paramsObject.getString("msg_content");

                // AudioChatTextMessage will parse the JSON string
                audioChatMessageParser.parseAudioChatMessage(msgContent);
            }
        } catch (JSONException e) {
            e.printStackTrace();
        }
    }
});

audioChatMessageParser.setAudioChatMessageListListener(new AudioChatMessageListListener() {
    @Override
    public void onMessageListUpdated(List<AudioChatMessage> messagesList) {
        // Update the UI list
        binding.messageList.onMessageListUpdated(messagesList);
    }
});

```
</Accordion>
:::
:::if{props.platform="iOS"}
You can also download the [subtitle component source code](https://github.com/ZEGOCLOUD/ai_agent_quick_start/tree/master/ios/ai_agent_quickstart/aiagent/audio/subtitles) to your project for use.
<Accordion title="Subtitle Component Usage Example" defaultOpen="false">
<CodeGroup>
```swift YourView.h
#import <UIKit/UIKit.h>
#import "ZegoAIAgentSubtitlesEventHandler.h"

NS_ASSUME_NONNULL_BEGIN

@interface YourView : UIView <ZegoAIAgentSubtitlesEventHandler>

@end
```

```swift YourView.m
#import "YourView.h"

#import <Masonry/Masonry.h>

#import "ZegoAIAgentSubtitlesTableView.h"
#import "ZegoAIAgentSubtitlesMessageDispatcher.h"

@interface YourView()<ZegoAIAgentSubtitlesEventHandler>
// AI Agent Subtitles
@property (nonatomic, strong, readwrite) ZegoAIAgentSubtitlesTableView *chatView;

@end

@implementation YourView

- (instancetype)initWithFrame:(CGRect)frame {
    self = [super initWithFrame:frame];
    if (self) {
        // Register events
        [self registerEventHandler];
        
        [self setupSubtitles];
    }
    return self;
}

- (void)dealloc {
    // Unregister events
    [self unregisterEventHandler];
}

- (void)setupSubtitles {
    // Add chat view - occupies the lower half of the screen
    CGRect chatFrame = CGRectMake(0,
                                 self.bounds.size.height / 2,
                                 self.bounds.size.width,
                                 self.bounds.size.height / 2);
    self.chatView = [[ZegoAIAgentSubtitlesTableView alloc] initWithFrame:chatFrame style:UITableViewStylePlain];
    
    [self addSubview:self.chatView];
    
    // Use Masonry to add constraints
}
    [self.chatView mas_makeConstraints:^(MASConstraintMaker *make) {
        make.left.right.bottom.equalTo(self);
        make.height.equalTo(self.mas_height).multipliedBy(0.5);
    }];
}

#pragma mark - ZegoAIAgentSubtitlesEventHandler

- (void)registerEventHandler {
    [[ZegoAIAgentSubtitlesMessageDispatcher sharedInstance] registerEventHandler:self];
}

- (void)unregisterEventHandler {
    [[ZegoAIAgentSubtitlesMessageDispatcher sharedInstance] unregisterEventHandler:self];
}

- (void)onRecvAsrChatMsg:(ZegoAIAgentAudioSubtitlesMessage *)message {
    [self.chatView handleRecvAsrMessage:message];
}

- (void)onRecvLLMChatMsg:(ZegoAIAgentAudioSubtitlesMessage *)message {
    [self.chatView handleRecvLLMMessage:message];
}

@end 
```
</CodeGroup>
</Accordion>
:::
:::if{props.platform="Web"}
If you are working on a Vue project, you can download the [subtitle component](https://github.com/ZEGOCLOUD/ai_agent_quick_start/blob/master/web/src/hooks/useChat.ts) to your project and use it directly.
<Accordion title="Vue Project Subtitle Component Usage Example" defaultOpen="false">
```javascript
// Example code for using the subtitle component
// Import the chatHook in your page
import { useChat } from "useChat";
import { onMounted, onBeforeUnmount } from 'vue';

// Call the useChat method, pass in the Express SDK instance. The messages will be rendered in your subtitle component.
const { messages, setupEventListeners, clearMessages } = useChat(zg);

onMounted(() => {
  // Register event listeners when the page loads
  setupEventListeners()
})

onBeforeUnmount(() => {
  // Clear messages when the page is destroyed
  clearMessages()
})```

```
</Accordion>
:::

## Precautions

- Message Sorting Processing: The data received through custom room messages may be out of order, and sorting needs to be performed based on the SeqId field.
- Streaming Text Processing:
> - Each ASR text sent is the full text. Messages with the same MessageId should completely replace the previous content.
> - Each LLM text sent is incremental. Messages with the same MessageId need to be cumulatively displayed after sorting.
- Memory Management: Please clear the cache of completed messages in time, especially when users engage in long conversations.