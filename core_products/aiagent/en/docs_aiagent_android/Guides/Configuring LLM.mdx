# Configuring LLM

---

To adapt to different scenarios, you may need to choose different Large Language Model (LLM) providers, including Volcano DouBao, MiniMax, Alibaba Tongyi Qwen, JieYue XingChen, DeepSeek, etc., or even further use a completely self-developed LLM. This article explains how to configure common large language model vendors and related precautions.

## Using Third-party LLMs

<Note title="Note">
Please contact ZEGOCLOUD Technical Support first to activate third-party LLM services and obtain the access Url and API Key.

Third-party LLMs must be compatible with the OpenAI protocol.
</Note>

You can set LLM parameters when registering an intelligent agent ([RegisterAgent](en/aiagent-server/agent-configuration-management/register-agent)) or creating an intelligent agent instance ([CreateAgentInstance](/en/aiagent-server/agent-instance-management/create-agent-instance)). Common parameters are as follows:

| Parameter | Type | Required | Description |
| --- | --- | --- | --- |
| Url | String | Yes | LLM callback address, which must be compatible with the OpenAI protocol. |
| ApiKey | String | No | LLM verification api key. Default is empty; in production environments, it is essential to enable the api key. |
| Model | String | Yes | The invoked model. Different LLM suppliers support different configurations; please refer to the corresponding documentation for input. |
| SystemPrompt | String | No | System prompt. It can include role settings, prompts, and response examples, etc. |
| Temperature | Float | No | Higher values will make the output more random, while lower values will make the output more focused and certain. |
| TopP | Float | No | Sampling method; smaller values result in more deterministic results, while larger values produce more random results. |
| Params | Object | No | Other LLM parameters, such as the maximum token number limit used, etc. Different LLM suppliers support different configurations; please refer to the corresponding documentation and fill in as needed.<Note title="Note">Parameter names should match those of each vendor's LLM.</Note> |

Here are configuration examples for common LLM vendors:

<Tabs>
<Tab title="Volcano Ark">
[Volcano Ark Large Model Service Platform](https://www.volcengine.com/docs/82379/1298454) model usage instructions document.
```json
"LLM": {
    "Url": "https://ark.cn-beijing.volces.com/api/v3/chat/completions",
    "ApiKey": "your_api_key",
    "Model": "ep-xxxxxxxxxx",    // Your inference access point created on the Volcano Ark Large Model Platform
    "SystemPrompt": "You are Xiao Zhi, an adult female, a **companion assistant created by ZegoCLOUD Technology**, knowledgeable in astronomy and geography, smart, wise, enthusiastic, and friendly.\nDialogue requirements: 1. Interact with users according to the character requirements.\n2. Do not exceed 100 words.",
    "Temperature": 1,
    "TopP": 0.7,
    "Params": {
        "max_tokens": 1024
    }
}
```
</Tab>
<Tab title="Alibaba Cloud ">
[Input and output parameters of the Tongyi Qwen API](https://bailian.console.aliyun.com/?tab=api#/api/?type=model&url=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F2712576.html) Model usage documentation.
```json
"LLM": {
    "Url": "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions",
    "ApiKey": "your_api_key",
    "Model": "qwen-plus",
    "SystemPrompt": "You are Xiaozhi, an adult woman, a companion assistant **created by ZEGOCLOUD**. knowledgeable in both astronomy and geography, intelligent, wise, enthusiastic, and friendly. \nDialogue requirements: 1. Dialogue with users according to the requirements of the persona. \n2.No more than 100 words.",
    "Temperature": 1,
    "TopP": 0.7,
    "Params": {
        "max_tokens": 1024
    }
}
```
</Tab>
<Tab title="Minimax">
[Minimax](https://platform.minimaxi.com/document/ChatCompletion%20v2?key=66701d281d57f38758d581d0#QklxsNSbaf6kM4j6wjO5eEek) model usage documentation.
```json
"LLM": {
    "Url": "https://api.minimax.chat/v1/text/chatcompletion_v2",
    "ApiKey": "your_api_key",
    "Model": "MiniMax-Text-01",
    "SystemPrompt": "You are Xiaozhi, an adult woman, a companion assistant **created by ZEGOCLOUD**. knowledgeable in both astronomy and geography, intelligent, wise, enthusiastic, and friendly. \nDialogue requirements: 1. Dialogue with users according to the requirements of the persona. \n2.No more than 100 words.",
    "Temperature": 1,
    "TopP": 0.7,
    "Params": {
        "max_tokens": 1024
    }
}
```
</Tab>
</Tabs>

## Use Custom LLM

The AI Agent server uses the OpenAI API protocol to call LLM services. Therefore, you can also use a custom large language model compatible with the OpenAI protocol. The following introduces the implementation method.

<Steps>
<Step title="Create a service that complies with the OpenAI API protocol">
Provide an interface compatible with [platform.openai.com](https://platform.openai.com/docs/api-reference/chat). Key points are as follows:

- Interface Path: Define a Url that can be called by the AI Agent, for example https://your-custom-llm-service/chat/completions.
- Request Format: Accept request parameters compatible with the OpenAI protocol.
- Response Format: Return streaming response data that is compatible with the OpenAI protocol and conforms to the SSE specification.
</Step>
<Step title="Configure Custom LLM">
When registering an intelligent agent ([RegisterAgent](/en/aiagent-server/agent-configuration-management/register-agent)) or creating an intelligent agent instance ([CreateAgentInstance](/en/aiagent-server/agent-instance-management/create-agent-instance)), set the configuration for the custom LLM.

```json
"LLM": {
    "Url": "https://your-custom-llm-service/chat/completions",
    "ApiKey": "your_api_key",
    "Model": "your_model",
    "SystemPrompt": "You are Xiaozhi, an adult woman, a companion assistant **created by ZEGOCLOUD**. knowledgeable in both astronomy and geography, intelligent, wise, enthusiastic, and friendly. \nDialogue requirements: 1. Dialogue with users according to the requirements of the persona. \n2.No more than 100 words.",
    "Temperature": 1,
    "TopP": 0.7,
    "Params": {
        "max_tokens": 1024
    }
}

```
</Step>
</Steps>

