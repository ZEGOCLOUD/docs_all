# Display Subtitles

---

This article introduces how to display subtitles during the process of a user having a voice call with an agent. As follows:

- User speech content: Streaming display of what the user is saying (real-time results of Speech Recognition (ASR))
- Agent speech content: Streaming display of the agent's output content (real-time output results of Large Language Models (LLM))

<Frame width="50%" height="50%" >
  <img src="https://media-resource.spreading.io/docuo/workspace740/af061ebc6eaf0f12ae9e7f72235bd04e/a0ae8a9fad.png" alt="image.png"/>
</Frame>

## Prerequisites

The ZEGO Express SDK and AI Agent have been integrated and basic voice call functionality has been implemented according to the [Quick Start](./../Quick%20start.mdx) document.

## Quick Implementation

During voice conversations between users and agents, the AI Agent server sends down ASR recognition text and LLM response text via custom messages in the RTC room. The client can listen for custom room messages and parse the corresponding status events to render the UI.

The processing flow for RTC room custom messages is as follows:
```mermaid
flowchart TD
    Start([Start]) --> Init[Implement onRecvExperimentalAPI callback and initialize subtitle UI component]
    Init --> ParseMessage[Parse RTC room custom messages]
    ParseMessage --> |Cmd=3| ProcessASR[Process ASR text]
    ParseMessage --> |Cmd=4| ProcessLLM[Process LLM text]
    ProcessASR --> UpdateSubtitles1[Update user subtitles]
    ProcessLLM --> UpdateSubtitles2[Update agent subtitles]
    UpdateSubtitles1 --> HandleEndFlags[Clear message cache after message ends]
    UpdateSubtitles2 --> HandleEndFlags[Clear message cache after message ends]
    HandleEndFlags --> End([End])
```

### Listening to Custom Room Messages

:::if{props.platform=undefined}
The client can obtain custom room messages with `method` as `liveroom.room.on_recive_room_channel_message` by listening to the `onRecvExperimentalAPI` callback. Below is an example of the listener callback code:

```java {1,3,14}
ZegoExpressEngine.getEngine().setEventHandler(new IZegoEventHandler() {
    @Override
    public void onRecvExperimentalAPI(String content) {
        super.onRecvExperimentalAPI(content);
        try {
            // Step 1: Parse the content into a JSONObject
            JSONObject json = new JSONObject(content);

            // Step 2: Check the value of the method field
            if (json.has("method") && json.getString("method")
                .equals("liveroom.room.on_recive_room_channel_message")) {
                // Step 3: Get and parse params
                JSONObject paramsObject = json.getJSONObject("params");
                String msgContent = paramsObject.getString("msg_content");

                // JSON string example: "{\"Timestamp\":1745224717,\"SeqId\":1467995418,\"Round\":2132219714,\"Cmd\":3,\"Data\":{\"MessageId\":\"2135894567\",\"Text\":\"你\",\"EndFlag\":false}}"
                // Parse the JSON string into an AudioChatMessage object
                AudioChatMessage chatMessage = gson.fromJson(msgContent, AudioChatMessage.class);
                if (chatMessage.cmd == 3) {
                    updateASRChatMessage(chatMessage);
                } else if (chatMessage.cmd == 4) {
                    addOrUpdateLLMChatMessage(chatMessage);
                }
            }
        } catch (JSONException e) {
            e.printStackTrace();
        }
    }
});

/**
 * Voice chat interface, structure of chat messages within the room sent by the backend server
 */
public static class AudioChatMessage {
    @SerializedName("Timestamp")
    public long timestamp;
    @SerializedName("SeqId")
    public int seqId;
    @SerializedName("Round")
    public int round;
    @SerializedName("Cmd")
    public int cmd;
    @SerializedName("Data")
    public Data data;
    public static class Data {
        @SerializedName("SpeakStatus")
        public int speakStatus;
        @SerializedName("Text")
        public String text;
        @SerializedName("MessageId")
        public String messageId;
        @SerializedName("EndFlag")
        public boolean endFlag;
    }
}
```
:::
:::if{props.platform="iOS"}
The client can obtain room custom messages with `method` as `liveroom.room.on_recive_room_channel_message` by implementing the `ZegoEventHandler` protocol and listening to the `onRecvExperimentalAPI` callback. Below is an example of the callback listener code:
<CodeGroup>
```swift YourViewController.h
// Implement ZegoEventHandler protocol
@interface YourService () <ZegoEventHandler>
@property (nonatomic, strong) YourViewController *youViewController;
@end

@implementation YourService

// Handle messages received from express onRecvExperimentalAPI
- (void)onRecvExperimentalAPI:(NSString *)content {
    // Forward to view for message content parsing
    [self.youViewController handleExpressExperimentalAPIContent:content];
}

@end // YourService implementation
```

```swift YourViewController.m
// Implement ZegoEventHandler protocol in the header file
@interface YourViewController () <ZegoEventHandler>

@end

@implementation YourViewController

// Parse custom signaling messages
- (void)handleExpressExperimentalAPIContent:(NSString *)content {
    // Parse JSON content
    NSError *error;
    NSData *jsonData = [content dataUsingEncoding:NSUTF8StringEncoding];
    NSDictionary *contentDict = [NSJSONSerialization JSONObjectWithData:jsonData 
                                                        options:NSJSONReadingMutableContainers 
                                                          error:&error];
    if (error || !contentDict) {
        NSLog(@"JSON parsing failed: %@", error);
        return;
    }
    // Check if it's a room message
    NSString *method = contentDict[@"method"];
    if (![method isEqualToString:@"liveroom.room.on_recive_room_channel_message"]) {
        return;
    }
    // Get message parameters
    NSDictionary *params = contentDict[@"params"];
    if (!params) {
        return;
    }
    NSString *msgContent = params[@"msg_content"];
    NSString *sendIdName = params[@"send_idname"];
    NSString *sendNickname = params[@"send_nickname"];
    NSString *roomId = params[@"roomid"];
    if (!msgContent || !sendIdName || !roomId) {
         NSLog(@"parseExperimentalAPIContent Parameters incomplete: msgContent=%@, sendIdName=%@, roomId=%@",
                msgContent, sendIdName, roomId);
        return;
    }
    
    // JSON string example: "{\"Timestamp\":1745224717,\"SeqId\":1467995418,\"Round\":2132219714,\"Cmd\":3,\"Data\":{\"MessageId\":\"2135894567\",\"Text\":\"你\",\"EndFlag\":false}}"
    // Parse message content
    [self handleMessageContent:msgContent userID:sendIdName userName:sendNickname ?: @""];
}

// Handle message content
- (void)handleMessageContent:(NSString *)command userID:(NSString *)userID userName:(NSString *)userName{
    NSDictionary* msgDict = [self dictFromJson:command];
    if (!msgDict) {
        return;
    }
  
    // Parse basic information
    int cmd = [msgDict[@"Cmd"] intValue];
    int64_t seqId = [msgDict[@"SeqId"] longLongValue];
    int64_t round = [msgDict[@"Round"] longLongValue];
    int64_t timestamp = [msgDict[@"Timestamp"] longLongValue];
    NSDictionary *data = msgDict[@"Data"];
  
    // Handle messages based on command type
    switch (cmd) {
        case 3: // ASR text
            [self handleAsrText:data seqId:seqId round:round timestamp:timestamp];
            break;
        case 4: // LLM text
            [self handleLlmText:data seqId:seqId round:round timestamp:timestamp];
            break;
    }
}

@end // YourViewController implementation
```
</CodeGroup>
:::
:::if{props.platform="Web"}
The client can obtain room custom messages with `method` as `onRecvRoomChannelMessage` by listening to the `recvExperimentalAPI` callback. Below is an example of the callback listener code:
```javascript {1,3,14}
zg.on("recvExperimentalAPI", (result) => {
  const { method, content } = result;
  if (method === "onRecvRoomChannelMessage") {
    try {
      // Parse the message
      const recvMsg = JSON.parse(content.msgContent);
      const { Cmd, SeqId, Data, Round } = recvMsg;
    } catch (error) {
      console.error("Failed to parse the message:", error);
    }
  }
});
// Enable the experimental API for onRecvRoomChannelMessage
zg.callExperimentalAPI({ method: "onRecvRoomChannelMessage", params: {} });

```
:::
### Room Custom Message Protocol

The fields of the room custom message are described as follows:

| Field | Type | Description |
| --- | --- | --- |
| Timestamp | int64 | Timestamp, at the second level |
| SeqId | int64 | Packet sequence number, may be out of order. Please sort the messages according to the sequence number. In extreme cases, the Id may not be continuous. |
| Round | int64 | Dialogue round, increases each time the user initiates speaking |
| Cmd | int | 3: Text from speech recognition (ASR)<br/>4: LLM text |
| Data | Object | Specific content, different Cmds correspond to different Data |

Data varies depending on the Cmd, specifically as follows:

<Tabs>
<Tab title="Cmd is 3">

| Field | Type | Description |
| --- | --- | --- |
| Text | string | User voice recognition (ASR) text<br/>Each issuance is the full text, supporting text correction |
| MessageId | string | Message id, each round of ASR text message id is unique |
| EndFlag | bool | End flag, true indicates that the ASR text of this round has been processed |

</Tab>
<Tab title="Cmd is 4">

| Field | Type | Description |
| --- | --- | --- |
| Text | string | LLM text<br/>Each issuance is incremental text |
| MessageId | string | Message id, each round of LLM text message id is unique |
| EndFlag | bool | End flag, true indicates that the LLM text of this round has been processed |

</Tab>
</Tabs>
### Processing Logic

Determine the message type based on the Cmd field, and obtain the message content from the Data field.
<Tabs>
<Tab title="Cmd is 3, ASR Text">

:::if{props.platform="Web"}
```javascript
 // Handle user message
 function handleUserMessage(data, seqId, round) {
    if (data.EndFlag) {
      // User has finished speaking
    }
    const content = data.Text;
    if (content) {
      // Use the ASR text corresponding to the latest seqId as the latest speech recognition result and update the UI
    }
 }
```
:::
:::if{props.platform="iOS"}
```swift
- (void)handleAsrText:(NSDictionary *)data seqId:(int64_t)seqId round:(int64_t)round timestamp:(int64_t)timestamp {
    NSString *content = data[@"Text"];
    NSString *messageId = data[@"MessageId"];
    BOOL endFlag = [data[@"EndFlag"] boolValue];
  
    if (content && content.length > 0) {
        // Process ASR message and update UI
    }
}
```
:::

The corresponding message processing flow is shown in the figure below:

<Frame width="auto" height="auto" >
  <img src="https://media-resource.spreading.io/docuo/workspace740/af061ebc6eaf0f12ae9e7f72235bd04e/5b7718691f.png" alt="image.png"/>
</Frame>

</Tab>
<Tab title="Cmd is 4, LLM Text">
:::if{props.platform="Web"}
```javascript
// Handle agent message
function handleAgentMessage(data, seqId, round) {
  const llmEndFlag = data.EndFlag;
  if (llmEndFlag) {
    // Agent has finished responding
  }
  const llmText = data.Text;
  if (llmText) {
      // Process agent message
  }
}
```
:::
:::if{props.platform="iOS"}
```swift
- (void)handleLlmText:(NSDictionary *)data seqId:(int64_t)seqId round:(int64_t)round timestamp:(int64_t)timestamp {
    NSString *content = data[@"Text"];
    NSString *messageId = data[@"MessageId"];
    BOOL endFlag = [data[@"EndFlag"] boolValue];
  
    if (content && content.length > 0) {
        // Process LLM message and update UI
    }
}
```
:::


The corresponding message processing flow is shown in the figure below. Among them:
- AI message cache: A HashMap, with key as MessageId and value as the new message.
- UI message list: An array containing user messages and AI messages, storing all messages displayed on the UI.

<Frame width="auto" height="auto" >
  <img src="https://media-resource.spreading.io/docuo/workspace740/af061ebc6eaf0f12ae9e7f72235bd04e/e2b7eb3ce5.png" alt="image copy.png"/>
</Frame>

</Tab>
</Tabs>


### Use the subtitle component

:::if{props.platform=undefined}
You can also directly download the [subtitle processing class source code](https://github.com/ZEGOCLOUD /ai_agent_quick_start/blob/master/android/QuickStart/app/src/main/java/im/zego/aiagent/express/quickstart/AudioChatMessageParser.java) to your project for direct use.
<Accordion title="Subtitle Processing Class Usage Example" defaultOpen="false">
```java
private AudioChatMessageParser audioChatMessageParser = new AudioChatMessageParser();

ZegoExpressEngine.getEngine().setEventHandler(new IZegoEventHandler() {
    @Override
    public void onRecvExperimentalAPI(String content) {
        super.onRecvExperimentalAPI(content);
        try {
            // Step 1: Parse content into a JSONObject
            JSONObject json = new JSONObject(content);

            // Step 2: Check the value of the method field
            if (json.has("method") && json.getString("method")
                .equals("liveroom.room.on_recive_room_channel_message")) {
                // Step 3: Get params and parse them
                JSONObject paramsObject = json.getJSONObject("params");
                String msgContent = paramsObject.getString("msg_content");

                // AudioChatTextMessage will parse the JSON string
                audioChatMessageParser.parseAudioChatMessage(msgContent);
            }
        } catch (JSONException e) {
            e.printStackTrace();
        }
    }
});

audioChatMessageParser.setAudioChatMessageListListener(new AudioChatMessageListListener() {
    @Override
    public void onMessageListUpdated(List<AudioChatMessage> messagesList) {
        // Update the UI list
        binding.messageList.onMessageListUpdated(messagesList);
    }
});

```
</Accordion>
:::
:::if{props.platform="iOS"}
You can also directly download the [subtitle component source code](https://github.com/ZEGOCLOUD /ai_agent_quick_start/tree/master/ios/ai_agent_quickstart/aiagent/audio/subtitles) to your project for direct use.
<Accordion title="Subtitle Group Usage Example" defaultOpen="false">
<CodeGroup>
```swift YourView.h
#import <UIKit/UIKit.h>
#import "ZegoAIAgentSubtitlesEventHandler.h"

NS_ASSUME_NONNULL_BEGIN

@interface YourView : UIView <ZegoAIAgentSubtitlesEventHandler>

@end
```

```swift YourView.m
#import "YourView.h"

#import <Masonry/Masonry.h>

#import "ZegoAIAgentSubtitlesTableView.h"
#import "ZegoAIAgentSubtitlesMessageDispatcher.h"

@interface YourView()<ZegoAIAgentSubtitlesEventHandler>
// AI Agent Subtitles
@property (nonatomic, strong, readwrite) ZegoAIAgentSubtitlesTableView *chatView;

@end

@implementation YourView

- (instancetype)initWithFrame:(CGRect)frame {
    self = [super initWithFrame:frame];
    if (self) {
        // Register events
        [self registerEventHandler];
        
        [self setupSubtitles];
    }
    return self;
}

- (void)dealloc {
    // Unregister events
    [self unregisterEventHandler];
}

- (void)setupSubtitles {
    // Add chat view - occupies the lower half of the screen
    CGRect chatFrame = CGRectMake(0,
                                 self.bounds.size.height / 2,
                                 self.bounds.size.width,
                                 self.bounds.size.height / 2);
    self.chatView = [[ZegoAIAgentSubtitlesTableView alloc] initWithFrame:chatFrame style:UITableViewStylePlain];
    
    [self addSubview:self.chatView];
    
    // Use Masonry to add constraints
}
    [self.chatView mas_makeConstraints:^(MASConstraintMaker *make) {
        make.left.right.bottom.equalTo(self);
        make.height.equalTo(self.mas_height).multipliedBy(0.5);
    }];
}

#pragma mark - ZegoAIAgentSubtitlesEventHandler

- (void)registerEventHandler {
    [[ZegoAIAgentSubtitlesMessageDispatcher sharedInstance] registerEventHandler:self];
}

- (void)unregisterEventHandler {
    [[ZegoAIAgentSubtitlesMessageDispatcher sharedInstance] unregisterEventHandler:self];
}

- (void)onRecvAsrChatMsg:(ZegoAIAgentAudioSubtitlesMessage *)message {
    [self.chatView handleRecvAsrMessage:message];
}

- (void)onRecvLLMChatMsg:(ZegoAIAgentAudioSubtitlesMessage *)message {
    [self.chatView handleRecvLLMMessage:message];
}

@end 
```
</CodeGroup>
</Accordion>
:::
:::if{props.platform="Web"}
If you are working on a Vue project, you can directly download the [subtitle processing hook](https://github.com/ZEGOCLOUD /ai_agent_quick_start/blob/master/web/src/hooks/useChat.ts) to your project and use it directly.
<Accordion title="Vue Project Subtitle Processing Hook Usage Example" defaultOpen="false">
```javascript
// Example code for using the subtitle component
// Import the chatHook in your page
import { useChat } from "useChat";
import { onMounted, onBeforeUnmount } from 'vue';

// Call the useChat method, pass in the Express SDK instance. The messages will be rendered in your subtitle component.
const { messages, setupEventListeners, clearMessages } = useChat(zg);

onMounted(() => {
  // Register event listeners when the page loads
  setupEventListeners()
})

onBeforeUnmount(() => {
  // Clear messages when the page is destroyed
  clearMessages()
})```

```
</Accordion>
:::
## Precautions

- Message Sorting Processing: The data received through custom room messages may be out of order, and sorting needs to be performed based on the SeqId field.
- Streaming Text Processing:
> - Each ASR text sent is the full text. Messages with the same MessageId should completely replace the previous content.
> - Each LLM text sent is incremental. Messages with the same MessageId need to be cumulatively displayed after sorting.
- Memory Management: Please clear the cache of completed messages in time, especially when users engage in long conversations.