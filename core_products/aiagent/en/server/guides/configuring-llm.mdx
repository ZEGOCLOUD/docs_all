# Configuring LLM

---

Depending on your use case, you can plug in any third-party LLM‚Äîwhether it's Volcano Ark, MiniMax, Qwen, Stepfun, DeepSeek, or your own in-house model. This guide walks you through configuring for the above kinds of LLMs and highlights key considerations.

## LLM Parameter Description

When using third-party LLM services or custom LLM services, you need to configure LLM parameters.

| Parameter       | Type            | Required | Description                                                                                                                                                  |
|-----------------|-----------------|----------|--------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Url             | String          | Yes      | LLM callback address, which must be compatible with the OpenAI protocol.                                                                                    |
| ApiKey          | String          | No       | Authentication credentials for accessing various models and related services provided by LLM.                                                               |
| Model           | String          | Yes      | The model to call. Different LLM service providers support different configurations, please refer to the corresponding documentation.                      |
| SystemPrompt    | String          | No       | System prompt. Can include role settings, prompts, and response examples.                                                                                   |
| Temperature     | Float           | No       | Higher values will make the output more random, while lower values will make the output more focused and deterministic.                                     |
| TopP            | Float           | No       | Sampling method, smaller values result in stronger determinism; larger values result in more randomness.                                                    |
| Params          | Object          | No       | Other LLM parameters, such as maximum Token number limit, etc. Different LLM providers support different configurations, please refer to the corresponding documentation and fill in as needed. <Note title="Note">Parameter names should match those of each vendor's LLM.</Note> |
| AddAgentInfo    | Bool            | No       | If this value is true, when the AI Agent backend sends requests to custom LLM services, the request parameters will include agent information `agent_info`. This value defaults to false. When using custom LLM, additional business logic can be implemented based on this parameter content. |

## Using Third-party LLMs

<Note title="Note">
Please contact ZEGOCLOUD Technical Support first to activate third-party LLM services and obtain the access Url and API Key.

Third-party LLMs must be compatible with the OpenAI protocol.
</Note>

You can set LLM parameters when registering an AI agent ([RegisterAgent](./../api-reference/agent-configuration-management/register-agent.mdx)) or creating an AI agent instance ([CreateAgentInstance](./../api-reference/agent-instance-management/create-agent-instance.mdx)).

Here are configuration samples for common LLM vendors:

<Tabs>
<Tab title="Volcano Ark">
For model usage docs, read [Volcano Ark Large Model Service Platform](https://www.volcengine.com/docs/82379/1298454).
```json
"LLM": {
    "Url": "POST https://ark.ap-southeast.bytepluses.com/api/v3/chat/completions",
    "ApiKey": "zego_test", // your api key (zego_test can be used during the integration testing period (within 2 weeks of AI Agent service activation))
    "Model": "doubao-lite-32k-240828",    // Your inference access point created on the Volcano Ark Large Model Platform
    "SystemPrompt": "You are Xiao Zhi, an adult female, a **companion assistant created by ZEGOCLOUD Technology**, knowledgeable in astronomy and geography, smart, wise, enthusiastic, and friendly.\nDialogue requirements: 1. Interact with users according to the character requirements.\n2. Do not exceed 100 words.",
    "Temperature": 1,
    "TopP": 0.7,
    "Params": {
        "max_tokens": 16384
    }
}
```
</Tab>
<Tab title="Qwen">
For model usage docs, read [Alibaba Cloud Model Studio - OpenAI compatibility - Chat](https://www.alibabacloud.com/help/en/model-studio/compatibility-of-openai-with-dashscope).
```json
"LLM": {
    "Url": "https://dashscope-intl.aliyuncs.com/compatible-mode/v1/chat/completions",
    "ApiKey": "zego_test", // your api key (zego_test can be used during the integration testing period (within 2 weeks of AI Agent service activation))
    "Model": "qwen-plus",
    "SystemPrompt": "You are Xiaozhi, an adult woman, a companion assistant **created by ZEGOCLOUD**. knowledgeable in everything, intelligent, wise, enthusiastic, and friendly. \nDialogue requirements: 1. Dialogue with users according to the requirements of the persona. \n2.No more than 100 words.",
    "Temperature": 1,
    "TopP": 0.7,
    "Params": {
        "max_tokens": 16384
    }
}
```
</Tab>
<Tab title="MiniMax">
For model usage docs, read [MiniMax - Chat Completion - API](https://www.minimax.io/platform/document/ChatCompletion%20v2?key=66701d281d57f38758d581d0#QklxsNSbaf6kM4j6wjO5eEek).

```json
"LLM": {
    "Url": "https://api.minimax.chat/v1/text/chatcompletion_v2",
    "ApiKey": "zego_test", // your api key (zego_test can be used during the integration testing period (within 2 weeks of AI Agent service activation))
    "Model": "MiniMax-Text-01",
    "SystemPrompt": "You are Xiaozhi, an adult woman, a companion assistant **created by ZEGOCLOUD**. knowledgeable in everything, intelligent, wise, enthusiastic, and friendly. \nDialogue requirements: 1. Dialogue with users according to the requirements of the persona. \n2.No more than 100 words.",
    "Temperature": 1,
    "TopP": 0.7,
    "Params": {
        "max_tokens": 16384
    }
}
```
</Tab>
</Tabs>

## Use Custom LLM

The ZEGOCLOUD AI Agent server uses the OpenAI API protocol to call LLM services. Therefore, you can also use any custom LLM compatible with the OpenAI protocol. The custom LLM can even call multiple sub-LLM models or perform RAG search and web search before integrating and outputting results at the underlying implementation level.

<Steps titleSite="h3">

<Step title="Implement Custom LLM" titleSize="h3">
Create an interface that conforms to the OpenAI API protocol.

<Tabs>
<Tab title="Interface Key Points">
Provide a `chat/completions` interface compatible with [platform.openai.com](https://platform.openai.com/docs/api-reference/chat). The key points are as follows:

- Interface path: The URL that can be called by AI Agent, such as `https://your-custom-llm-service/chat/completions`.
- Request format: Accept request headers and request bodies compatible with the OpenAI protocol.
- Response format: Return streaming response data compatible with the OpenAI protocol and conforming to the SSE specification.


<Accordion title="AI Agent backend request body example code to the chat/completions interface" defaultOpen="false">
```json
{
    "model": "your model name", // Corresponding to LLM.Model parameter
    "temperature": 1, // Corresponding to LLM.Temperature parameter
    "top_p": 0.7, // Corresponding to LLM.TopP parameter
    "max_tokens": 16384, // Corresponding to LLM.Params.max_tokens parameter
    "messages":[
        {
            "role": "system",
            "content": "Please answer the user's question in a friendly manner based on the knowledge base content provided by the user. If the user's question is not in the knowledge base, please politely tell the user that we do not have related knowledge base content." // Corresponding to LLM.SystemPrompt parameter
        },
        ... // Other messages
    ],
    ... // Other parameters
    // If the LLM.AddAgentInfo parameter is true, it will contain agent_info information
    "agent_info": {
        "room_id": "Room id",
        "agent_instance_id" : "Agent instance id",
        "agent_user_id" : "Agent user id",
        "user_id": "User id",
        "round_id": 1, // Round id
        "time_stamp": 193243200 // Millisecond level timestamp
    }
}
```
</Accordion>

<Accordion title="Chat Completion Streaming Response Object Block Example" defaultOpen="false">
```json
data: {"id":"d7ae7c4a-1524-4fe5-9d58-e4d59b89d8f0","object":"chat.completion.chunk","created":1709899323,"model":"step-1-8k","choices":[{"index":0,"delta":{"role":"","content":"ÊÇ®"},"finish_reason":""}],"usage":{"prompt_tokens":83,"completion_tokens":1,"total_tokens":84}}
data: {"id":"d7ae7c4a-1524-4fe5-9d58-e4d59b89d8f0","object":"chat.completion.chunk","created":1709899323,"model":"step-1-8k","choices":[{"index":0,"delta":{"role":"","content":"Â•Ω"},"finish_reason":""}],"usage":{"prompt_tokens":83,"completion_tokens":2,"total_tokens":85}}
data: {"id":"d7ae7c4a-1524-4fe5-9d58-e4d59b89d8f0","object":"chat.completion.chunk","created":1709899323,"model":"step-1-8k","choices":[{"index":0,"delta":{"role":"","content":"ÔºÅ"},"finish_reason":""}],"usage":{"prompt_tokens":83,"completion_tokens":3,"total_tokens":86}}
data: {"id":"d7ae7c4a-1524-4fe5-9d58-e4d59b89d8f0","object":"chat.completion.chunk","created":1709899323,"model":"step-1-8k","choices":[{"index":0,"delta":{"role":"","content":"Âç≥"},"finish_reason":""}],"usage":{"prompt_tokens":83,"completion_tokens":4,"total_tokens":87}}
data: {"id":"d7ae7c4a-1524-4fe5-9d58-e4d59b89d8f0","object":"chat.completion.chunk","created":1709899323,"model":"step-1-8k","choices":[{"index":0,"delta":{"role":"","content":"ÊûÑ"},"finish_reason":""}],"usage":{"prompt_tokens":83,"completion_tokens":5,"total_tokens":88}}
...
data: {"id":"d7ae7c4a-1524-4fe5-9d58-e4d59b89d8f0","object":"chat.completion.chunk","created":1709899323,"model":"step-1-8k","choices":[{"index":0,"delta":{"role":"","content":"Êõ¥Â§öÁöÑ"},"finish_reason":""}],"usage":{"prompt_tokens":83,"completion_tokens":147,"total_tokens":230}}
data: {"id":"d7ae7c4a-1524-4fe5-9d58-e4d59b89d8f0","object":"chat.completion.chunk","created":1709899323,"model":"step-1-8k","choices":[{"index":0,"delta":{"role":"","content":"‰ª∑ÂÄº"},"finish_reason":""}],"usage":{"prompt_tokens":83,"completion_tokens":148,"total_tokens":231}}
data: {"id":"d7ae7c4a-1524-4fe5-9d58-e4d59b89d8f0","object":"chat.completion.chunk","created":1709899323,"model":"step-1-8k","choices":[{"index":0,"delta":{"role":"","content":"„ÄÇ"},"finish_reason":""}],"usage":{"prompt_tokens":83,"completion_tokens":149,"total_tokens":232}}
data: {"id":"d7ae7c4a-1524-4fe5-9d58-e4d59b89d8f0","object":"chat.completion.chunk","created":1709899323,"model":"step-1-8k","choices":[{"index":0,"delta":{"role":"","content":""},"finish_reason":""}],"usage":{"prompt_tokens":83,"completion_tokens":150,"total_tokens":233}}
data: {"id":"d7ae7c4a-1524-4fe5-9d58-e4d59b89d8f0","object":"chat.completion.chunk","created":1709899323,"model":"step-1-8k","choices":[{"index":0,"delta":{"role":"","content":""},"finish_reason":"stop"}],"usage":{"prompt_tokens":83,"completion_tokens":150,"total_tokens":233}}
data: [DONE]
```
</Accordion>
<Warning title="Warning">
The custom LLM streaming data format notes are as follows:
- Each line of data must start with `data: ` (note: there is a space after the colon).
- Each line of data is a single line or has a line break at the end.
- The last valid data must contain `"finish_reason":"stop"`.
- Finally, a termination data must be sent: `data: [DONE]`.

If the format is incorrect, the AI Agent may not output or output incomplete.
</Warning>
</Tab>
<Tab title="Process and Example">

<Note title="Note">Here, we take a common knowledge base query as an example to demonstrate the implementation process of defining LLM. You can modify the implementation logic according to your actual business needs.</Note>


<Steps titleSite="p">
<Step title="Parse Request Parameters">
Parse the request parameters and get the necessary information.
```js
export async function POST(request: NextRequest) {
    try {
        // !mark
        const requestData: ChatCompletionCreateParams = await request.json();
        console.log("requestData", requestData);
        // Check necessary fields
        if (!requestData.messages || requestData.messages.length === 0) {
            return NextResponse.json(
                { error: 'Messages are required' },
                { status: 400 }
            );
        }
        // Read the latest User Message (the latest is at the end of the array)
        // When AIAgent requests your interface, it will carry the Messages parameter. This parameter also includes SystemPrompt.
        // !mark
        const latestUserMessage = [...requestData.messages].reverse().find(message => message.role === 'user');

        // ... other code
    } catch (error) {
        // ... other code
    }
}
```
</Step>
<Step title="Retrieve from Knowledge Base">
Retrieve the knowledge base based on the latest User Message.
```js
let kbContent = "";
// !mark
const ragflowResponse = await retrieveFromRagflow({
    question: latestUserMessage?.content as string,
});
kbContent = ragflowResponse.kbContent;
```
<Note title="Note">
Usually, intent recognition and question enhancement are used to improve the answer quality before querying the knowledge base.
- Intent recognition: Identify user intent. If the user does not need to query the knowledge base, answer the user's question directly. Otherwise, continue. For example: User says "Hello".
- Question enhancement: Supplement and enhance the user's latest question based on historical conversations and preset conditions. For example: User asks "How about 2024?" then enhance to "What is the net profit of the company in 2024?"
</Note>
</Step>
<Step title="Merge the user's latest question and knowledge base fragments and call LLM to answer">

<Note title="Note">Some models provide context disk cache capabilities, so the price calculation has a cheaper cache price. Keeping the SystemPrompt unchanged and only replacing the User Message can effectively improve the cache hit probability and reduce the cost and shorten the inference time.</Note>
```js
// !mark(1:4)
requestData.messages[requestData.messages.length - 1] = {
    role: 'user',
    content: `${latestUserMessage?.content}\nHere is the knowledge base retrieval result:\n${kbContent}`,
};
// Call LLM to answer (using OpenAI SDK)
// LLM_BASE_URL_REAL is the URL of the real LLM service
const openai = new OpenAI({
    apiKey: apiKey,
    baseURL: process.env.LLM_BASE_URL_REAL
});
// Process the streaming response
const completion = await openai.chat.completions.create({
    model: model,
    stream: true,
    messages: requestData.messages
});
console.log("completion created successfully");
// Create a streaming response
const stream = new TransformStream();
const writer = stream.writable.getWriter();
const encoder = new TextEncoder();
for await (const chunk of completion) {
    // Note‚ö†Ô∏è: AIAgent requires that the last valid data must contain "finish_reason":"stop" and a termination data must be sent: data: [DONE]. If it is not sent, the AI Agent may not answer or answer incomplete.
    // Some models do not return finish_reason in the streaming response, in this case, you need to modify the chunk content and then return it to AIAgent.
    const ssePart = `data: ${JSON.stringify(chunk)}\n`;
    // Write the streaming response data until the streaming data ends
    // !mark
    writer.write(encoder.encode(ssePart));
}
// Send the termination mark
// !mark
writer.write(encoder.encode('data: [DONE]\n\n'));
writer.close();
```
</Step>
</Steps>
<Accordion title="Complete example code for the chat/completions interface" defaultOpen="false">
<CodeGroup>
```json title="Node.js(Next.js)"
import { NextResponse } from 'next/server';
import type { NextRequest } from 'next/server';
import { retrieveFromRagflow } from '@/lib/rag/ragflow';
import OpenAI from 'openai';
import type { ChatCompletionCreateParams } from 'openai/resources/chat';
import { retrieveFromBailian } from '@/lib/rag/bailian';


export async function POST(request: NextRequest) {
    // Authentication check
    const authHeader = request.headers.get('authorization');
    if (!authHeader || !authHeader.startsWith('Bearer ')) {
        return NextResponse.json(
            { error: 'Unauthorized' },
            { status: 401 }
        );
    }

    try {
        const requestData: ChatCompletionCreateParams = await request.json();
        console.log("requestData", requestData);
        console.log("requestData", JSON.stringify(requestData));

        // Read the API key, which is the value of apiKey when using the following method. The AIAgent server also uses the following method.
        // const openai = new OpenAI({
        //     apiKey: "xxx",
        //     baseURL: "xxx"
        // });
        // After reading the apiKey, you can do the necessary business verification. It is not necessarily the apiKey of the LLM, because it is transparent, so you can pass any content.
        // !mark
        const apiKey = authHeader.split(' ')[1];

        // Check necessary fields
        if (!requestData.messages || requestData.messages.length === 0) {
            return NextResponse.json(
                { error: 'Messages are required' },
                { status: 400 }
            );
        }

        // Check if streaming response is required
        if (requestData.stream) {
            // Read Model
            // Since the Model is fixed when registering AIAgent or creating AIAgent instance, you can pass a normal Model to LLM here.
            // You can also pass some additional business information through this value. For example, this Model is actually a business flag, indicating live/chat room, etc.
            // !mark
            const model = requestData.model;

            // Read SystemPrompt
            // Since the SystemPrompt is fixed when registering AIAgent or creating AIAgent instance, you can pass a normal SystemPrompt to LLM here.
            // You can also pass some additional business information through this value. For example, include user information, level, preferences, etc. Then modify the actual SystemPrompt of LLM when calling LLM.
            // !mark
            const systemMessage = requestData.messages.find(message => message.role === 'system');

            // Read the latest User Message (the latest is at the end of the array)
            // When AIAgent requests your interface, it will carry the Messages parameter. This parameter also includes SystemPrompt.
            // !mark
            const latestUserMessage = [...requestData.messages].reverse().find(message => message.role === 'user');

            // Read other LLM parameters compatible with the OpenAI protocol, which are not repeated here.

            // Create a streaming response
            // !mark(1:3)
            const stream = new TransformStream();
            const writer = stream.writable.getWriter();
            const encoder = new TextEncoder();
            try {
                let kbContent = "";
                // Call the knowledge base retrieval interface to get the knowledge base query result
                // !mark(1:11)
                if (process.env.KB_TYPE === "ragflow") {
                    console.log("Call Ragflow knowledge base retrieval interface");
                    const ragflowResponse = await retrieveFromRagflow({
                        question: latestUserMessage?.content as string,
                    });
                    kbContent = ragflowResponse.kbContent;
                } else if (process.env.KB_TYPE === "bailian") {
                    console.log("Call Bailian knowledge base retrieval interface");
                    const bailianResponse = await retrieveFromBailian({ query: latestUserMessage?.content as string });
                    kbContent = bailianResponse.kbContent;
                }

                // Merge the user's latest User Message and knowledge base retrieval result, replace the last element of the messages array, and then call LLM to answer
                // Noteüîî: Some models provide context disk cache capabilities, so the price calculation has a cheaper cache price. Keeping the SystemPrompt unchanged and only replacing the User Message can effectively improve the cache hit probability and reduce the cost and shorten the inference time.
                // !mark(1:4)
                requestData.messages[requestData.messages.length - 1] = {
                    role: 'user',
                    content: `${latestUserMessage?.content}\nHere is the knowledge base retrieval result:\n${kbContent}`,
                };

                // Call LLM to answer (using OpenAI SDK)
                const openai = new OpenAI({
                    apiKey: apiKey,
                    baseURL: process.env.LLM_BASE_URL_REAL
                });
                // Process the streaming response
                const completion = await openai.chat.completions.create({
                    model: model,
                    stream: true,
                    messages: requestData.messages
                });
                console.log("completion created successfully");
                for await (const chunk of completion) {
                    // Note‚ö†Ô∏è: AIAgent requires that the last valid data must contain "finish_reason":"stop" and a termination data must be sent: data: [DONE]. If it is not sent, the AI Agent may not answer or answer incomplete.
                    // Some models do not return finish_reason in the streaming response, in this case, you need to modify the chunk content and then return it to AIAgent.
                    const ssePart = `data: ${JSON.stringify(chunk)}\n`;
                    // Write the streaming response data until the streaming data ends
                    // !mark
                    writer.write(encoder.encode(ssePart));
                }

            } catch (error) {
                console.error('Stream processing error:', error);
            } finally {
                // Send the termination mark
                // !mark
                writer.write(encoder.encode('data: [DONE]\n\n'));
                writer.close();
                console.log("writer closed");
            }


            return new Response(stream.readable, {
                headers: {
                    'Content-Type': 'text/event-stream',
                    'Cache-Control': 'no-cache',
                    'Connection': 'keep-alive',
                    'Access-Control-Allow-Origin': '*',
                },
            });
        } else {
            // AIAgent does not support non-streaming response, return error code directly
            return NextResponse.json(
                { error: 'Streaming is required' },
                { status: 400 }
            );
        }
    } catch (error) {
        console.error('Error processing request:', error);
        return NextResponse.json(
            { error: 'Internal server error' },
            { status: 500 }
        );
    }
}

// Add OPTIONS method to support CORS preflight
export async function OPTIONS() {
    return NextResponse.json({}, {
        headers: {
            'Access-Control-Allow-Origin': '*',
            'Access-Control-Allow-Methods': 'POST, OPTIONS',
            'Access-Control-Allow-Headers': 'Content-Type, Authorization',
        },
    });
}
```
</CodeGroup>
</Accordion>
</Tab>

</Tabs>

</Step>
<Step title="Register Agent and Use Custom LLM" titleSize="h3">

When registering the agent ([RegisterAgent](./../api-reference/agent-configuration-management/register-agent.mdx)), set the custom LLM URL, and require the LLM to answer the user's question based on the knowledge base content in the `SystemPrompt`.

```javascript title="Register Agent Call Example"
// Please replace the LLM and TTS authentication parameters such as ApiKey, appid, token, etc. with your actual authentication parameters.
async registerAgent(agentId: string, agentName: string) {
    // Request interface: https://aigc-aiagent-api.zegotech.cn?Action=RegisterAgent
    const action = 'RegisterAgent';
    // !mark(4:9)
    const body = {
        AgentId: agentId,
        Name: agentName,
        LLM: {
            Url: "https://your-custom-llm-service/chat/completions",
            ApiKey: "your_api_key",
            Model: "your_model",
            SystemPrompt: "Please answer the user's question in a friendly manner based on the knowledge base content provided by the user. If the user's question is not in the knowledge base, please politely tell the user that we do not have related knowledge base content."
        },
        TTS: {
            Vendor: "ByteDance",
            Params: {
                "app": {
                    "appid": "zego_test",
                    "token": "zego_test",
                    "cluster": "volcano_tts"
                },
                "audio": {
                    "voice_type": "zh_female_wanwanxiaohe_moon_bigtts"
                }
            }
        }
    };
    // The sendRequest method encapsulates the request URL and public parameters. For details, please refer to: https://doc-zh.zego.im/aiagent-server/api-reference/accessing-server-apis
    return this.sendRequest<any>(action, body);
}
```

</Step>

</Steps>

You can now chat with your custom LLM.

### Best Practices

Detailed usage cases please refer to [Use AI Agent with RAG](./../best-practices/use-ai-agent-with-rag.mdx).