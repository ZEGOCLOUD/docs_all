#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å…¨æ–°çš„AIæ•°æ®æ›´æ–°è„šæœ¬
æ•´åˆé¡µé¢ä¸‹è½½ã€datasetåˆ›å»º/æ›´æ–°ã€assistantåˆ›å»º/æ›´æ–°åŠŸèƒ½
"""

import json
import re
import asyncio
import sys
import os
import time
import shutil
import requests
import xml.etree.ElementTree as ET
import subprocess
import logging
from pathlib import Path
from urllib.parse import urlparse
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass

# å¯¼å…¥ç°æœ‰æ¨¡å—
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

try:
    import requests
    print(f"âœ… requests å·²åŠ è½½ï¼Œç‰ˆæœ¬: {requests.__version__}")
except ImportError as e:
    print(f"âŒ éœ€è¦å®‰è£… requests: pip install requests")
    requests = None

try:
    from crawl4ai import AsyncWebCrawler
    CRAWL4AI_AVAILABLE = True
except ImportError:
    print("éœ€è¦å®‰è£… crawl4ai: pip install crawl4ai")
    CRAWL4AI_AVAILABLE = False
    AsyncWebCrawler = None

# å¸¸é‡å®šä¹‰
CHINESE_SITEMAP_URL = "https://doc-zh.zego.im/sitemap.xml"
ENGLISH_SITEMAP_URL = "https://www.zegocloud.com/docs/sitemap.xml"
CHINESE_BASE_URL = "https://doc-zh.zego.im/"
ENGLISH_BASE_URL = "https://www.zegocloud.com/docs/"

@dataclass
class Config:
    """é…ç½®ç±»"""
    ragflow_base_url: str = None
    api_key: str = None
    max_retries: int = 3
    retry_delay: int = 1
    concurrent_downloads: int = 5
    
    def __post_init__(self):
        self.ragflow_base_url = os.getenv('RAGFLOW_BASE_URL', '')
        self.api_key = os.getenv('RAGFLOW_API_KEY', '')

class UpdateAIDataManager:
    """AIæ•°æ®æ›´æ–°ç®¡ç†å™¨"""
    
    def __init__(self):
        self.config = Config()
        self.data_dir = Path("data")
        self.data_dir.mkdir(parents=True, exist_ok=True)
        self.static_data_dir = Path("../../static/data")
        self.static_data_dir.mkdir(parents=True, exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        self.setup_logging()
        
        # è¯­è¨€ç›¸å…³é…ç½®
        self.language = None
        self.config_file = None
        self.sitemap_url = None
        self.base_url = None
        self.faq_dataset_name = None
        
        # é”™è¯¯è®°å½•
        self.download_errors = {}
        self.dataset_errors = {}
        self.assistant_errors = {}
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—è®°å½•"""
        # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•
        script_dir = Path(__file__).parent
        log_file = script_dir / "update.log"
        
        # é…ç½®æ—¥å¿—æ ¼å¼
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()  # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°
            ]
        )
        
        self.logger = logging.getLogger(__name__)
        self.logger.info("=== AIæ•°æ®æ›´æ–°è„šæœ¬å¯åŠ¨ ===")
    
    def log_error(self, message: str):
        """è®°å½•é”™è¯¯ä¿¡æ¯ï¼ŒåŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ—¥å¿—æ–‡ä»¶"""
        print(f"âŒ {message}")
        self.logger.error(message)
    
    def log_warning(self, message: str):
        """è®°å½•è­¦å‘Šä¿¡æ¯"""
        print(f"âš ï¸  {message}")
        self.logger.warning(message)
        
    def select_language(self) -> str:
        """é€‰æ‹©å¤„ç†è¯­è¨€"""
        print("\n=== é€‰æ‹©å¤„ç†è¯­è¨€ ===")
        print("è¯·é€‰æ‹©è¦å¤„ç†çš„è¯­è¨€:")
        print("1. ä¸­æ–‡ (é»˜è®¤)")
        print("2. è‹±æ–‡")
        
        choice = input("\nè¯·é€‰æ‹© (ç›´æ¥å›è½¦é»˜è®¤ä¸­æ–‡): ").strip()
        
        if choice == "2":
            self.language = "en"
            self.config_file = "../../docuo.config.en.json"
            self.sitemap_url = ENGLISH_SITEMAP_URL
            self.base_url = ENGLISH_BASE_URL
            self.faq_dataset_name = "FAQ-EN"
            print("âœ… å·²é€‰æ‹©è‹±æ–‡")
            self.logger.info("é€‰æ‹©è¯­è¨€: è‹±æ–‡")
        else:
            self.language = "zh"
            self.config_file = "../../docuo.config.zh.json"
            self.sitemap_url = CHINESE_SITEMAP_URL
            self.base_url = CHINESE_BASE_URL
            self.faq_dataset_name = "FAQ-ZH"
            print("âœ… å·²é€‰æ‹©ä¸­æ–‡")
            self.logger.info("é€‰æ‹©è¯­è¨€: ä¸­æ–‡")
            
        return self.language

    def get_git_commits(self, limit: int = 10) -> List[Dict]:
        """è·å–æœ€è¿‘çš„gitæäº¤è®°å½•"""
        try:
            # è·å–æœ€è¿‘çš„æäº¤è®°å½•
            cmd = ['git', 'log', '--oneline', f'-{limit}', '--pretty=format:%H|%s|%ad', '--date=short']
            result = subprocess.run(cmd, capture_output=True, text=True, cwd='../..')

            if result.returncode != 0:
                self.log_error(f"è·å–gitæäº¤è®°å½•å¤±è´¥: {result.stderr}")
                return []

            commits = []
            for line in result.stdout.strip().split('\n'):
                if '|' in line:
                    parts = line.split('|', 2)
                    if len(parts) >= 3:
                        commits.append({
                            'hash': parts[0],
                            'message': parts[1],
                            'date': parts[2]
                        })

            return commits
        except Exception as e:
            self.log_error(f"è·å–gitæäº¤è®°å½•å¼‚å¸¸: {e}")
            return []

    def get_changed_files_since_commit(self, commit_hash: str) -> List[str]:
        """è·å–ä»æŒ‡å®šæäº¤åˆ°æœ€æ–°æäº¤çš„å˜æ›´æ–‡ä»¶ï¼ˆåŒ…å«æŒ‡å®šæäº¤æœ¬èº«ï¼‰"""
        try:
            # è·å–å˜æ›´çš„æ–‡ä»¶åˆ—è¡¨ï¼Œä½¿ç”¨ commit_hash^..HEAD æ¥åŒ…å«æŒ‡å®šæäº¤çš„å˜æ›´
            cmd = ['git', 'diff', '--name-only', f'{commit_hash}^..HEAD']
            result = subprocess.run(cmd, capture_output=True, text=True, cwd='../..')

            if result.returncode != 0:
                self.log_error(f"è·å–å˜æ›´æ–‡ä»¶å¤±è´¥: {result.stderr}")
                return []

            changed_files = []
            for line in result.stdout.strip().split('\n'):
                if line.strip() and line.endswith('.mdx'):
                    changed_files.append(line.strip())

            return changed_files
        except Exception as e:
            self.log_error(f"è·å–å˜æ›´æ–‡ä»¶å¼‚å¸¸: {e}")
            return []

    def match_files_to_instances(self, changed_files: List[str], config_data: Dict) -> Dict[str, List[str]]:
        """å°†å˜æ›´æ–‡ä»¶åŒ¹é…åˆ°å¯¹åº”çš„å®ä¾‹"""
        instance_files = {}
        instances = config_data.get('instances', [])

        for file_path in changed_files:
            for instance in instances:
                instance_path = instance.get('path', '')
                instance_id = instance.get('id', '')
                locale = instance.get('locale', '')

                # åªå¤„ç†å½“å‰è¯­è¨€çš„å®ä¾‹
                if locale != self.language:
                    continue

                # æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦ä»¥å®ä¾‹çš„pathå¼€å¤´
                if instance_path and file_path.startswith(instance_path):
                    if instance_id not in instance_files:
                        instance_files[instance_id] = []
                    instance_files[instance_id].append(file_path)
                    break  # æ‰¾åˆ°åŒ¹é…çš„å®ä¾‹åè·³å‡ºå¾ªç¯ï¼Œé¿å…é‡å¤åŒ¹é…

        return instance_files

    def convert_file_path_to_url(self, file_path: str, route_base_path: str, instance_path: str = None) -> str:
        """å°†æ–‡ä»¶è·¯å¾„è½¬æ¢ä¸ºURL

        Args:
            file_path: æ–‡ä»¶è·¯å¾„ï¼Œå¦‚ core_products/aiagent/zh/server/API reference/Agent Configuration Management/Register Agent.mdx
            route_base_path: è·¯ç”±åŸºç¡€è·¯å¾„ï¼Œå¦‚ aiagent-server
            instance_path: å®ä¾‹è·¯å¾„ï¼Œå¦‚ core_products/aiagent/zh/server
        """
        # ç§»é™¤æ–‡ä»¶æ‰©å±•å
        path_without_ext = file_path.replace('.mdx', '').replace('.md', '')

        # å¦‚æœæä¾›äº†instance_pathï¼Œå…ˆç§»é™¤åŒ¹é…çš„è·¯å¾„éƒ¨åˆ†
        if instance_path:
            # ç¡®ä¿instance_pathä¸ä»¥/ç»“å°¾
            instance_path = instance_path.rstrip('/')

            # å¦‚æœæ–‡ä»¶è·¯å¾„ä»¥instance_pathå¼€å¤´ï¼Œç§»é™¤è¿™éƒ¨åˆ†
            if path_without_ext.startswith(instance_path):
                # ç§»é™¤instance_pathéƒ¨åˆ†ï¼Œä¿ç•™å‰©ä½™è·¯å¾„
                remaining_path = path_without_ext[len(instance_path):].lstrip('/')
            else:
                # å¦‚æœä¸åŒ¹é…ï¼Œä½¿ç”¨åŸæœ‰é€»è¾‘ä½œä¸ºfallback
                remaining_path = self._fallback_path_extraction(path_without_ext)
        else:
            # æ²¡æœ‰instance_pathæ—¶ä½¿ç”¨åŸæœ‰é€»è¾‘
            remaining_path = self._fallback_path_extraction(path_without_ext)

        # å¦‚æœremaining_pathä¸ºç©ºï¼Œè¿”å›åŸºç¡€URL
        if not remaining_path:
            base_domain = "https://doc-zh.zego.im" if self.language == 'zh' else "https://www.zegocloud.com/docs"
            return f"{base_domain}/{route_base_path}"

        # è½¬æ¢ä¸ºURLæ ¼å¼ï¼šå°å†™ï¼Œç©ºæ ¼å’Œä¸‹åˆ’çº¿è½¬ä¸ºè¿å­—ç¬¦
        url_path = remaining_path.lower().replace(' ', '-').replace('_', '-')

        # æ„å»ºå®Œæ•´URL
        base_domain = "https://doc-zh.zego.im" if self.language == 'zh' else "https://www.zegocloud.com/docs"
        full_url = f"{base_domain}/{route_base_path}/{url_path}"

        return full_url

    def _fallback_path_extraction(self, path_without_ext: str) -> str:
        """åŸæœ‰çš„è·¯å¾„æå–é€»è¾‘ï¼Œä½œä¸ºfallback"""
        path_parts = path_without_ext.split('/')

        # æ‰¾åˆ°è¯­è¨€æ ‡è¯†ç¬¦çš„ä½ç½®
        lang_index = -1
        for i, part in enumerate(path_parts):
            if part in ['zh', 'en']:
                lang_index = i
                break

        # å¦‚æœæ‰¾åˆ°è¯­è¨€æ ‡è¯†ç¬¦ï¼Œå–è¯­è¨€æ ‡è¯†ç¬¦åé¢çš„éƒ¨åˆ†
        if lang_index >= 0 and lang_index < len(path_parts) - 1:
            remaining_parts = path_parts[lang_index + 1:]
        else:
            # å¦‚æœæ²¡æ‰¾åˆ°è¯­è¨€æ ‡è¯†ç¬¦ï¼Œå–æœ€åä¸¤ä¸ªéƒ¨åˆ†
            remaining_parts = path_parts[-2:] if len(path_parts) >= 2 else path_parts[-1:]

        return '/'.join(remaining_parts)

    def load_config_file(self) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        config_path = Path(self.config_file)
        if not config_path.exists():
            error_msg = f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {self.config_file}"
            self.log_error(error_msg)
            raise FileNotFoundError(error_msg)
        
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            error_msg = f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}"
            self.log_error(error_msg)
            raise

    def get_groups_from_config(self, config_data: Dict) -> Dict[str, Dict]:
        """ä»é…ç½®ä¸­æå–groupä¿¡æ¯"""
        groups = {}
        instances = config_data.get('instances', [])
        
        for instance in instances:
            nav_info = instance.get('navigationInfo', {})
            group_info = nav_info.get('group', {})
            group_id = group_info.get('id')
            group_name = group_info.get('name')
            
            if group_id and group_name:
                if group_id not in groups:
                    groups[group_id] = {
                        'id': group_id,
                        'name': group_name,
                        'instances': []
                    }
                groups[group_id]['instances'].append(instance)
        
        return groups

    def select_groups_and_instances(self, groups: Dict[str, Dict]) -> List[Dict]:
        """é€‰æ‹©è¦å¤„ç†çš„groupså’Œinstances"""
        print("\n=== é€‰æ‹©è¦å¤„ç†çš„äº§å“ç»„ ===")

        # æ˜¾ç¤ºæ‰€æœ‰groupé€‰é¡¹
        group_list = list(groups.values())
        for i, group in enumerate(group_list, 1):
            print(f"{i}. {group['id']}-{group['name']} ({len(group['instances'])} ä¸ªå®ä¾‹)")

        while True:
            try:
                choice = input(f"\nè¯·é€‰æ‹©äº§å“ç»„ (1-{len(group_list)}): ").strip()
                group_index = int(choice) - 1
                if 0 <= group_index < len(group_list):
                    selected_group = group_list[group_index]
                    break
                else:
                    print("æ— æ•ˆçš„é€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥!")
            except ValueError:
                print("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—!")

        print(f"\nå·²é€‰æ‹©äº§å“ç»„: {selected_group['name']}")

        # é€‰æ‹©è¯¥groupä¸‹çš„instances
        instances = selected_group['instances']
        print(f"\n=== é€‰æ‹©è¦å¤„ç†çš„å®ä¾‹ ===")
        print("è¯¥äº§å“ç»„ä¸‹çš„å®ä¾‹:")

        for i, instance in enumerate(instances, 1):
            platform = instance.get('navigationInfo', {}).get('platform', 'Unknown')
            print(f"{i}. {instance.get('id', 'Unknown')} - {platform}")

        print("0. å¤„ç†æ‰€æœ‰å®ä¾‹")

        while True:
            try:
                choice = input(f"\nè¯·é€‰æ‹©å®ä¾‹ (0-{len(instances)}, ç›´æ¥å›è½¦å¤„ç†æ‰€æœ‰): ").strip()
                if not choice:  # ç›´æ¥å›è½¦
                    selected_instances = instances
                    print("âœ… å°†å¤„ç†æ‰€æœ‰å®ä¾‹")
                    break
                elif choice == "0":
                    selected_instances = instances
                    print("âœ… å°†å¤„ç†æ‰€æœ‰å®ä¾‹")
                    break
                else:
                    instance_index = int(choice) - 1
                    if 0 <= instance_index < len(instances):
                        selected_instances = [instances[instance_index]]
                        platform = selected_instances[0].get('navigationInfo', {}).get('platform', 'Unknown')
                        print(f"âœ… å·²é€‰æ‹©å®ä¾‹: {selected_instances[0].get('id')} - {platform}")
                        break
                    else:
                        print("æ— æ•ˆçš„é€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥!")
            except ValueError:
                print("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—!")

        return selected_instances

    def select_download_mode(self) -> str:
        """é€‰æ‹©ä¸‹è½½æ¨¡å¼"""
        print("\n=== é€‰æ‹©ä¸‹è½½æ¨¡å¼ ===")
        print("è¯·é€‰æ‹©ä¸‹è½½æ¨¡å¼:")
        print("1. æ ¹æ®gitå˜æ›´è®°å½•ä¸‹è½½å¹¶æ›´æ–°")
        print("2. é€‰æ‹©æŒ‡å®šå®ä¾‹æ›´æ–°")
        print("3. è·³è¿‡ä¸‹è½½")

        while True:
            try:
                choice = input("\nè¯·é€‰æ‹© (1-3)ç›´æ¥å›è½¦é»˜è®¤1: ").strip()
                if choice == "1":
                    return "git_changes"
                elif choice == "2":
                    return "select_instances"
                elif choice == "3":
                    return "skip_download"
                else:
                    return "git_changes"
            except ValueError:
                print("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—!")

    def select_dataset_mode(self) -> bool:
        """é€‰æ‹©æ˜¯å¦å¤„ç†dataset"""
        print("\n=== é€‰æ‹©æ˜¯å¦å¤„ç†Dataset ===")
        print("è¯·é€‰æ‹©æ˜¯å¦éœ€è¦å¤„ç†Datasetï¼ˆçŸ¥è¯†åº“åˆ›å»º/æ›´æ–°ï¼‰:")
        print("1. å¤„ç†Dataset (é»˜è®¤)")
        print("2. è·³è¿‡Datasetå¤„ç†")

        while True:
            try:
                choice = input("\nè¯·é€‰æ‹© (1-2, ç›´æ¥å›è½¦é»˜è®¤å¤„ç†): ").strip()
                if choice == "1" or choice == "":
                    print("âœ… å°†å¤„ç†Dataset")
                    return True
                elif choice == "2":
                    print("â­ï¸  å°†è·³è¿‡Datasetå¤„ç†")
                    return False
                else:
                    print("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—!")
            except ValueError:
                print("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—!")

    def select_git_commit_and_get_changes(self, config_data: Dict) -> Tuple[List[Dict], Dict[str, List[str]]]:
        """é€‰æ‹©gitæäº¤å¹¶è·å–å˜æ›´"""
        print("\n=== æ ¹æ®gitå˜æ›´è®°å½•ä¸‹è½½ ===")

        # è·å–æœ€è¿‘çš„æäº¤è®°å½•
        commits = self.get_git_commits(10)
        if not commits:
            self.log_error("æ— æ³•è·å–gitæäº¤è®°å½•")
            return [], {}

        print("\næœ€è¿‘çš„10ä¸ªæäº¤:")
        for i, commit in enumerate(commits, 1):
            print(f"{i}. {commit['hash'][:8]} - {commit['message']} ({commit['date']})")

        # è®©ç”¨æˆ·é€‰æ‹©èµ·å§‹æäº¤
        while True:
            choice = input(f"\nè¯·é€‰æ‹©ä»å“ªä¸ªæäº¤å¼€å§‹ç»Ÿè®¡å˜æ›´ (1-{len(commits)}, ç›´æ¥å›è½¦é€‰æ‹©æœ€æ–°æäº¤): ").strip()
            if not choice:  # ç›´æ¥å›è½¦ï¼Œé€‰æ‹©æœ€æ–°æäº¤
                selected_commit = commits[0]
                print(f"âœ… å·²é€‰æ‹©æœ€æ–°æäº¤: {selected_commit['hash'][:8]} - {selected_commit['message']}")
                break
            try:
                commit_index = int(choice) - 1
                if 0 <= commit_index < len(commits):
                    selected_commit = commits[commit_index]
                    print(f"âœ… å·²é€‰æ‹©æäº¤: {selected_commit['hash'][:8]} - {selected_commit['message']}")
                    break
                else:
                    print("æ— æ•ˆçš„é€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥!")
            except ValueError:
                print("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—!")

        # è·å–å˜æ›´æ–‡ä»¶
        print(f"\nğŸ“Š æ­£åœ¨è·å–ä» {selected_commit['hash'][:8]} åˆ°æœ€æ–°æäº¤çš„å˜æ›´æ–‡ä»¶...")
        changed_files = self.get_changed_files_since_commit(selected_commit['hash'])

        if not changed_files:
            self.log_error("æ²¡æœ‰æ‰¾åˆ°å˜æ›´çš„.mdxæ–‡ä»¶")
            return [], {}

        print(f"âœ… æ‰¾åˆ° {len(changed_files)} ä¸ªå˜æ›´çš„.mdxæ–‡ä»¶:")
        for file in changed_files:
            print(f"  - {file}")

        # åŒ¹é…æ–‡ä»¶åˆ°å®ä¾‹
        print(f"\nğŸ” æ­£åœ¨åŒ¹é…å˜æ›´æ–‡ä»¶åˆ°å®ä¾‹...")
        instance_files = self.match_files_to_instances(changed_files, config_data)

        if not instance_files:
            self.log_error("æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„å®ä¾‹")
            return [], {}

        print(f"âœ… åŒ¹é…åˆ° {len(instance_files)} ä¸ªå®ä¾‹:")
        affected_instances = []
        for instance_id, files in instance_files.items():
            print(f"  - {instance_id}: {len(files)} ä¸ªæ–‡ä»¶")
            # æ‰¾åˆ°å¯¹åº”çš„å®ä¾‹é…ç½®
            for instance in config_data.get('instances', []):
                if instance.get('id') == instance_id:
                    affected_instances.append(instance)
                    break

        return affected_instances, instance_files

    def get_sitemap_urls(self, route_base_path: str) -> List[str]:
        """ä»sitemapè·å–åŒ¹é…çš„URLs"""
        try:
            print(f"æ­£åœ¨è·å–sitemap: {self.sitemap_url}")

            if requests is None:
                self.log_error("requests æ¨¡å—ä¸å¯ç”¨ï¼Œæ— æ³•è·å–sitemap")
                return []

            response = requests.get(self.sitemap_url, timeout=30)
            response.raise_for_status()

            root = ET.fromstring(response.content)
            namespaces = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}

            urls = []
            url_elements = root.findall('.//ns:url', namespaces)
            if len(url_elements) == 0:
                url_elements = root.findall('.//url')

            # æ„å»ºåŒ¹é…æ¨¡å¼
            pattern = f"{self.base_url}{route_base_path}"

            for url_elem in url_elements:
                loc_elem = url_elem.find('ns:loc', namespaces)
                if loc_elem is None:
                    loc_elem = url_elem.find('loc')

                if loc_elem is not None and loc_elem.text:
                    url = loc_elem.text.strip()
                    if url.startswith(pattern):
                        urls.append(url)

            print(f"åŒ¹é…åˆ° {len(urls)} ä¸ªURLs (æ¨¡å¼: {pattern})")
            return urls

        except Exception as e:
            self.log_error(f"è·å–sitemapå¤±è´¥: {e}")
            return []

    def get_filename_from_url(self, url: str, title: str = "") -> str:
        """æ ¹æ®URLç”Ÿæˆæ–‡ä»¶å"""
        parsed = urlparse(url)

        if title:
            clean_title = re.sub(r'[^\w\s-]', '', title).strip()
            clean_title = re.sub(r'[-\s]+', '-', clean_title)
            base_name = f"{clean_title}---{parsed.netloc}{parsed.path}"
        else:
            base_name = f"{parsed.netloc}{parsed.path}"

        filename = base_name.replace("/", ">").replace("&", "^^").replace('=', '^^^')
        return f"{filename}.md"

    async def download_url_content(self, url: str, target_dir: Path) -> bool:
        """ä¸‹è½½å•ä¸ªURLçš„å†…å®¹ï¼Œæ”¯æŒé‡è¯•"""
        if not CRAWL4AI_AVAILABLE:
            self.log_error(f"crawl4ai æœªå®‰è£…: {url}")
            return False

        for attempt in range(self.config.max_retries):
            try:
                async with AsyncWebCrawler(verbose=False) as crawler:
                    result = await crawler.arun(
                        url=url,
                        word_count_threshold=10,
                        extraction_strategy="NoExtractionStrategy",
                        chunking_strategy="RegexChunking",
                        bypass_cache=True
                    )

                    if result.success:
                        title = result.metadata.get('title', '') if result.metadata else ''
                        filename = self.get_filename_from_url(url, title)
                        file_path = target_dir / filename

                        with open(file_path, 'w', encoding='utf-8') as f:
                            f.write(f"# {title}\n\n")
                            f.write(f"**URL:** {url}\n\n")
                            f.write("---\n\n")
                            f.write(result.markdown)

                        print(f"âœ… ä¸‹è½½æˆåŠŸ: {url}")
                        return True
                    else:
                        self.log_error(f"ä¸‹è½½å¤±è´¥ (å°è¯• {attempt + 1}/{self.config.max_retries}): {url} - {result.error_message}")
                        if attempt < self.config.max_retries - 1:
                            await asyncio.sleep(self.config.retry_delay * (2 ** attempt))  # æŒ‡æ•°é€€é¿

            except Exception as e:
                self.log_error(f"ä¸‹è½½å¼‚å¸¸ (å°è¯• {attempt + 1}/{self.config.max_retries}): {url} - {str(e)}")
                if attempt < self.config.max_retries - 1:
                    await asyncio.sleep(self.config.retry_delay * (2 ** attempt))  # æŒ‡æ•°é€€é¿

        self.log_error(f"ä¸‹è½½æœ€ç»ˆå¤±è´¥: {url}")
        return False

    async def download_instance_files(self, instance: Dict, group_id: str) -> bool:
        """ä¸‹è½½å•ä¸ªinstanceçš„æ–‡ä»¶"""
        instance_id = instance.get('id')
        route_base_path = instance.get('routeBasePath')
        platform = instance.get('navigationInfo', {}).get('platform', 'Unknown')

        print(f"\nğŸ“¥ å¼€å§‹å¤„ç†å®ä¾‹: {instance_id} - {platform}")

        if not route_base_path:
            self.log_error(f"å®ä¾‹ {instance_id} ç¼ºå°‘ routeBasePath")
            return False

        # åˆ›å»ºç›®å½•ç»“æ„: data/group_id/instance_id
        target_dir = self.data_dir / group_id / instance_id

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨æ–‡ä»¶
        existing_files = []
        if target_dir.exists():
            existing_files = list(target_dir.glob("*.md"))

        if existing_files:
            print(f"ğŸ“ å‘ç°å·²å­˜åœ¨ {len(existing_files)} ä¸ªæ–‡ä»¶åœ¨ç›®å½•: {target_dir}")
            print(f"âš ï¸  é‡æ–°ä¸‹è½½å°†åˆ é™¤ç°æœ‰æ–‡ä»¶å¹¶é‡æ–°è·å–æ‰€æœ‰å†…å®¹")

            while True:
                choice = input(f"æ˜¯å¦é‡æ–°ä¸‹è½½æ‰€æœ‰æ–‡ä»¶? (y/n, é»˜è®¤y): ").strip().lower()
                if choice in ['', 'y', 'yes']:
                    print(f"ğŸ—‘ï¸  åˆ é™¤å·²å­˜åœ¨çš„ç›®å½•: {target_dir}")
                    shutil.rmtree(target_dir)
                    target_dir.mkdir(parents=True, exist_ok=True)
                    break
                elif choice in ['n', 'no']:
                    print(f"âœ… è·³è¿‡ä¸‹è½½ï¼Œä½¿ç”¨ç°æœ‰æ–‡ä»¶")
                    return True
                else:
                    print("è¯·è¾“å…¥ y æˆ– n")
        else:
            target_dir.mkdir(parents=True, exist_ok=True)

        # è·å–URLs
        urls = self.get_sitemap_urls(route_base_path)
        if not urls:
            self.log_error(f"æœªæ‰¾åˆ°åŒ¹é…çš„URLsï¼Œæ— æ³•ç»§ç»­å¤„ç†")
            return False

        print(f"ğŸ“ ç›®æ ‡ç›®å½•: {target_dir}")
        print(f"ğŸš€ å¼€å§‹ä¸‹è½½ {len(urls)} ä¸ªæ–‡ä»¶...")

        # æ‰¹é‡ä¸‹è½½
        success_count = 0
        failed_count = 0
        batch_size = self.config.concurrent_downloads

        for i in range(0, len(urls), batch_size):
            batch = urls[i:i + batch_size]
            tasks = [self.download_url_content(url, target_dir) for url in batch]
            results = await asyncio.gather(*tasks, return_exceptions=True)

            for result in results:
                if result is True:
                    success_count += 1
                else:
                    failed_count += 1

        success_rate = success_count / len(urls) if urls else 0
        print(f"ğŸ“Š ä¸‹è½½ç»Ÿè®¡: {success_count}/{len(urls)} ä¸ªæ–‡ä»¶æˆåŠŸ (æˆåŠŸç‡: {success_rate:.1%})")

        # å¦‚æœæˆåŠŸç‡ä½äº50%ï¼Œè®¤ä¸ºä¸‹è½½å¤±è´¥
        if success_rate < 0.5:
            self.log_error(f"ä¸‹è½½å¤±è´¥ç‡è¿‡é«˜ ({failed_count}/{len(urls)} å¤±è´¥)ï¼Œç»ˆæ­¢å¤„ç†")
            return False

        if failed_count > 0:
            self.log_warning(f"æœ‰ {failed_count} ä¸ªæ–‡ä»¶ä¸‹è½½å¤±è´¥ï¼Œä½†æˆåŠŸç‡å¯æ¥å—ï¼Œç»§ç»­å¤„ç†")

        return success_count > 0

    async def download_git_changed_files(self, affected_instances: List[Dict], instance_files: Dict[str, List[str]]) -> bool:
        """æ ¹æ®gitå˜æ›´ä¸‹è½½æ–‡ä»¶"""
        print(f"\nğŸ“¥ å¼€å§‹æ ¹æ®gitå˜æ›´ä¸‹è½½æ–‡ä»¶...")

        success_count = 0
        failed_count = 0

        for instance in affected_instances:
            instance_id = instance.get('id')
            route_base_path = instance.get('routeBasePath')
            platform = instance.get('navigationInfo', {}).get('platform', 'Unknown')
            group_id = instance.get('navigationInfo', {}).get('group', {}).get('id')

            print(f"\nğŸ“¥ å¤„ç†å®ä¾‹: {instance_id} - {platform}")

            if not route_base_path:
                self.log_error(f"å®ä¾‹ {instance_id} ç¼ºå°‘ routeBasePath")
                failed_count += 1
                continue

            if not group_id:
                self.log_error(f"å®ä¾‹ {instance_id} ç¼ºå°‘ group_id")
                failed_count += 1
                continue

            # åˆ›å»ºç›®å½•ç»“æ„: data/group_id/instance_id
            target_dir = self.data_dir / group_id / instance_id
            target_dir.mkdir(parents=True, exist_ok=True)

            # è·å–è¯¥å®ä¾‹çš„å˜æ›´æ–‡ä»¶
            changed_files = instance_files.get(instance_id, [])
            if not changed_files:
                print(f"âš ï¸  å®ä¾‹ {instance_id} æ²¡æœ‰å˜æ›´æ–‡ä»¶")
                continue

            print(f"ğŸ“ ç›®æ ‡ç›®å½•: {target_dir}")
            print(f"ğŸ”„ å¤„ç† {len(changed_files)} ä¸ªå˜æ›´æ–‡ä»¶:")
            # å…ˆåˆ é™¤ç›®æ ‡ç›®å½•
            shutil.rmtree(target_dir, ignore_errors=True)
            target_dir.mkdir(parents=True, exist_ok=True)

            # è·å–å®ä¾‹è·¯å¾„
            instance_path = instance.get('path', '')

            # è½¬æ¢æ–‡ä»¶è·¯å¾„ä¸ºURLå¹¶ä¸‹è½½
            urls_to_download = []
            for file_path in changed_files:
                url = self.convert_file_path_to_url(file_path, route_base_path, instance_path)
                urls_to_download.append(url)
                print(f"  - {file_path} -> {url}")

            if not urls_to_download:
                self.log_error(f"æ²¡æœ‰æœ‰æ•ˆçš„URLéœ€è¦ä¸‹è½½")
                failed_count += 1
                continue

            # æ‰¹é‡ä¸‹è½½
            print(f"ğŸš€ å¼€å§‹ä¸‹è½½ {len(urls_to_download)} ä¸ªæ–‡ä»¶...")
            batch_success = 0
            batch_failed = 0
            batch_size = self.config.concurrent_downloads

            for i in range(0, len(urls_to_download), batch_size):
                batch = urls_to_download[i:i + batch_size]
                tasks = [self.download_url_content(url, target_dir) for url in batch]
                results = await asyncio.gather(*tasks, return_exceptions=True)

                for result in results:
                    if result is True:
                        batch_success += 1
                    else:
                        batch_failed += 1

            batch_success_rate = batch_success / len(urls_to_download) if urls_to_download else 0
            print(f"ğŸ“Š å®ä¾‹ {instance_id} ä¸‹è½½ç»Ÿè®¡: {batch_success}/{len(urls_to_download)} ä¸ªæ–‡ä»¶æˆåŠŸ (æˆåŠŸç‡: {batch_success_rate:.1%})")

            if batch_success_rate >= 0.5:  # æˆåŠŸç‡50%ä»¥ä¸Šè®¤ä¸ºæˆåŠŸ
                success_count += 1
            else:
                failed_count += 1
                self.log_error(f"å®ä¾‹ {instance_id} ä¸‹è½½å¤±è´¥ç‡è¿‡é«˜")

        total_success_rate = success_count / len(affected_instances) if affected_instances else 0
        print(f"\nğŸ“Š æ€»ä½“ä¸‹è½½ç»Ÿè®¡: {success_count}/{len(affected_instances)} ä¸ªå®ä¾‹æˆåŠŸ (æˆåŠŸç‡: {total_success_rate:.1%})")

        return total_success_rate >= 0.5

    def get_documents_by_filename(self, dataset_id: str, filenames: List[str], instance: Dict = None) -> List[str]:
        """æ ¹æ®æ–‡ä»¶åè·å–æ–‡æ¡£ID

        Args:
            dataset_id: æ•°æ®é›†ID
            filenames: æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆåŸå§‹æ–‡ä»¶è·¯å¾„ï¼‰
            instance: å®ä¾‹ä¿¡æ¯ï¼Œç”¨äºè·å–è·¯å¾„è½¬æ¢æ‰€éœ€çš„å‚æ•°
        """
        if not requests:
            self.log_error("requests æœªå®‰è£…ï¼Œæ— æ³•è·å–æ–‡æ¡£")
            return []

        headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {self.config.api_key}'
            }

        try:
            # è·å–datasetä¸­çš„æ‰€æœ‰æ–‡æ¡£
            response = requests.get(
                f"{self.config.ragflow_base_url}/datasets/{dataset_id}/documents",
                headers=headers
            )

            if response.status_code != 200:
                self.log_error(f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: HTTP {response.status_code}")
                return []

            data = response.json()

            if data.get('code') == 102:
                print(f"âš ï¸  æ•°æ®é›†ä¸ºç©º: {dataset_id}")
                return []

            elif data.get('code') != 0:
                self.log_error(f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {data.get('code')} {data.get('message')} {dataset_id}")
                return []

            data_content = data.get('data', {})

            # æ ¹æ®RagFlow APIæ–‡æ¡£ï¼ŒList documentsè¿”å›çš„dataæ˜¯ä¸€ä¸ªå¯¹è±¡ï¼ŒåŒ…å«docsæ•°ç»„
            if isinstance(data_content, dict):
                documents = data_content.get('docs', [])
            elif isinstance(data_content, list):
                # å…¼å®¹å¯èƒ½çš„ç›´æ¥æ•°ç»„æ ¼å¼
                documents = data_content
            else:
                self.log_error(f"æ–‡æ¡£æ•°æ®æ ¼å¼é”™è¯¯: {type(data_content)}")
                return []

            if not isinstance(documents, list):
                self.log_error(f"æ–‡æ¡£åˆ—è¡¨æ ¼å¼é”™è¯¯: {type(documents)}")
                return []

            document_ids = []

            # å°†æ–‡ä»¶è·¯å¾„è½¬æ¢ä¸ºä¸‹è½½æ–‡ä»¶åæ ¼å¼
            converted_filenames = []
            if instance:
                route_base_path = instance.get('routeBasePath', '')
                instance_path = instance.get('path', '')

                for filename in filenames:
                    try:
                        # å°†æ–‡ä»¶è·¯å¾„è½¬æ¢ä¸ºURL
                        url = self.convert_file_path_to_url(filename, route_base_path, instance_path)
                        # å°†URLè½¬æ¢ä¸ºä¸‹è½½æ–‡ä»¶åæ ¼å¼ï¼ˆä¸å¸¦titleï¼Œå› ä¸ºæˆ‘ä»¬æ²¡æœ‰titleä¿¡æ¯ï¼‰
                        download_filename = self.get_filename_from_url(url)
                        converted_filenames.append(download_filename)
                        print(f"ğŸ”„ æ–‡ä»¶è·¯å¾„è½¬æ¢: {filename} -> {url} -> {download_filename}")
                    except Exception as e:
                        print(f"âš ï¸  æ–‡ä»¶è·¯å¾„è½¬æ¢å¤±è´¥: {filename} - {e}")
                        # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ–‡ä»¶åä½œä¸ºfallback
                        converted_filenames.append(filename)
            else:
                # å¦‚æœæ²¡æœ‰å®ä¾‹ä¿¡æ¯ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æ–‡ä»¶å
                converted_filenames = filenames
                print("âš ï¸  æ²¡æœ‰å®ä¾‹ä¿¡æ¯ï¼Œæ— æ³•è¿›è¡Œæ–‡ä»¶åè½¬æ¢ï¼Œä½¿ç”¨åŸå§‹æ–‡ä»¶å")

            # åŒ¹é…æ–‡ä»¶å
            for i, original_filename in enumerate(filenames):
                converted_filename = converted_filenames[i] if i < len(converted_filenames) else original_filename

                found_match = False
                for doc in documents:
                    if not isinstance(doc, dict):
                        continue

                    doc_name = doc.get('name', '')
                    doc_id = doc.get('id', '')

                    if not doc_id:
                        continue

                    # åŒ¹é…---åé¢çš„éƒ¨åˆ†
                    if doc_name.endswith(f"---{converted_filename}"):
                        document_ids.append(doc_id)
                        print(f"âœ… æ‰¾åˆ°åŒ¹é…æ–‡æ¡£: {doc_name} (ID: {doc_id}) <- {original_filename}")
                        found_match = True
                        break

                if not found_match:
                    print(f"âš ï¸  æœªæ‰¾åˆ°åŒ¹é…æ–‡æ¡£: {original_filename} -> {converted_filename}")

            return document_ids

        except Exception as e:
            self.log_error(f"è·å–æ–‡æ¡£å¼‚å¸¸: {e}")
            return []

    def delete_specific_documents(self, dataset_id: str, document_ids: List[str]) -> bool:
        """åˆ é™¤æŒ‡å®šçš„æ–‡æ¡£"""
        if not document_ids:
            print("âš ï¸  æ²¡æœ‰æ–‡æ¡£éœ€è¦åˆ é™¤")
            return True

        if not requests:
            self.log_error("requests æœªå®‰è£…ï¼Œæ— æ³•åˆ é™¤æ–‡æ¡£")
            return False

        headers = {
            'Authorization': f'Bearer {self.config.api_key}',
            'Content-Type': 'application/json'
        }

        try:
            response = requests.delete(
                f"{self.config.ragflow_base_url}/datasets/{dataset_id}/documents",
                headers=headers,
                json={'ids': document_ids}
            )
            data = response.json()

            if data.get('code') == 0:
                print(f"âœ… æˆåŠŸåˆ é™¤ {len(document_ids)} ä¸ªæ–‡æ¡£")
                return True
            else:
                self.log_error(f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {data.get('message')}")
                return False

        except Exception as e:
            self.log_error(f"åˆ é™¤æ–‡æ¡£å¼‚å¸¸: {e}")
            return False

    def create_or_get_dataset(self, dataset_name: str) -> Optional[str]:
        """åˆ›å»ºæˆ–è·å–dataset"""
        print(f"ğŸ“š å¤„ç†çŸ¥è¯†åº“: {dataset_name}")

        if not requests:
            self.log_error("requests æœªå®‰è£…ï¼Œæ— æ³•å¤„ç†çŸ¥è¯†åº“")
            return None

        headers = {
            'Authorization': f'Bearer {self.config.api_key}',
            'Content-Type': 'application/json'
        }

        try:
            # å…ˆæŸ¥æ‰¾æ˜¯å¦å­˜åœ¨
            response = requests.get(
                f"{self.config.ragflow_base_url}/datasets?name={dataset_name}",
                headers=headers
            )
            data = response.json()

            if data.get('code') == 0 and data.get('data'):
                dataset_id = data['data'][0]['id']
                print(f"âœ… æ‰¾åˆ°å·²å­˜åœ¨çš„çŸ¥è¯†åº“: {dataset_id}")
                return dataset_id

            # åˆ›å»ºæ–°çš„dataset
            config = {
                'name': dataset_name,
                'language': 'Chinese' if self.language == 'zh' else 'English',
                'permission': 'team',
                'embedding_model': os.getenv('RAGFLOW_EMBEDDING_MODEL_ZH', 'BAAI/bge-small-zh-v1.5') if self.language == 'zh' else os.getenv('RAGFLOW_EMBEDDING_MODEL_EN', 'BAAI/bge-small-en-v1.5'),
                'parser_config': {'chunk_token_num': os.getenv('RAGFLOW_CHUNK_TOKEN_NUM', 512)}
            }

            response = requests.post(
                f"{self.config.ragflow_base_url}/datasets",
                headers=headers,
                json=config
            )
            data = response.json()

            if data.get('code') == 0:
                dataset_id = data['data']['id']
                print(f"âœ… åˆ›å»ºçŸ¥è¯†åº“æˆåŠŸ: {dataset_id}")
                return dataset_id
            else:
                self.log_error(f"åˆ›å»ºçŸ¥è¯†åº“å¤±è´¥: {data.get('message')}")
                return None

        except Exception as e:
            self.log_error(f"å¤„ç†çŸ¥è¯†åº“å¼‚å¸¸: {e}")
            return None

    def upload_and_parse_documents(self, dataset_id: str, file_paths: List[str], clear_existing: bool = True) -> bool:
        """ä¸Šä¼ å¹¶è§£ææ–‡æ¡£"""
        if not file_paths:
            print("âš ï¸  æ²¡æœ‰æ–‡ä»¶éœ€è¦ä¸Šä¼ ")
            return False

        print(f"ğŸ“¤ ä¸Šä¼  {len(file_paths)} ä¸ªæ–‡æ¡£åˆ°çŸ¥è¯†åº“")

        headers = {'Authorization': f'Bearer {self.config.api_key}'}

        try:
            # æ ¹æ®å‚æ•°å†³å®šæ˜¯å¦æ¸…ç©ºç°æœ‰æ–‡æ¡£
            if clear_existing:
                response = requests.delete(
                    f"{self.config.ragflow_base_url}/datasets/{dataset_id}/documents",
                    headers={**headers, 'Content-Type': 'application/json'},
                    json={'ids': []}
                )
                print("ğŸ—‘ï¸  å·²æ¸…ç©ºçŸ¥è¯†åº“ä¸­çš„ç°æœ‰æ–‡æ¡£")
            else:
                print("ğŸ“ ä¿ç•™ç°æœ‰æ–‡æ¡£ï¼Œä»…ä¸Šä¼ æ–°æ–‡æ¡£")

            # æ‰¹é‡ä¸Šä¼ æ–‡ä»¶
            batch_size = 10
            all_file_ids = []

            for i in range(0, len(file_paths), batch_size):
                batch_files = file_paths[i:i + batch_size]
                files = []

                for file_path in batch_files:
                    if Path(file_path).exists():
                        files.append(('file', open(file_path, 'rb')))

                if files:
                    response = requests.post(
                        f"{self.config.ragflow_base_url}/datasets/{dataset_id}/documents",
                        headers=headers,
                        files=files
                    )

                    # å…³é—­æ–‡ä»¶å¥æŸ„
                    for _, file_handle in files:
                        file_handle.close()

                    data = response.json()
                    if data.get('code') == 0 and data.get('data'):
                        batch_ids = [item['id'] for item in data['data']]
                        all_file_ids.extend(batch_ids)
                        print(f"âœ… æ‰¹æ¬¡ä¸Šä¼ æˆåŠŸ: {len(batch_ids)} ä¸ªæ–‡ä»¶")

            if all_file_ids:
                # è§£ææ–‡æ¡£
                response = requests.post(
                    f"{self.config.ragflow_base_url}/datasets/{dataset_id}/chunks",
                    headers={**headers, 'Content-Type': 'application/json'},
                    json={'document_ids': all_file_ids}
                )
                print("âœ… æ–‡æ¡£è§£æå®Œæˆ")
                return True
            else:
                self.log_error("æ²¡æœ‰æˆåŠŸä¸Šä¼ çš„æ–‡ä»¶")
                return False

        except Exception as e:
            self.log_error(f"ä¸Šä¼ æ–‡æ¡£å¼‚å¸¸: {e}")
            return False

    def get_group_questions(self, config_data: Dict, group_id: str) -> List[str]:
        """ä»åŒä¸€groupçš„å…¶ä»–å®ä¾‹ä¸­è·å–questions"""
        all_questions = []
        instances = config_data.get('instances', [])

        for instance in instances:
            nav_info = instance.get('navigationInfo', {})
            instance_group_id = nav_info.get('group', {}).get('id')

            if instance_group_id == group_id:
                questions = instance.get('askAi', {}).get('questions', [])
                if questions:
                    all_questions.extend(questions)

        # å»é‡å¹¶ä¿æŒé¡ºåº
        unique_questions = []
        for q in all_questions:
            if q not in unique_questions:
                unique_questions.append(q)

        return unique_questions

    def generate_ai_search_mapping(self, selected_instances: List[Dict], group_id: str, config_data: Dict) -> Dict:
        """ç”Ÿæˆæˆ–æ›´æ–°AIæœç´¢æ˜ å°„é…ç½®"""
        print(f"\nğŸ”§ ç”ŸæˆAIæœç´¢æ˜ å°„é…ç½®...")

        # åŠ è½½ç°æœ‰é…ç½®
        mapping_file = self.static_data_dir / "ai_search_mapping.json"
        if mapping_file.exists():
            with open(mapping_file, 'r', encoding='utf-8') as f:
                mapping_config = json.load(f)
        else:
            mapping_config = {}

        # ç¡®ä¿groupå­˜åœ¨
        if group_id not in mapping_config:
            mapping_config[group_id] = {}

        # è·å–åŒgroupçš„questionsä½œä¸ºé»˜è®¤å€¼
        group_questions = self.get_group_questions(config_data, group_id)
        print(f"ğŸ“‹ ä»åŒgroupè·å–åˆ° {len(group_questions)} ä¸ªé»˜è®¤é—®é¢˜")

        # ä¸ºæ¯ä¸ªinstanceç”Ÿæˆé…ç½®
        for instance in selected_instances:
            instance_id = instance.get('id')
            platform = instance.get('navigationInfo', {}).get('platform', 'Unknown')
            instance_questions = instance.get('askAi', {}).get('questions', [])

            # å¦‚æœå®ä¾‹æ²¡æœ‰questionsï¼Œä½¿ç”¨åŒgroupçš„questions
            questions = instance_questions if instance_questions else group_questions

            # dataset_namesåŒ…å«instance_idå’ŒFAQ
            dataset_names = [instance_id, self.faq_dataset_name]

            assistant_name = instance_id

            mapping_config[group_id][platform] = {
                "dataset_names": dataset_names,
                "assistant_name": assistant_name,
                "questions": questions,
                "chat_id": ""  # å°†åœ¨åˆ›å»ºassistantåæ›´æ–°
            }

            questions_source = "å®ä¾‹è‡ªæœ‰" if instance_questions else "åŒgroupé»˜è®¤"
            print(f"ğŸ“ {platform}: ä½¿ç”¨äº† {len(questions)} ä¸ªé—®é¢˜ ({questions_source})")

        # ä¿å­˜é…ç½®
        with open(mapping_file, 'w', encoding='utf-8') as f:
            json.dump(mapping_config, f, ensure_ascii=False, indent=2)

        print(f"âœ… AIæœç´¢æ˜ å°„é…ç½®å·²æ›´æ–°: {mapping_file}")
        return mapping_config

    def create_or_update_assistant(self, assistant_name: str, dataset_names: List[str]) -> Optional[str]:
        """åˆ›å»ºæˆ–æ›´æ–°assistantï¼Œæ”¯æŒå¯¹code 102é”™è¯¯è¿›è¡Œé‡è¯•"""
        print(f"ğŸ¤– å¤„ç†Assistant: {assistant_name}")

        if not requests:
            self.log_error("requests æœªå®‰è£…ï¼Œæ— æ³•å¤„ç†Assistant")
            return None

        headers = {
            'Authorization': f'Bearer {self.config.api_key}',
            'Content-Type': 'application/json'
        }

        max_retries_for_102 = 3
        retry_delay_for_102 = 10  # 10ç§’

        for attempt in range(max_retries_for_102 + 1):  # 0, 1, 2, 3 å…±4æ¬¡å°è¯•ï¼ˆ1æ¬¡åˆå§‹ + 3æ¬¡é‡è¯•ï¼‰
            try:
                # è·å–datasetsæ˜ å°„
                dataset_ids = []
                for dataset_name in dataset_names:
                    dataset_id = self.create_or_get_dataset(dataset_name)
                    if dataset_id:
                        dataset_ids.append(dataset_id)
                    else:
                        self.log_error(f"æ— æ³•è·å–dataset: {dataset_name}")
                        return None

                if not dataset_ids:
                    self.log_error("æ²¡æœ‰æœ‰æ•ˆçš„dataset IDs")
                    return None

                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                response = requests.get(
                    f"{self.config.ragflow_base_url}/chats",
                    headers=headers,
                    params={"name": assistant_name}
                )
                data = response.json()

                existing_id = None
                if data.get('code') == 0 and data.get('data'):
                    existing_id = data['data'][0]['id']
                    print(f"æ‰¾åˆ°å·²å­˜åœ¨çš„Assistant: {existing_id}")

                # å‡†å¤‡payload
                is_chinese = self.language == 'zh'
                empty_response = "éå¸¸æŠ±æ­‰ï¼ŒçŸ¥è¯†åº“ä¸­æš‚æ—¶æ‰¾ä¸åˆ°ç›¸å…³ä¿¡æ¯ï¼Œè¯·å°è¯•æ›´è¯¦ç»†æè¿°ä½ çš„é—®é¢˜æˆ–è€…å°è¯•å…¶ä»–é—®é¢˜ã€‚" if is_chinese else "Sorry, no relevant information found in the knowledge base. Please try to describe your question in more detail or try other questions."

                # ç®€åŒ–çš„promptæ¨¡æ¿
                prompt_template = """
# è§’è‰²
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œåå­—å«Miss Zã€‚ä½ çš„ä¸»è¦èŒè´£æ˜¯åŸºäºçŸ¥è¯†åº“ä¸­çš„ä¿¡æ¯æ¥æ€»ç»“å¹¶å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

## æŠ€èƒ½
### æŠ€èƒ½1: å›ç­”ç®€å•é—®é¢˜
- å¦‚æœé—®é¢˜å¯ä»¥ç”¨ä¸€ä¸ªæ¥å£ã€ä¸€ä¸ªé…ç½®æˆ–è€…ä¸€å¥ç®€çŸ­çš„è¯å›ç­”åˆ™æ˜¯ç®€å•é—®é¢˜
- ç®€å•æ˜ç¡®çš„é—®é¢˜åˆ™ç›´æ¥ç”¨æœ€ç®€çŸ­çš„ç­”æ¡ˆå›ç­”å³å¯
- ç®€å•é—®é¢˜ä¸éœ€è¦æä¾›ç¤ºä¾‹ä»£ç 
- å¦‚æœä¸€äº›æ“ä½œæœ‰å‰ææ¡ä»¶æˆ–è€…å…¶ä»–è¦æ±‚åˆ™ä¹Ÿéœ€è¦æ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·

### æŠ€èƒ½2: å›ç­”å¤æ‚é—®é¢˜
- å¦‚æœé—®é¢˜éœ€è¦é€šè¿‡å¤šä¸ªæ­¥éª¤æˆ–è€…å¤šä¸ªæ¥å£é…åˆå®ç°çš„é‚£ä¹ˆå°±æ˜¯å¤æ‚é—®é¢˜
- å…ˆæ ¹æ®çŸ¥è¯†åº“å†…å®¹ç®€è¦æ€»ç»“è¯´æ˜å®ç°æ­¥éª¤
- æ¯ä¸ªæ­¥éª¤é…åˆæœ€ç®€æ´ä¸”å¿…è¦çš„ç¤ºä¾‹ä»£ç è¯´æ˜

### æŠ€èƒ½3: å¤šè¯­è¨€æ”¯æŒ
- ä½¿ç”¨ä¸­æ–‡è¿›è¡Œå›ç­”ï¼Œç¡®ä¿æ²Ÿé€šæ— éšœç¢

### æŠ€èƒ½4: æç¤º
- æ¯æ¬¡å›ç­”æœ€åéƒ½æç¤ºç”¨æˆ·ï¼šå¦‚æœæœ‰ä»»ä½•ç–‘é—®ï¼Œè¯·è”ç³» ZEGO æŠ€æœ¯æ”¯æŒã€‚

## é™åˆ¶
- ç»å¯¹ä¸èƒ½æé€ ä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯æ¶‰åŠåˆ°æ•°å­—å’Œä»£ç æ—¶ï¼Œå¿…é¡»ä¿è¯ä¿¡æ¯çš„å‡†ç¡®æ€§
- å›ç­”æ ¼å¼éœ€éµå¾ªMarkdownè§„èŒƒï¼Œä½¿ç­”æ¡ˆç»“æ„æ¸…æ™°ã€æ˜“è¯»
- å½“çŸ¥è¯†åº“ä¸­çš„ä¿¡æ¯ä¸ç”¨æˆ·é—®é¢˜æ— å…³æ—¶ï¼Œç›´æ¥å›å¤ï¼š"{empty_response}"
- æ‰€æœ‰å›ç­”éƒ½åº”åŸºäºçŸ¥è¯†åº“ä¸­çš„ç°æœ‰èµ„æ–™ï¼Œä¸å¾—è¶…å‡ºå…¶èŒƒå›´

## çŸ¥è¯†åº“
{{knowledge}}

ä»¥ä¸Šå°±æ˜¯ç›¸å…³çš„çŸ¥è¯†ã€‚
""" if is_chinese else """
# Role
You are an intelligent assistant named Miss Z. Your primary duty is to summarize and answer user questions based on the information in the knowledge base.

## Skills
### Skill 1: Answering Simple Questions
- A simple question is one that can be answered with a single interface, configuration, or a brief statement
- For straightforward questions, provide the most concise answer possible
- Simple questions do not require example code
- If there are prerequisites or other requirements, make sure to inform the user clearly

### Skill 2: Answering Complex Questions
- A complex question requires multiple steps or interfaces to be addressed
- Begin by briefly summarizing the implementation steps based on the knowledge base content
- Provide the most concise and necessary example code for each step

### Skill 3: Multilingual Support
- Answer in English to ensure smooth communication

### Skill 4: Reminder
- End each response with: If you have any questions, please contact ZEGOCLOUD technical support.

## Limitations
- Never fabricate information, especially when it involves numbers and code; accuracy is crucial
- Follow Markdown format for clear and readable answers
- When the knowledge base information is irrelevant to the user's question, reply: "{empty_response}"
- All responses should be based on existing materials in the knowledge base and should not exceed its scope

## Knowledge Base
{{knowledge}}

Above is the relevant knowledge.
"""

                payload = {
                    "name": assistant_name,
                    "dataset_ids": dataset_ids,
                    "llm": {
                        "model_name": "qwen-plus",
                        "frequency_penalty": 0.7,
                        "max_tokens": 4096,
                        "presence_penalty": 0.4,
                        "temperature": 0.1,
                        "top_p": 0.3
                    },
                    "prompt": {
                        "empty_response": empty_response,
                        "prompt": prompt_template.format(empty_response=empty_response)
                    }
                }

                if existing_id:
                    # æ›´æ–°
                    response = requests.put(
                        f"{self.config.ragflow_base_url}/chats/{existing_id}",
                        headers=headers,
                        json=payload
                    )
                    action = "æ›´æ–°"
                    assistant_id = existing_id
                else:
                    # åˆ›å»º
                    response = requests.post(
                        f"{self.config.ragflow_base_url}/chats",
                        headers=headers,
                        json=payload
                    )
                    action = "åˆ›å»º"

                data = response.json()
                
                # æ£€æŸ¥å“åº”ç 
                if data.get('code') == 0:
                    # æˆåŠŸ
                    if existing_id:
                        assistant_id = existing_id
                    else:
                        assistant_id = data.get('data', {}).get('id')
                    
                    if assistant_id:
                        print(f"âœ… {action}AssistantæˆåŠŸ: {assistant_name} (ID: {assistant_id})")
                        return assistant_id
                    else:
                        self.log_error(f"{action}Assistantå¤±è´¥ï¼Œæœªè·å–åˆ°ID: {assistant_name}")
                        return None
                        
                elif data.get('code') == 102:
                    # code 102 é”™è¯¯ï¼Œéœ€è¦é‡è¯•
                    if attempt < max_retries_for_102:
                        print(f"âš ï¸  {action}Assistanté‡åˆ°code 102é”™è¯¯ (å°è¯• {attempt + 1}/{max_retries_for_102 + 1}): {assistant_name} {data}")
                        print(f"â³ ç­‰å¾… {retry_delay_for_102} ç§’åé‡è¯•...")
                        time.sleep(retry_delay_for_102)
                        continue
                    else:
                        self.log_error(f"{action}Assistantå¤±è´¥ï¼Œcode 102 é‡è¯• {max_retries_for_102} æ¬¡åä»ç„¶å¤±è´¥: {assistant_name}")
                        return None
                else:
                    # å…¶ä»–é”™è¯¯ç ï¼Œç›´æ¥å¤±è´¥ä¸é‡è¯•
                    self.log_error(f"{action}Assistantå¤±è´¥ï¼Œé”™è¯¯ç  {data.get('code')}: {assistant_name} - {data.get('message', 'æœªçŸ¥é”™è¯¯')}")
                    return None

            except Exception as e:
                if attempt < max_retries_for_102:
                    self.log_error(f"å¤„ç†Assistantå¼‚å¸¸ (å°è¯• {attempt + 1}/{max_retries_for_102 + 1}): {assistant_name} - {e}")
                    print(f"â³ ç­‰å¾… {retry_delay_for_102} ç§’åé‡è¯•...")
                    time.sleep(retry_delay_for_102)
                    continue
                else:
                    self.log_error(f"å¤„ç†Assistantå¼‚å¸¸ï¼Œé‡è¯• {max_retries_for_102} æ¬¡åä»ç„¶å¤±è´¥: {assistant_name} - {e}")
                    return None

        # å¦‚æœåˆ°è¾¾è¿™é‡Œï¼Œè¯´æ˜æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
        return None

    def update_mapping_with_chat_ids(self, mapping_config: Dict, group_id: str, selected_instances: List[Dict]) -> List[str]:
        """æ›´æ–°æ˜ å°„é…ç½®ä¸­çš„chat_idï¼Œè¿”å›å¤±è´¥çš„Assistantåˆ—è¡¨"""
        print(f"\nğŸ”„ æ›´æ–°æ˜ å°„é…ç½®ä¸­çš„chat_id...")

        failed_assistants = []

        for instance in selected_instances:
            platform = instance.get('navigationInfo', {}).get('platform', 'Unknown')
            instance_id = instance.get('id')

            if group_id in mapping_config and platform in mapping_config[group_id]:
                assistant_name = mapping_config[group_id][platform]['assistant_name']
                dataset_names = mapping_config[group_id][platform]['dataset_names']

                print(f"ğŸ¤– å¤„ç†Assistant: {assistant_name}")

                # åˆ›å»ºæˆ–æ›´æ–°assistant
                chat_id = self.create_or_update_assistant(assistant_name, dataset_names)
                if chat_id:
                    mapping_config[group_id][platform]['chat_id'] = chat_id
                    print(f"âœ… Assistantå¤„ç†æˆåŠŸ: {assistant_name}")
                else:
                    failed_assistants.append(assistant_name)
                    self.log_error(f"Assistantå¤„ç†å¤±è´¥: {assistant_name}")
            else:
                print(f"âš ï¸  æœªæ‰¾åˆ°æ˜ å°„é…ç½®: {group_id}/{platform}")
                failed_assistants.append(f"{instance_id}-{platform}")

        # ä¿å­˜æ›´æ–°åçš„é…ç½®
        mapping_file = self.static_data_dir / "ai_search_mapping.json"
        with open(mapping_file, 'w', encoding='utf-8') as f:
            json.dump(mapping_config, f, ensure_ascii=False, indent=2)

        if failed_assistants:
            print(f"âš ï¸  {len(failed_assistants)} ä¸ªAssistantå¤„ç†å¤±è´¥")
        else:
            print(f"âœ… æ‰€æœ‰Assistantå¤„ç†æˆåŠŸ")

        print(f"âœ… æ˜ å°„é…ç½®å·²æ›´æ–°å®Œæˆ")
        return failed_assistants

    def log_download_errors(self, failed_instances: List[str], failed_files: Dict[str, List[str]]):
        """è®°å½•ä¸‹è½½é”™è¯¯"""
        if failed_instances:
            print("\n=== ä¸‹è½½é”™è¯¯æ€»ç»“ ===")
            print(f"âŒ ä»¥ä¸‹å®ä¾‹ä¸‹è½½å¤±è´¥:")
            for instance in failed_instances:
                print(f"  - {instance}")
                if instance in failed_files:
                    print("    å¤±è´¥æ–‡ä»¶:")
                    for file in failed_files[instance]:
                        print(f"      - {file}")
    
    def log_dataset_errors(self, failed_datasets: List[str], error_details: Dict[str, str]):
        """è®°å½•Datasetå¤„ç†é”™è¯¯"""
        if failed_datasets:
            print("\n=== Datasetå¤„ç†é”™è¯¯æ€»ç»“ ===")
            print(f"âŒ ä»¥ä¸‹Datasetå¤„ç†å¤±è´¥:")
            for dataset in failed_datasets:
                print(f"  - {dataset}")
                if dataset in error_details:
                    print(f"    é”™è¯¯åŸå› : {error_details[dataset]}")
    
    def log_assistant_errors(self, failed_assistants: List[str], error_details: Dict[str, str]):
        """è®°å½•Assistantå¤„ç†é”™è¯¯"""
        if failed_assistants:
            print("\n=== Assistantå¤„ç†é”™è¯¯æ€»ç»“ ===")
            print(f"âŒ ä»¥ä¸‹Assistantå¤„ç†å¤±è´¥:")
            for assistant in failed_assistants:
                print(f"  - {assistant}")
                if assistant in error_details:
                    print(f"    é”™è¯¯åŸå› : {error_details[assistant]}")

async def main():
    """ä¸»å‡½æ•°"""
    manager = UpdateAIDataManager()

    print("=== AIæ•°æ®æ›´æ–°è„šæœ¬ ===")
    print("æœ¬è„šæœ¬å°†å®Œæ•´æ‰§è¡Œï¼šé¡µé¢ä¸‹è½½ -> Datasetæ›´æ–° -> Assistantæ›´æ–°")

    try:
        # 1. é€‰æ‹©è¯­è¨€
        manager.select_language()

        # 2. åŠ è½½é…ç½®æ–‡ä»¶
        print(f"\nğŸ“– åŠ è½½é…ç½®æ–‡ä»¶: {manager.config_file}")
        config_data = manager.load_config_file()

        # 3. æå–groups
        groups = manager.get_groups_from_config(config_data)
        if not groups:
            manager.log_error("é…ç½®æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„groups")
            return

        print(f"âœ… æ‰¾åˆ° {len(groups)} ä¸ªäº§å“ç»„")

        # 4. é€‰æ‹©ä¸‹è½½æ¨¡å¼
        download_mode = manager.select_download_mode()

        selected_instances = []
        instance_files = {}
        group_id = None

        if download_mode == "git_changes":
            # æ ¹æ®gitå˜æ›´è®°å½•ä¸‹è½½
            affected_instances, instance_files = manager.select_git_commit_and_get_changes(config_data)
            if not affected_instances:
                manager.log_error("æ²¡æœ‰æ‰¾åˆ°å—å½±å“çš„å®ä¾‹")
                return

            selected_instances = affected_instances
            group_id = selected_instances[0].get('navigationInfo', {}).get('group', {}).get('id')

        elif download_mode == "select_instances":
            # é€‰æ‹©æŒ‡å®šå®ä¾‹æ›´æ–°
            selected_instances = manager.select_groups_and_instances(groups)
            if not selected_instances:
                manager.log_error("æ²¡æœ‰é€‰æ‹©ä»»ä½•å®ä¾‹")
                return

            group_id = selected_instances[0].get('navigationInfo', {}).get('group', {}).get('id')

        elif download_mode == "skip_download":
            # è·³è¿‡ä¸‹è½½ï¼Œä½†ä»éœ€è¦é€‰æ‹©å®ä¾‹è¿›è¡Œåç»­å¤„ç†
            selected_instances = manager.select_groups_and_instances(groups)
            if not selected_instances:
                manager.log_error("æ²¡æœ‰é€‰æ‹©ä»»ä½•å®ä¾‹")
                return

            group_id = selected_instances[0].get('navigationInfo', {}).get('group', {}).get('id')

        if not group_id:
            print("âŒ æ— æ³•è·å–group_id")
            return

        print(f"\nğŸš€ å¼€å§‹å¤„ç† {len(selected_instances)} ä¸ªå®ä¾‹...")

        # 5. ä¸‹è½½é¡µé¢æ–‡ä»¶
        print(f"\n=== ç¬¬1æ­¥: ä¸‹è½½é¡µé¢æ–‡ä»¶ ===")

        failed_instances = []
        failed_files = {}

        if download_mode == "git_changes":
            # æ ¹æ®gitå˜æ›´ä¸‹è½½
            print(f"ğŸ“Š å°†æ ¹æ®gitå˜æ›´ä¸‹è½½ {len(selected_instances)} ä¸ªå®ä¾‹çš„å˜æ›´æ–‡ä»¶")
            for instance in selected_instances:
                instance_id = instance.get('id')
                success = await manager.download_git_changed_files([instance], {instance_id: instance_files.get(instance_id, [])})
                if not success:
                    failed_instances.append(instance_id)
                    failed_files[instance_id] = instance_files.get(instance_id, [])

        elif download_mode == "select_instances":
            # é€‰æ‹©å®ä¾‹ä¸‹è½½
            print(f"ğŸ“Š å°†ä¸‹è½½ {len(selected_instances)} ä¸ªå®ä¾‹çš„æ‰€æœ‰æ–‡ä»¶")
            for instance in selected_instances:
                instance_id = instance.get('id')
                success = await manager.download_instance_files(instance, group_id)
                if not success:
                    failed_instances.append(instance_id)
                    failed_files[instance_id] = []  # è¿™é‡Œæ— æ³•è·å–å…·ä½“å¤±è´¥çš„æ–‡ä»¶åˆ—è¡¨

        if failed_instances:
            manager.log_download_errors(failed_instances, failed_files)
            print("âŒ ç”±äºä¸‹è½½å¤±è´¥ï¼Œç»ˆæ­¢æ‰§è¡Œæµç¨‹")
            return

        # 6. é€‰æ‹©æ˜¯å¦å¤„ç†datasets
        process_datasets = manager.select_dataset_mode()

        # 7. åˆ›å»ºæˆ–æ›´æ–°datasets
        failed_datasets = []
        dataset_errors = {}

        if process_datasets:
            print(f"\n=== ç¬¬2æ­¥: åˆ›å»ºæˆ–æ›´æ–°Datasets ===")

            for instance in selected_instances:
                instance_id = instance.get('id')

                # è·å–æ–‡ä»¶è·¯å¾„
                target_dir = manager.data_dir / group_id / instance_id
                if not target_dir.exists():
                    print(f"âŒ å®ä¾‹ {instance_id} ç›®å½•ä¸å­˜åœ¨: {target_dir}")
                    failed_datasets.append(instance_id)
                    dataset_errors[instance_id] = f"ç›®å½•ä¸å­˜åœ¨: {target_dir}"
                    continue

                md_files = list(target_dir.glob("*.md"))
                if not md_files:
                    print(f"âŒ å®ä¾‹ {instance_id} æ²¡æœ‰æ‰¾åˆ°markdownæ–‡ä»¶")
                    failed_datasets.append(instance_id)
                    dataset_errors[instance_id] = "æ²¡æœ‰æ‰¾åˆ°markdownæ–‡ä»¶"
                    continue

                file_paths = [str(f) for f in md_files]
                print(f"ğŸ“š å¤„ç†å®ä¾‹ {instance_id}ï¼Œæ‰¾åˆ° {len(md_files)} ä¸ªæ–‡ä»¶")

                # åˆ›å»ºæˆ–è·å–dataset
                dataset_id = manager.create_or_get_dataset(instance_id)
                if not dataset_id:
                    print(f"âŒ æ— æ³•åˆ›å»ºæˆ–è·å–dataset: {instance_id}")
                    failed_datasets.append(instance_id)
                    dataset_errors[instance_id] = "æ— æ³•åˆ›å»ºæˆ–è·å–dataset"
                    continue

                # æ ¹æ®ä¸‹è½½æ¨¡å¼å¤„ç†æ–‡æ¡£
                if download_mode == "git_changes" and instance_id in instance_files:
                    # gitå˜æ›´æ¨¡å¼ï¼šå…ˆåˆ é™¤å¯¹åº”çš„æ—§æ–‡æ¡£ï¼Œå†ä¸Šä¼ æ–°æ–‡æ¡£
                    changed_files = instance_files[instance_id]
                    print(f"ğŸ”„ gitå˜æ›´æ¨¡å¼ï¼šå¤„ç† {len(changed_files)} ä¸ªå˜æ›´æ–‡ä»¶")

                    # è·å–éœ€è¦åˆ é™¤çš„æ–‡æ¡£IDï¼Œä¼ å…¥å®ä¾‹ä¿¡æ¯ç”¨äºæ–‡ä»¶åè½¬æ¢
                    document_ids_to_delete = manager.get_documents_by_filename(dataset_id, changed_files, instance)
                    if document_ids_to_delete:
                        print(f"ğŸ—‘ï¸  åˆ é™¤ {len(document_ids_to_delete)} ä¸ªæ—§æ–‡æ¡£")
                        delete_success = manager.delete_specific_documents(dataset_id, document_ids_to_delete)
                        if not delete_success:
                            print(f"âš ï¸  åˆ é™¤æ—§æ–‡æ¡£å¤±è´¥ï¼Œä½†ç»§ç»­ä¸Šä¼ æ–°æ–‡æ¡£")

                    # ä¸Šä¼ æ–°æ–‡æ¡£ï¼ˆä¸æ¸…ç©ºæ•´ä¸ªdatasetï¼‰
                    upload_success = manager.upload_and_parse_documents(dataset_id, file_paths, clear_existing=False)
                else:
                    # ä¼ ç»Ÿæ¨¡å¼ï¼šæ¸…ç©ºå¹¶é‡æ–°ä¸Šä¼ æ‰€æœ‰æ–‡æ¡£
                    upload_success = manager.upload_and_parse_documents(dataset_id, file_paths, clear_existing=True)

                if not upload_success:
                    print(f"âŒ æ–‡æ¡£ä¸Šä¼ å¤±è´¥: {instance_id}")
                    failed_datasets.append(instance_id)
                    dataset_errors[instance_id] = "æ–‡æ¡£ä¸Šä¼ å¤±è´¥"
                    continue

                print(f"âœ… Datasetå¤„ç†æˆåŠŸ: {instance_id}")

            if failed_datasets:
                manager.log_dataset_errors(failed_datasets, dataset_errors)
                print("âŒ ç”±äºDatasetåˆ›å»º/æ›´æ–°å¤±è´¥ï¼Œç»ˆæ­¢æ‰§è¡Œæµç¨‹")
                return
        else:
            print(f"\n=== ç¬¬2æ­¥: è·³è¿‡Datasetå¤„ç† ===")
            print(f"â­ï¸  è·³è¿‡Datasetå¤„ç†æ­¥éª¤ï¼Œç›´æ¥è¿›å…¥Assistantå¤„ç†")

        # 8. ç”ŸæˆAIæœç´¢æ˜ å°„é…ç½®
        print(f"\n=== ç¬¬3æ­¥: ç”ŸæˆAIæœç´¢æ˜ å°„é…ç½® ===")
        mapping_config = manager.generate_ai_search_mapping(selected_instances, group_id, config_data)

        # 9. åˆ›å»ºæˆ–æ›´æ–°Assistants
        print(f"\n=== ç¬¬4æ­¥: åˆ›å»ºæˆ–æ›´æ–°Assistants ===")
        failed_assistants = []
        assistant_errors = {}

        for instance in selected_instances:
            platform = instance.get('navigationInfo', {}).get('platform', 'Unknown')
            instance_id = instance.get('id')

            if group_id in mapping_config and platform in mapping_config[group_id]:
                assistant_name = mapping_config[group_id][platform]['assistant_name']
                dataset_names = mapping_config[group_id][platform]['dataset_names']

                print(f"ğŸ¤– å¤„ç†Assistant: {assistant_name}")

                # åˆ›å»ºæˆ–æ›´æ–°assistant
                chat_id = manager.create_or_update_assistant(assistant_name, dataset_names)
                if chat_id:
                    mapping_config[group_id][platform]['chat_id'] = chat_id
                    print(f"âœ… Assistantå¤„ç†æˆåŠŸ: {assistant_name}")
                else:
                    failed_assistants.append(assistant_name)
                    assistant_errors[assistant_name] = "åˆ›å»ºæˆ–æ›´æ–°å¤±è´¥"
                    print(f"âŒ Assistantå¤„ç†å¤±è´¥: {assistant_name}")
            else:
                print(f"âš ï¸  æœªæ‰¾åˆ°æ˜ å°„é…ç½®: {group_id}/{platform}")
                failed_assistants.append(f"{instance_id}-{platform}")
                assistant_errors[f"{instance_id}-{platform}"] = "æœªæ‰¾åˆ°æ˜ å°„é…ç½®"

        if failed_assistants:
            manager.log_assistant_errors(failed_assistants, assistant_errors)
            print("âš ï¸  éƒ¨åˆ†Assistantå¤„ç†å¤±è´¥ï¼Œä½†ä¸å½±å“å…¶ä»–åŠŸèƒ½çš„æ­£å¸¸ä½¿ç”¨")

        print(f"\nğŸ‰ æ‰€æœ‰æ­¥éª¤æ‰§è¡Œå®Œæˆ!")
        print(f"âœ… æˆåŠŸå¤„ç†äº† {len(selected_instances)} ä¸ªå®ä¾‹")

        print(f"ğŸ“ æ–‡ä»¶ä¿å­˜åœ¨: {manager.data_dir / group_id}")
        print(f"ğŸ”§ é…ç½®æ–‡ä»¶: {manager.static_data_dir / 'ai_search_mapping.json'}")

    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
    except Exception as e:
        print(f"\nâŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())
