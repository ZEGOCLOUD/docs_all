#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å…¨æ–°çš„AIæ•°æ®æ›´æ–°è„šæœ¬
æ•´åˆé¡µé¢ä¸‹è½½ã€datasetåˆ›å»º/æ›´æ–°åŠŸèƒ½
"""

import json
import re
import asyncio
import sys
import os
import shutil
import requests
import xml.etree.ElementTree as ET
import subprocess
import logging
import argparse
from pathlib import Path
from urllib.parse import urlparse
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass

# å¯¼å…¥ç°æœ‰æ¨¡å—
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

try:
    import requests
    print(f"âœ… requests å·²åŠ è½½ï¼Œç‰ˆæœ¬: {requests.__version__}")
except ImportError as e:
    print(f"âŒ éœ€è¦å®‰è£… requests: pip install requests")
    requests = None


# å¸¸é‡å®šä¹‰
CHINESE_SITEMAP_URL = "https://doc-zh.zego.im/sitemap.xml"
ENGLISH_SITEMAP_URL = "https://www.zegocloud.com/docs/sitemap.xml"
CHINESE_BASE_URL = "https://doc-zh.zego.im/"
ENGLISH_BASE_URL = "https://www.zegocloud.com/docs/"

@dataclass
class Config:
    """é…ç½®ç±»"""
    ragflow_base_url: str = None
    api_key: str = None
    max_retries: int = 3
    retry_delay: int = 1
    concurrent_downloads: int = 5

    def __post_init__(self):
        # ä¼˜å…ˆå°è¯•åŠ è½½åŒç›®å½•ä¸‹çš„ .envï¼ˆæ— ç¬¬ä¸‰æ–¹ä¾èµ–ï¼‰ï¼Œä»…è®¾ç½®æœªåœ¨ç¯å¢ƒä¸­å­˜åœ¨çš„é”®
        try:
            env_path = Path(__file__).parent / ".env"
            if env_path.exists():
                for line in env_path.read_text(encoding='utf-8').splitlines():
                    line = line.strip()
                    if not line or line.startswith('#'):
                        continue
                    if '=' in line:
                        k, v = line.split('=', 1)
                        k = k.strip()
                        v = v.strip().strip('"').strip("'")
                        if k and k not in os.environ:
                            os.environ[k] = v
        except Exception:
            pass

        # è¯»å–å¹¶è§„èŒƒåŒ– RAGFlow åŸºç¡€åœ°å€
        base = os.getenv('RAGFLOW_BASE_URL', '').strip().rstrip('/')
        if base:
            if not (base.startswith('http://') or base.startswith('https://')):
                base = 'https://' + base
            # ç¡®ä¿åŒ…å« /api/v1 å‰ç¼€
            if not re.search(r"/api/v\d+($|/)", base):
                base = base + '/api/v1'
        self.ragflow_base_url = base

        # API Key
        self.api_key = os.getenv('RAGFLOW_API_KEY', '')

class UpdateAIDataManager:
    """AIæ•°æ®æ›´æ–°ç®¡ç†å™¨"""

    def __init__(self):
        self.config = Config()
        self.data_dir = Path("data")
        self.data_dir.mkdir(parents=True, exist_ok=True)
        self.static_data_dir = Path("../../static/data")
        self.static_data_dir.mkdir(parents=True, exist_ok=True)

        # è®¾ç½®æ—¥å¿—
        self.setup_logging()

        # è¯­è¨€ç›¸å…³é…ç½®
        self.language = None
        self.config_file = None
        self.sitemap_url = None
        self.base_url = None
        self.faq_dataset_name = None

        # é”™è¯¯è®°å½•
        self.download_errors = {}
        self.dataset_errors = {}

    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—è®°å½•"""
        # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•
        script_dir = Path(__file__).parent
        log_file = script_dir / "update.log"

        # é…ç½®æ—¥å¿—æ ¼å¼
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()  # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°
            ]
        )

        self.logger = logging.getLogger(__name__)
        self.logger.info("=== AIæ•°æ®æ›´æ–°è„šæœ¬å¯åŠ¨ ===")

    def log_error(self, message: str):
        """è®°å½•é”™è¯¯ä¿¡æ¯ï¼ŒåŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ—¥å¿—æ–‡ä»¶"""
        print(f"âŒ {message}")
        self.logger.error(message)

    def log_warning(self, message: str):
        """è®°å½•è­¦å‘Šä¿¡æ¯"""
        print(f"âš ï¸  {message}")
        self.logger.warning(message)

    def select_language(self) -> str:
        """é€‰æ‹©å¤„ç†è¯­è¨€"""
        print("\n=== é€‰æ‹©å¤„ç†è¯­è¨€ ===")
        print("è¯·é€‰æ‹©è¦å¤„ç†çš„è¯­è¨€:")
        print("1. ä¸­æ–‡ (é»˜è®¤)")
        print("2. è‹±æ–‡")

        choice = input("\nè¯·é€‰æ‹© (ç›´æ¥å›è½¦é»˜è®¤ä¸­æ–‡): ").strip()

        if choice == "2":
            self.language = "en"
            self.config_file = "../../docuo.config.en.json"
            self.sitemap_url = ENGLISH_SITEMAP_URL
            self.base_url = ENGLISH_BASE_URL
            self.faq_dataset_name = "FAQ-EN"
            print("âœ… å·²é€‰æ‹©è‹±æ–‡")
            self.logger.info("é€‰æ‹©è¯­è¨€: è‹±æ–‡")
        else:
            self.language = "zh"
            self.config_file = "../../docuo.config.zh.json"
            self.sitemap_url = CHINESE_SITEMAP_URL
            self.base_url = CHINESE_BASE_URL
            self.faq_dataset_name = "FAQ-ZH"
            print("âœ… å·²é€‰æ‹©ä¸­æ–‡")
            self.logger.info("é€‰æ‹©è¯­è¨€: ä¸­æ–‡")

        return self.language

    def get_git_commits(self, limit: int = 10) -> List[Dict]:
        """è·å–æœ€è¿‘çš„gitæäº¤è®°å½•"""
        try:
            # è·å–æœ€è¿‘çš„æäº¤è®°å½•
            cmd = ['git', 'log', '--oneline', f'-{limit}', '--pretty=format:%H|%s|%ad', '--date=short']
            result = subprocess.run(cmd, capture_output=True, text=True, cwd='../..')

            if result.returncode != 0:
                self.log_error(f"è·å–gitæäº¤è®°å½•å¤±è´¥: {result.stderr}")
                return []

            commits = []
            for line in result.stdout.strip().split('\n'):
                if '|' in line:
                    parts = line.split('|', 2)
                    if len(parts) >= 3:
                        commits.append({
                            'hash': parts[0],
                            'message': parts[1],
                            'date': parts[2]
                        })

            return commits
        except Exception as e:
            self.log_error(f"è·å–gitæäº¤è®°å½•å¼‚å¸¸: {e}")
            return []

    def get_changed_files_since_commit(self, commit_hash: str) -> List[str]:
        """è·å–ä»æŒ‡å®šæäº¤åˆ°æœ€æ–°æäº¤çš„å˜æ›´æ–‡ä»¶ï¼ˆåŒ…å«æŒ‡å®šæäº¤æœ¬èº«ï¼‰"""
        try:
            # è·å–å˜æ›´çš„æ–‡ä»¶åˆ—è¡¨ï¼Œä½¿ç”¨ commit_hash^..HEAD æ¥åŒ…å«æŒ‡å®šæäº¤çš„å˜æ›´
            cmd = ['git', 'diff', '--name-only', f'{commit_hash}^..HEAD']
            result = subprocess.run(cmd, capture_output=True, text=True, cwd='../..')

            if result.returncode != 0:
                self.log_error(f"è·å–å˜æ›´æ–‡ä»¶å¤±è´¥: {result.stderr}")
                return []

            changed_files = []
            for line in result.stdout.strip().split('\n'):
                if line.strip() and line.endswith('.mdx'):
                    changed_files.append(line.strip())

            return changed_files
        except Exception as e:
            self.log_error(f"è·å–å˜æ›´æ–‡ä»¶å¼‚å¸¸: {e}")
            return []

    def match_files_to_instances(self, changed_files: List[str], config_data: Dict) -> Dict[str, List[str]]:
        """å°†å˜æ›´æ–‡ä»¶åŒ¹é…åˆ°å¯¹åº”çš„å®ä¾‹"""
        instance_files = {}
        instances = config_data.get('instances', [])

        for file_path in changed_files:
            for instance in instances:
                instance_path = instance.get('path', '')
                instance_id = instance.get('id', '')
                locale = instance.get('locale', '')

                # åªå¤„ç†å½“å‰è¯­è¨€çš„å®ä¾‹
                if locale != self.language:
                    continue

                # æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦ä»¥å®ä¾‹çš„pathå¼€å¤´
                if instance_path and file_path.startswith(instance_path):
                    if instance_id not in instance_files:
                        instance_files[instance_id] = []
                    instance_files[instance_id].append(file_path)
                    break  # æ‰¾åˆ°åŒ¹é…çš„å®ä¾‹åè·³å‡ºå¾ªç¯ï¼Œé¿å…é‡å¤åŒ¹é…

        return instance_files

    def convert_file_path_to_url(self, file_path: str, route_base_path: str, instance_path: str = None) -> str:
        """å°†æ–‡ä»¶è·¯å¾„è½¬æ¢ä¸ºURL

        Args:
            file_path: æ–‡ä»¶è·¯å¾„ï¼Œå¦‚ core_products/aiagent/zh/server/API reference/Agent Configuration Management/Register Agent.mdx
            route_base_path: è·¯ç”±åŸºç¡€è·¯å¾„ï¼Œå¦‚ aiagent-server
            instance_path: å®ä¾‹è·¯å¾„ï¼Œå¦‚ core_products/aiagent/zh/server
        """
        # ç§»é™¤æ–‡ä»¶æ‰©å±•å
        path_without_ext = file_path.replace('.mdx', '').replace('.md', '')

        # å¦‚æœæä¾›äº†instance_pathï¼Œå…ˆç§»é™¤åŒ¹é…çš„è·¯å¾„éƒ¨åˆ†
        if instance_path:
            # ç¡®ä¿instance_pathä¸ä»¥/ç»“å°¾
            instance_path = instance_path.rstrip('/')

            # å¦‚æœæ–‡ä»¶è·¯å¾„ä»¥instance_pathå¼€å¤´ï¼Œç§»é™¤è¿™éƒ¨åˆ†
            if path_without_ext.startswith(instance_path):
                # ç§»é™¤instance_pathéƒ¨åˆ†ï¼Œä¿ç•™å‰©ä½™è·¯å¾„
                remaining_path = path_without_ext[len(instance_path):].lstrip('/')
            else:
                # å¦‚æœä¸åŒ¹é…ï¼Œä½¿ç”¨åŸæœ‰é€»è¾‘ä½œä¸ºfallback
                remaining_path = self._fallback_path_extraction(path_without_ext)
        else:
            # æ²¡æœ‰instance_pathæ—¶ä½¿ç”¨åŸæœ‰é€»è¾‘
            remaining_path = self._fallback_path_extraction(path_without_ext)

        # å¦‚æœremaining_pathä¸ºç©ºï¼Œè¿”å›åŸºç¡€URL
        if not remaining_path:
            base_domain = "https://doc-zh.zego.im" if self.language == 'zh' else "https://www.zegocloud.com/docs"
            return f"{base_domain}/{route_base_path}"

        # è½¬æ¢ä¸ºURLæ ¼å¼ï¼šå°å†™ï¼Œç©ºæ ¼å’Œä¸‹åˆ’çº¿è½¬ä¸ºè¿å­—ç¬¦
        url_path = remaining_path.lower().replace(' ', '-').replace('_', '-')

        # æ„å»ºå®Œæ•´URL
        base_domain = "https://doc-zh.zego.im" if self.language == 'zh' else "https://www.zegocloud.com/docs"
        full_url = f"{base_domain}/{route_base_path}/{url_path}"

        return full_url

    def _fallback_path_extraction(self, path_without_ext: str) -> str:
        """åŸæœ‰çš„è·¯å¾„æå–é€»è¾‘ï¼Œä½œä¸ºfallback"""
        path_parts = path_without_ext.split('/')

        # æ‰¾åˆ°è¯­è¨€æ ‡è¯†ç¬¦çš„ä½ç½®
        lang_index = -1
        for i, part in enumerate(path_parts):
            if part in ['zh', 'en']:
                lang_index = i
                break

        # å¦‚æœæ‰¾åˆ°è¯­è¨€æ ‡è¯†ç¬¦ï¼Œå–è¯­è¨€æ ‡è¯†ç¬¦åé¢çš„éƒ¨åˆ†
        if lang_index >= 0 and lang_index < len(path_parts) - 1:
            remaining_parts = path_parts[lang_index + 1:]
        else:
            # å¦‚æœæ²¡æ‰¾åˆ°è¯­è¨€æ ‡è¯†ç¬¦ï¼Œå–æœ€åä¸¤ä¸ªéƒ¨åˆ†
            remaining_parts = path_parts[-2:] if len(path_parts) >= 2 else path_parts[-1:]

        return '/'.join(remaining_parts)

    def load_config_file(self) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        config_path = Path(self.config_file)
        if not config_path.exists():
            error_msg = f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {self.config_file}"
            self.log_error(error_msg)
            raise FileNotFoundError(error_msg)

        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            error_msg = f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}"
            self.log_error(error_msg)
            raise

    def get_groups_from_config(self, config_data: Dict) -> Dict[str, Dict]:
        """ä»é…ç½®ä¸­æå–groupä¿¡æ¯"""
        groups = {}
        instances = config_data.get('instances', [])

        for instance in instances:
            nav_info = instance.get('navigationInfo', {})
            group_info = nav_info.get('group', {})
            group_id = group_info.get('id')
            group_name = group_info.get('name')

            if group_id and group_name:
                if group_id not in groups:
                    groups[group_id] = {
                        'id': group_id,
                        'name': group_name,
                        'instances': []
                    }
                groups[group_id]['instances'].append(instance)

        return groups

    def select_groups_and_instances(self, groups: Dict[str, Dict]) -> List[Dict]:
        """é€‰æ‹©è¦å¤„ç†çš„groupså’Œinstances"""
        print("\n=== é€‰æ‹©è¦å¤„ç†çš„äº§å“ç»„ ===")

        # æ˜¾ç¤ºæ‰€æœ‰groupé€‰é¡¹
        group_list = list(groups.values())
        for i, group in enumerate(group_list, 1):
            print(f"{i}. {group['id']}-{group['name']} ({len(group['instances'])} ä¸ªå®ä¾‹)")

        while True:
            try:
                choice = input(f"\nè¯·é€‰æ‹©äº§å“ç»„ (1-{len(group_list)}): ").strip()
                group_index = int(choice) - 1
                if 0 <= group_index < len(group_list):
                    selected_group = group_list[group_index]
                    break
                else:
                    print("æ— æ•ˆçš„é€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥!")
            except ValueError:
                print("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—!")

        print(f"\nå·²é€‰æ‹©äº§å“ç»„: {selected_group['name']}")

        # é€‰æ‹©è¯¥groupä¸‹çš„instances
        instances = selected_group['instances']
        print(f"\n=== é€‰æ‹©è¦å¤„ç†çš„å®ä¾‹ ===")
        print("è¯¥äº§å“ç»„ä¸‹çš„å®ä¾‹:")

        for i, instance in enumerate(instances, 1):
            platform = instance.get('navigationInfo', {}).get('platform', 'Unknown')
            print(f"{i}. {instance.get('id', 'Unknown')} - {platform}")

        print("0. å¤„ç†æ‰€æœ‰å®ä¾‹")

        while True:
            try:
                choice = input(f"\nè¯·é€‰æ‹©å®ä¾‹ (0-{len(instances)}, ç›´æ¥å›è½¦å¤„ç†æ‰€æœ‰): ").strip()
                if not choice:  # ç›´æ¥å›è½¦
                    selected_instances = instances
                    print("âœ… å°†å¤„ç†æ‰€æœ‰å®ä¾‹")
                    break
                elif choice == "0":
                    selected_instances = instances
                    print("âœ… å°†å¤„ç†æ‰€æœ‰å®ä¾‹")
                    break
                else:
                    instance_index = int(choice) - 1
                    if 0 <= instance_index < len(instances):
                        selected_instances = [instances[instance_index]]
                        platform = selected_instances[0].get('navigationInfo', {}).get('platform', 'Unknown')
                        print(f"âœ… å·²é€‰æ‹©å®ä¾‹: {selected_instances[0].get('id')} - {platform}")
                        break
                    else:
                        print("æ— æ•ˆçš„é€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥!")
            except ValueError:
                print("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—!")

        return selected_instances

    def select_download_mode(self) -> str:
        """é€‰æ‹©ä¸‹è½½æ¨¡å¼"""
        print("\n=== é€‰æ‹©ä¸‹è½½æ¨¡å¼ ===")
        print("è¯·é€‰æ‹©ä¸‹è½½æ¨¡å¼:")
        print("1. æ ¹æ®gitå˜æ›´è®°å½•ä¸‹è½½å¹¶æ›´æ–°")
        print("2. é€‰æ‹©æŒ‡å®šå®ä¾‹æ›´æ–°")
        print("3. è·³è¿‡ä¸‹è½½")

        while True:
            try:
                choice = input("\nè¯·é€‰æ‹© (1-3)ç›´æ¥å›è½¦é»˜è®¤1: ").strip()
                if choice == "1":
                    return "git_changes"
                elif choice == "2":
                    return "select_instances"
                elif choice == "3":
                    return "skip_download"
                else:
                    return "git_changes"
            except ValueError:
                print("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—!")

    def select_dataset_mode(self) -> bool:
        """é€‰æ‹©æ˜¯å¦å¤„ç†dataset"""
        print("\n=== é€‰æ‹©æ˜¯å¦å¤„ç†Dataset ===")
        print("è¯·é€‰æ‹©æ˜¯å¦éœ€è¦å¤„ç†Datasetï¼ˆçŸ¥è¯†åº“åˆ›å»º/æ›´æ–°ï¼‰:")
        print("1. å¤„ç†Dataset (é»˜è®¤)")
        print("2. è·³è¿‡Datasetå¤„ç†")

        while True:
            try:
                choice = input("\nè¯·é€‰æ‹© (1-2, ç›´æ¥å›è½¦é»˜è®¤å¤„ç†): ").strip()
                if choice == "1" or choice == "":
                    print("âœ… å°†å¤„ç†Dataset")
                    return True
                elif choice == "2":
                    print("â­ï¸  å°†è·³è¿‡Datasetå¤„ç†")
                    return False
                else:
                    print("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—!")
            except ValueError:
                print("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—!")

    def select_git_commit_and_get_changes(self, config_data: Dict) -> Tuple[List[Dict], Dict[str, List[str]]]:
        """é€‰æ‹©gitæäº¤å¹¶è·å–å˜æ›´"""
        print("\n=== æ ¹æ®gitå˜æ›´è®°å½•ä¸‹è½½ ===")

        # è·å–æœ€è¿‘çš„æäº¤è®°å½•
        commits = self.get_git_commits(10)
        if not commits:
            self.log_error("æ— æ³•è·å–gitæäº¤è®°å½•")
            return [], {}

        print("\næœ€è¿‘çš„10ä¸ªæäº¤:")
        for i, commit in enumerate(commits, 1):
            print(f"{i}. {commit['hash'][:8]} - {commit['message']} ({commit['date']})")

        # è®©ç”¨æˆ·é€‰æ‹©èµ·å§‹æäº¤
        while True:
            choice = input(f"\nè¯·é€‰æ‹©ä»å“ªä¸ªæäº¤å¼€å§‹ç»Ÿè®¡å˜æ›´ (1-{len(commits)}, ç›´æ¥å›è½¦é€‰æ‹©æœ€æ–°æäº¤): ").strip()
            if not choice:  # ç›´æ¥å›è½¦ï¼Œé€‰æ‹©æœ€æ–°æäº¤
                selected_commit = commits[0]
                print(f"âœ… å·²é€‰æ‹©æœ€æ–°æäº¤: {selected_commit['hash'][:8]} - {selected_commit['message']}")
                break
            try:
                commit_index = int(choice) - 1
                if 0 <= commit_index < len(commits):
                    selected_commit = commits[commit_index]
                    print(f"âœ… å·²é€‰æ‹©æäº¤: {selected_commit['hash'][:8]} - {selected_commit['message']}")
                    break
                else:
                    print("æ— æ•ˆçš„é€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥!")
            except ValueError:
                print("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—!")

        # è·å–å˜æ›´æ–‡ä»¶
        print(f"\nğŸ“Š æ­£åœ¨è·å–ä» {selected_commit['hash'][:8]} åˆ°æœ€æ–°æäº¤çš„å˜æ›´æ–‡ä»¶...")
        changed_files = self.get_changed_files_since_commit(selected_commit['hash'])

        if not changed_files:
            self.log_error("æ²¡æœ‰æ‰¾åˆ°å˜æ›´çš„.mdxæ–‡ä»¶")
            return [], {}

        print(f"âœ… æ‰¾åˆ° {len(changed_files)} ä¸ªå˜æ›´çš„.mdxæ–‡ä»¶:")
        for file in changed_files:
            print(f"  - {file}")

        # åŒ¹é…æ–‡ä»¶åˆ°å®ä¾‹
        print(f"\nğŸ” æ­£åœ¨åŒ¹é…å˜æ›´æ–‡ä»¶åˆ°å®ä¾‹...")
        instance_files = self.match_files_to_instances(changed_files, config_data)

        if not instance_files:
            self.log_error("æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„å®ä¾‹")
            return [], {}

        print(f"âœ… åŒ¹é…åˆ° {len(instance_files)} ä¸ªå®ä¾‹:")
        affected_instances = []
        for instance_id, files in instance_files.items():
            print(f"  - {instance_id}: {len(files)} ä¸ªæ–‡ä»¶")
            # æ‰¾åˆ°å¯¹åº”çš„å®ä¾‹é…ç½®
            for instance in config_data.get('instances', []):
                if instance.get('id') == instance_id:
                    affected_instances.append(instance)
                    break

        return affected_instances, instance_files

    def get_sitemap_urls(self, route_base_path: str) -> List[str]:
        """ä»sitemapè·å–åŒ¹é…çš„URLsï¼ˆä¸¥æ ¼åŒ¹é…ä¸€çº§ /{routeBasePath}/ï¼‰"""
        try:
            print(f"æ­£åœ¨è·å–sitemap: {self.sitemap_url}")

            # è‹±æ–‡æš‚æœªå¯ç”¨ï¼Œé¢„ç•™é€»è¾‘
            if self.language == 'en':
                self.log_warning("è‹±æ–‡ sitemap æš‚æœªå¯ç”¨ï¼Œå…ˆè·³è¿‡ã€‚")
                return []

            if requests is None:
                self.log_error("requests æ¨¡å—ä¸å¯ç”¨ï¼Œæ— æ³•è·å–sitemap")
                return []

            response = requests.get(self.sitemap_url, timeout=30)
            response.raise_for_status()

            root = ET.fromstring(response.content)
            namespaces = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}

            urls: List[str] = []
            url_elements = root.findall('.//ns:url', namespaces)
            if len(url_elements) == 0:
                url_elements = root.findall('.//url')

            rb = (route_base_path or '').strip('/')
            want_netloc = urlparse(self.base_url).netloc

            for url_elem in url_elements:
                loc_elem = url_elem.find('ns:loc', namespaces)
                if loc_elem is None:
                    loc_elem = url_elem.find('loc')

                if loc_elem is None or not loc_elem.text:
                    continue

                u = loc_elem.text.strip()
                parsed = urlparse(u)
                if parsed.netloc != want_netloc:
                    continue

                path = parsed.path or '/'
                if path == f"/{rb}" or path == f"/{rb}/" or path.startswith(f"/{rb}/"):
                    urls.append(f"{parsed.scheme}://{parsed.netloc}{path}")

            print(f"åŒ¹é…åˆ° {len(urls)} ä¸ªURLs (route: /{rb}/)")
            return urls

        except Exception as e:
            self.log_error(f"è·å–sitemapå¤±è´¥: {e}")
            return []

    def get_filename_from_url(self, url: str, title: str = "") -> str:
        """æ ¹æ®URLç”Ÿæˆæ–‡ä»¶å"""
        parsed = urlparse(url)

        if title:
            # å»é™¤æ ‡é¢˜ä¸­çš„ HTML/JSX æ ‡ç­¾
            title = re.sub(r'<[^>]*>', '', title)
            clean_title = re.sub(r'[^\w\s-]', '', title).strip()
            clean_title = re.sub(r'[-\s]+', '-', clean_title)
            base_name = f"{clean_title}---{parsed.netloc}{parsed.path}"
        else:
            base_name = f"{parsed.netloc}{parsed.path}"

        filename = base_name.replace("/", ">").replace("&", "^^").replace('=', '^^^')
        return f"{filename}.md"

    def _to_md_url(self, page_url: str) -> str:
        parsed = urlparse(page_url)
        path = parsed.path.rstrip('/')
        if path.endswith('.md'):
            md_path = path
        else:
            md_path = f"{path}.md"
        return f"{parsed.scheme}://{parsed.netloc}{md_path}"

    def _extract_title_from_markdown(self, content: str) -> str:
        if not content:
            return ""
        # ä¼˜å…ˆåŒ¹é… Markdown çš„ä¸€çº§æ ‡é¢˜
        m = re.search(r'(?im)^\s*#\s+(.+)$', content)
        if m:
            text = re.sub(r'<[^>]+>', '', m.group(1)).strip()
            if text:
                return text
        # å°è¯•åŒ¹é… HTML çš„ <h1>
        m = re.search(r'(?is)<h1[^>]*>(.*?)</h1>', content)
        if m:
            text = re.sub(r'<[^>]+>', '', m.group(1)).strip()
            if text:
                return text
        # é¡ºå»¶åŒ¹é…äºŒçº§åˆ°å…­çº§æ ‡é¢˜
        for level in range(2, 7):
            m = re.search(rf'(?im)^\s*#{{{level}}}\s+(.+)$', content)
            if m:
                text = re.sub(r'<[^>]+>', '', m.group(1)).strip()
                if text:
                    return text
        # å…œåº•ï¼šè¿”å›ç¬¬ä¸€è¡Œéç©ºæ–‡æœ¬ï¼ˆç§»é™¤å¯èƒ½å­˜åœ¨çš„æ ‡ç­¾ï¼‰
        for line in content.splitlines():
            if line.strip():
                text = re.sub(r'<[^>]+>', '', line).strip()
                if text:
                    return text[:80]
        return ""


    async def download_url_content(self, url: str, target_dir: Path, instance_label: str) -> bool:
        """ç›´æ¥ä¸‹è½½å¯¹åº” .md å†…å®¹å¹¶ä¿å­˜ï¼Œæ ‡é¢˜å– instance.label + é¦–ä¸ª H1ï¼ˆæ—  H1 é¡ºå»¶ H2/H3...ï¼‰"""
        page_url = url  # ç”¨äºç”Ÿæˆæœ€ç»ˆæ–‡ä»¶åï¼ˆä¸å¸¦ .mdï¼‰
        md_url = self._to_md_url(page_url)

        for attempt in range(self.config.max_retries):
            try:
                if requests is None:
                    self.log_error("requests æ¨¡å—ä¸å¯ç”¨ï¼Œæ— æ³•ä¸‹è½½")
                    return False

                loop = asyncio.get_running_loop()

                def _fetch():
                    return requests.get(md_url, timeout=30)

                response = await loop.run_in_executor(None, _fetch)

                if response.status_code == 200:
                    content = response.text
                    md_title = self._extract_title_from_markdown(content)
                    combined_title = f"{instance_label} {md_title}".strip() if (instance_label or md_title) else md_title

                    filename = self.get_filename_from_url(page_url, combined_title)
                    file_path = target_dir / filename

                    with open(file_path, 'w', encoding='utf-8') as f:
                        f.write(content)

                    print(f"âœ… ä¸‹è½½æˆåŠŸ: {md_url}")
                    return True
                else:
                    self.log_warning(f"HTTP {response.status_code}: {md_url} (å°è¯• {attempt + 1}/{self.config.max_retries})")

            except Exception as e:
                self.log_warning(f"ä¸‹è½½å¼‚å¸¸ (å°è¯• {attempt + 1}/{self.config.max_retries}): {md_url} - {e}")

            if attempt < self.config.max_retries - 1:
                await asyncio.sleep(self.config.retry_delay * (2 ** attempt))  # æŒ‡æ•°é€€é¿

        self.log_error(f"ä¸‹è½½æœ€ç»ˆå¤±è´¥: {md_url}")
        return False

    async def download_instance_files(self, instance: Dict, group_id: str) -> bool:
        """ä¸‹è½½å•ä¸ªinstanceçš„æ–‡ä»¶"""
        instance_id = instance.get('id')
        route_base_path = instance.get('routeBasePath')
        platform = instance.get('navigationInfo', {}).get('platform', 'Unknown')

        print(f"\nğŸ“¥ å¼€å§‹å¤„ç†å®ä¾‹: {instance_id} - {platform}")

        if not route_base_path:
            self.log_error(f"å®ä¾‹ {instance_id} ç¼ºå°‘ routeBasePath")
            return False

        # åˆ›å»ºç›®å½•ç»“æ„: data/group_id/instance_id
        target_dir = self.data_dir / group_id / instance_id

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨æ–‡ä»¶
        existing_files = []
        if target_dir.exists():
            existing_files = list(target_dir.glob("*.md"))

        if existing_files:
            print(f"ğŸ“ å‘ç°å·²å­˜åœ¨ {len(existing_files)} ä¸ªæ–‡ä»¶åœ¨ç›®å½•: {target_dir}")
            print(f"âš ï¸  é‡æ–°ä¸‹è½½å°†åˆ é™¤ç°æœ‰æ–‡ä»¶å¹¶é‡æ–°è·å–æ‰€æœ‰å†…å®¹")

            # é»˜è®¤ç›´æ¥é‡æ–°ä¸‹è½½å¹¶è¦†ç›–
            print(f"ğŸ—‘ï¸  åˆ é™¤å·²å­˜åœ¨çš„ç›®å½•: {target_dir}")
            shutil.rmtree(target_dir)
            target_dir.mkdir(parents=True, exist_ok=True)
        else:
            target_dir.mkdir(parents=True, exist_ok=True)

        # è·å–URLs
        urls = self.get_sitemap_urls(route_base_path)
        if not urls:
            self.log_error(f"æœªæ‰¾åˆ°åŒ¹é…çš„URLsï¼Œæ— æ³•ç»§ç»­å¤„ç†")
            return False

        print(f"ğŸ“ ç›®æ ‡ç›®å½•: {target_dir}")
        print(f"ğŸš€ å¼€å§‹ä¸‹è½½ {len(urls)} ä¸ªæ–‡ä»¶...")

        # æ‰¹é‡ä¸‹è½½
        success_count = 0
        failed_count = 0
        batch_size = self.config.concurrent_downloads

        for i in range(0, len(urls), batch_size):
            batch = urls[i:i + batch_size]
            tasks = [self.download_url_content(url, target_dir, instance.get('label', '')) for url in batch]
            results = await asyncio.gather(*tasks, return_exceptions=True)

            for result in results:
                if result is True:
                    success_count += 1
                else:
                    failed_count += 1

        success_rate = success_count / len(urls) if urls else 0
        print(f"ğŸ“Š ä¸‹è½½ç»Ÿè®¡: {success_count}/{len(urls)} ä¸ªæ–‡ä»¶æˆåŠŸ (æˆåŠŸç‡: {success_rate:.1%})")

        # å¦‚æœæˆåŠŸç‡ä½äº50%ï¼Œè®¤ä¸ºä¸‹è½½å¤±è´¥
        if success_rate < 0.5:
            self.log_error(f"ä¸‹è½½å¤±è´¥ç‡è¿‡é«˜ ({failed_count}/{len(urls)} å¤±è´¥)ï¼Œç»ˆæ­¢å¤„ç†")
            return False

        if failed_count > 0:
            self.log_warning(f"æœ‰ {failed_count} ä¸ªæ–‡ä»¶ä¸‹è½½å¤±è´¥ï¼Œä½†æˆåŠŸç‡å¯æ¥å—ï¼Œç»§ç»­å¤„ç†")

        return success_count > 0

    async def download_git_changed_files(self, affected_instances: List[Dict], instance_files: Dict[str, List[str]]) -> bool:
        """æ ¹æ®gitå˜æ›´ä¸‹è½½æ–‡ä»¶"""
        print(f"\nğŸ“¥ å¼€å§‹æ ¹æ®gitå˜æ›´ä¸‹è½½æ–‡ä»¶...")

        success_count = 0
        failed_count = 0

        for instance in affected_instances:
            instance_id = instance.get('id')
            route_base_path = instance.get('routeBasePath')
            platform = instance.get('navigationInfo', {}).get('platform', 'Unknown')
            group_id = instance.get('navigationInfo', {}).get('group', {}).get('id')

            print(f"\nğŸ“¥ å¤„ç†å®ä¾‹: {instance_id} - {platform}")

            if not route_base_path:
                self.log_error(f"å®ä¾‹ {instance_id} ç¼ºå°‘ routeBasePath")
                failed_count += 1
                continue

            if not group_id:
                self.log_error(f"å®ä¾‹ {instance_id} ç¼ºå°‘ group_id")
                failed_count += 1
                continue

            # åˆ›å»ºç›®å½•ç»“æ„: data/group_id/instance_id
            target_dir = self.data_dir / group_id / instance_id
            target_dir.mkdir(parents=True, exist_ok=True)

            # è·å–è¯¥å®ä¾‹çš„å˜æ›´æ–‡ä»¶
            changed_files = instance_files.get(instance_id, [])
            if not changed_files:
                print(f"âš ï¸  å®ä¾‹ {instance_id} æ²¡æœ‰å˜æ›´æ–‡ä»¶")
                continue

            print(f"ğŸ“ ç›®æ ‡ç›®å½•: {target_dir}")
            print(f"ğŸ”„ å¤„ç† {len(changed_files)} ä¸ªå˜æ›´æ–‡ä»¶:")
            # å…ˆåˆ é™¤ç›®æ ‡ç›®å½•
            shutil.rmtree(target_dir, ignore_errors=True)
            target_dir.mkdir(parents=True, exist_ok=True)

            # è·å–å®ä¾‹è·¯å¾„
            instance_path = instance.get('path', '')

            # è½¬æ¢æ–‡ä»¶è·¯å¾„ä¸ºURLå¹¶ä¸‹è½½
            urls_to_download = []
            for file_path in changed_files:
                url = self.convert_file_path_to_url(file_path, route_base_path, instance_path)
                urls_to_download.append(url)
                print(f"  - {file_path} -> {url}")

            if not urls_to_download:
                self.log_error(f"æ²¡æœ‰æœ‰æ•ˆçš„URLéœ€è¦ä¸‹è½½")
                failed_count += 1
                continue

            # æ‰¹é‡ä¸‹è½½
            print(f"ğŸš€ å¼€å§‹ä¸‹è½½ {len(urls_to_download)} ä¸ªæ–‡ä»¶...")
            batch_success = 0
            batch_failed = 0
            batch_size = self.config.concurrent_downloads

            for i in range(0, len(urls_to_download), batch_size):
                batch = urls_to_download[i:i + batch_size]
                tasks = [self.download_url_content(url, target_dir, instance.get('label', '')) for url in batch]
                results = await asyncio.gather(*tasks, return_exceptions=True)

                for result in results:
                    if result is True:
                        batch_success += 1
                    else:
                        batch_failed += 1

            batch_success_rate = batch_success / len(urls_to_download) if urls_to_download else 0
            print(f"ğŸ“Š å®ä¾‹ {instance_id} ä¸‹è½½ç»Ÿè®¡: {batch_success}/{len(urls_to_download)} ä¸ªæ–‡ä»¶æˆåŠŸ (æˆåŠŸç‡: {batch_success_rate:.1%})")

            if batch_success_rate >= 0.5:  # æˆåŠŸç‡50%ä»¥ä¸Šè®¤ä¸ºæˆåŠŸ
                success_count += 1
            else:
                failed_count += 1
                self.log_error(f"å®ä¾‹ {instance_id} ä¸‹è½½å¤±è´¥ç‡è¿‡é«˜")

        total_success_rate = success_count / len(affected_instances) if affected_instances else 0
        print(f"\nğŸ“Š æ€»ä½“ä¸‹è½½ç»Ÿè®¡: {success_count}/{len(affected_instances)} ä¸ªå®ä¾‹æˆåŠŸ (æˆåŠŸç‡: {total_success_rate:.1%})")

        return total_success_rate >= 0.5

    def get_documents_by_filename(self, dataset_id: str, filenames: List[str], instance: Dict = None) -> List[str]:
        """æ ¹æ®æ–‡ä»¶åè·å–æ–‡æ¡£ID

        Args:
            dataset_id: æ•°æ®é›†ID
            filenames: æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆåŸå§‹æ–‡ä»¶è·¯å¾„ï¼‰
            instance: å®ä¾‹ä¿¡æ¯ï¼Œç”¨äºè·å–è·¯å¾„è½¬æ¢æ‰€éœ€çš„å‚æ•°
        """
        if not requests:
            self.log_error("requests æœªå®‰è£…ï¼Œæ— æ³•è·å–æ–‡æ¡£")
            return []

        headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {self.config.api_key}'
            }

        try:
            # è·å–datasetä¸­çš„æ‰€æœ‰æ–‡æ¡£
            response = requests.get(
                f"{self.config.ragflow_base_url}/datasets/{dataset_id}/documents",
                headers=headers,
                params={"page": 1, "page_size": 200},
                timeout=30,
            )

            if response.status_code != 200:
                self.log_error(f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: HTTP {response.status_code}")
                return []

            data = response.json()

            if data.get('code') == 102:
                print(f"âš ï¸  æ•°æ®é›†ä¸ºç©º: {dataset_id}")
                return []

            elif data.get('code') != 0:
                self.log_error(f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {data.get('code')} {data.get('message')} {dataset_id}")
                return []

            data_content = data.get('data', {})

            # æ ¹æ®RagFlow APIæ–‡æ¡£ï¼ŒList documentsè¿”å›çš„dataæ˜¯ä¸€ä¸ªå¯¹è±¡ï¼ŒåŒ…å«docsæ•°ç»„
            if isinstance(data_content, dict):
                documents = data_content.get('docs', [])
            elif isinstance(data_content, list):
                # å…¼å®¹å¯èƒ½çš„ç›´æ¥æ•°ç»„æ ¼å¼
                documents = data_content
            else:
                self.log_error(f"æ–‡æ¡£æ•°æ®æ ¼å¼é”™è¯¯: {type(data_content)}")
                return []

            if not isinstance(documents, list):
                self.log_error(f"æ–‡æ¡£åˆ—è¡¨æ ¼å¼é”™è¯¯: {type(documents)}")
                return []

            document_ids = []

            # å°†æ–‡ä»¶è·¯å¾„è½¬æ¢ä¸ºä¸‹è½½æ–‡ä»¶åæ ¼å¼
            converted_filenames = []
            if instance:
                route_base_path = instance.get('routeBasePath', '')
                instance_path = instance.get('path', '')

                for filename in filenames:
                    try:
                        # å°†æ–‡ä»¶è·¯å¾„è½¬æ¢ä¸ºURL
                        url = self.convert_file_path_to_url(filename, route_base_path, instance_path)
                        # å°†URLè½¬æ¢ä¸ºä¸‹è½½æ–‡ä»¶åæ ¼å¼ï¼ˆä¸å¸¦titleï¼Œå› ä¸ºæˆ‘ä»¬æ²¡æœ‰titleä¿¡æ¯ï¼‰
                        download_filename = self.get_filename_from_url(url)
                        converted_filenames.append(download_filename)
                        print(f"ğŸ”„ æ–‡ä»¶è·¯å¾„è½¬æ¢: {filename} -> {url} -> {download_filename}")
                    except Exception as e:
                        print(f"âš ï¸  æ–‡ä»¶è·¯å¾„è½¬æ¢å¤±è´¥: {filename} - {e}")
                        # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ–‡ä»¶åä½œä¸ºfallback
                        converted_filenames.append(filename)
            else:
                # å¦‚æœæ²¡æœ‰å®ä¾‹ä¿¡æ¯ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æ–‡ä»¶å
                converted_filenames = filenames
                print("âš ï¸  æ²¡æœ‰å®ä¾‹ä¿¡æ¯ï¼Œæ— æ³•è¿›è¡Œæ–‡ä»¶åè½¬æ¢ï¼Œä½¿ç”¨åŸå§‹æ–‡ä»¶å")

            # åŒ¹é…æ–‡ä»¶å
            for i, original_filename in enumerate(filenames):
                converted_filename = converted_filenames[i] if i < len(converted_filenames) else original_filename

                found_match = False
                for doc in documents:
                    if not isinstance(doc, dict):
                        continue

                    doc_name = doc.get('name', '')
                    doc_id = doc.get('id', '')

                    if not doc_id:
                        continue

                    # åŒ¹é…---åé¢çš„éƒ¨åˆ†
                    if doc_name.endswith(f"---{converted_filename}"):
                        document_ids.append(doc_id)
                        print(f"âœ… æ‰¾åˆ°åŒ¹é…æ–‡æ¡£: {doc_name} (ID: {doc_id}) <- {original_filename}")
                        found_match = True
                        break

                if not found_match:
                    print(f"âš ï¸  æœªæ‰¾åˆ°åŒ¹é…æ–‡æ¡£: {original_filename} -> {converted_filename}")

            return document_ids

        except Exception as e:
            self.log_error(f"è·å–æ–‡æ¡£å¼‚å¸¸: {e}")
            return []

    def delete_specific_documents(self, dataset_id: str, document_ids: List[str]) -> bool:
        """åˆ é™¤æŒ‡å®šçš„æ–‡æ¡£"""
        if not document_ids:
            print("âš ï¸  æ²¡æœ‰æ–‡æ¡£éœ€è¦åˆ é™¤")
            return True

        if not requests:
            self.log_error("requests æœªå®‰è£…ï¼Œæ— æ³•åˆ é™¤æ–‡æ¡£")
            return False

        headers = {
            'Authorization': f'Bearer {self.config.api_key}',
            'Content-Type': 'application/json'
        }

        try:
            response = requests.delete(
                f"{self.config.ragflow_base_url}/datasets/{dataset_id}/documents",
                headers=headers,
                json={'ids': document_ids}
            )
            data = response.json()

            if data.get('code') == 0:
                print(f"âœ… æˆåŠŸåˆ é™¤ {len(document_ids)} ä¸ªæ–‡æ¡£")
                return True
            else:
                self.log_error(f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {data.get('message')}")
                return False

        except Exception as e:
            self.log_error(f"åˆ é™¤æ–‡æ¡£å¼‚å¸¸: {e}")
            return False

    def create_or_get_dataset(self, dataset_name: str) -> Optional[str]:
        """åˆ›å»ºæˆ–è·å–dataset"""
        print(f"ğŸ“š å¤„ç†çŸ¥è¯†åº“: {dataset_name}")

        if not requests:
            self.log_error("requests æœªå®‰è£…ï¼Œæ— æ³•å¤„ç†çŸ¥è¯†åº“")
            return None

        # åŸºç¡€æ ¡éªŒ
        if not self.config.ragflow_base_url:
            self.log_error("RAGFLOW_BASE_URL æœªè®¾ç½®æˆ–æ— æ•ˆï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡ RAGFLOW_BASE_URLï¼ˆå¦‚ï¼šhttp://<host>:<port>/api/v1ï¼‰")
            return None

        headers = {
            'Authorization': f'Bearer {self.config.api_key}',
            'Content-Type': 'application/json'
        }

        try:
            # å…ˆæŸ¥æ‰¾æ˜¯å¦å­˜åœ¨ï¼ˆå¸¦åˆ†é¡µï¼Œpage_size=200ï¼‰
            params = {"page": 1, "page_size": 200, "name": dataset_name}
            print(f"ğŸ” åˆ—ä¸¾ Datasets è¯·æ±‚: url={self.config.ragflow_base_url}/datasets, params={params}")
            response = requests.get(
                f"{self.config.ragflow_base_url}/datasets",
                headers=headers,
                params=params,
                timeout=30,
            )
            print(f"ğŸ” åˆ—ä¸¾ Datasets å“åº”: status={response.status_code}, body={response.text[:400]}")
            data = response.json()

            if data.get('code') == 0 and data.get('data'):
                items = data['data']
                for it in items:
                    if it.get('name', '').lower() == dataset_name.lower():
                        dataset_id = it['id']
                        print(f"âœ… æ‰¾åˆ°å·²å­˜åœ¨çš„çŸ¥è¯†åº“: {dataset_id}")
                        return dataset_id

            # åˆ›å»ºæ–°çš„datasetï¼ˆæŒ‰æ–‡æ¡£ï¼šä¸ä¼  languageï¼›ç¡®ä¿ embedding_model æ»¡è¶³ name@factoryï¼›parser_config.chunk_token_num ä¸ºæ•´æ•°ï¼‰
            # è®¡ç®— embedding_model
            if self.language == 'zh':
                default_model = 'BAAI/bge-large-zh-v1.5@BAAI'
                embedding_model = os.getenv('RAGFLOW_EMBEDDING_MODEL_ZH', default_model)
            else:
                default_model = 'bce-embedding-base_v1@maidalun1020'
                embedding_model = os.getenv('RAGFLOW_EMBEDDING_MODEL_EN', default_model)
            # å¦‚æœç¼ºå°‘ @factoryï¼Œå°½é‡è¡¥ @BAAIï¼ˆå…œåº•ï¼‰
            if '@' not in embedding_model:
                embedding_model = embedding_model + '@BAAI'

            # è§£æ chunk_token_num ä¸º int
            ctn_raw = os.getenv('RAGFLOW_CHUNK_TOKEN_NUM', '512')
            try:
                chunk_token_num = int(str(ctn_raw).strip())
            except Exception:
                print(f"âš ï¸  RAGFLOW_CHUNK_TOKEN_NUM æ— æ³•è§£æä¸ºæ•´æ•°ï¼Œä½¿ç”¨é»˜è®¤ 512ï¼ŒåŸå§‹å€¼: {ctn_raw}")
                chunk_token_num = 512

            config = {
                'name': dataset_name,
                'permission': 'team',
                'embedding_model': embedding_model,
                'parser_config': {
                    'chunk_token_num': chunk_token_num
                }
            }
            print(f"ğŸ”§ åˆ›å»º Dataset è¯·æ±‚: url={self.config.ragflow_base_url}/datasets, body={config}")

            response = requests.post(
                f"{self.config.ragflow_base_url}/datasets",
                headers=headers,
                json=config,
                timeout=30,
            )
            try:
                data = response.json()
            except Exception:
                data = {"code": -1, "message": response.text[:500]}

            if data.get('code') == 0:
                dataset_id = data['data']['id']
                print(f"âœ… åˆ›å»ºçŸ¥è¯†åº“æˆåŠŸ: {dataset_id}")
                return dataset_id
            else:
                self.log_error(f"åˆ›å»ºçŸ¥è¯†åº“å¤±è´¥: HTTP {response.status_code}, code={data.get('code')}, message={data.get('message')}")
                return None

        except Exception as e:
            self.log_error(f"å¤„ç†çŸ¥è¯†åº“å¼‚å¸¸: {e}")
            return None

    def upload_and_parse_documents(self, dataset_id: str, file_paths: List[str], clear_existing: bool = True) -> bool:
        """ä¸Šä¼ å¹¶è§£ææ–‡æ¡£"""
        if not file_paths:
            print("âš ï¸  æ²¡æœ‰æ–‡ä»¶éœ€è¦ä¸Šä¼ ")
            return False

        print(f"ğŸ“¤ ä¸Šä¼  {len(file_paths)} ä¸ªæ–‡æ¡£åˆ°çŸ¥è¯†åº“")

        headers = {'Authorization': f'Bearer {self.config.api_key}'}

        try:
            # æ ¹æ®å‚æ•°å†³å®šæ˜¯å¦æ¸…ç©ºç°æœ‰æ–‡æ¡£
            if clear_existing:
                response = requests.delete(
                    f"{self.config.ragflow_base_url}/datasets/{dataset_id}/documents",
                    headers={**headers, 'Content-Type': 'application/json'},
                    json={'ids': None}
                )
                print("ğŸ—‘ï¸  å·²æ¸…ç©ºçŸ¥è¯†åº“ä¸­çš„ç°æœ‰æ–‡æ¡£")
            else:
                print("ğŸ“ ä¿ç•™ç°æœ‰æ–‡æ¡£ï¼Œä»…ä¸Šä¼ æ–°æ–‡æ¡£")

            # æ‰¹é‡ä¸Šä¼ æ–‡ä»¶
            batch_size = 10
            all_file_ids = []

            for i in range(0, len(file_paths), batch_size):
                batch_files = file_paths[i:i + batch_size]
                files = []

                for file_path in batch_files:
                    if Path(file_path).exists():
                        files.append(('file', open(file_path, 'rb')))

                if files:
                    response = requests.post(
                        f"{self.config.ragflow_base_url}/datasets/{dataset_id}/documents",
                        headers=headers,
                        files=files
                    )

                    # å…³é—­æ–‡ä»¶å¥æŸ„
                    for _, file_handle in files:
                        file_handle.close()

                    data = response.json()
                    if data.get('code') == 0 and data.get('data'):
                        batch_ids = [item['id'] for item in data['data']]
                        all_file_ids.extend(batch_ids)
                        print(f"âœ… æ‰¹æ¬¡ä¸Šä¼ æˆåŠŸ: {len(batch_ids)} ä¸ªæ–‡ä»¶")

            if all_file_ids:
                # è§£ææ–‡æ¡£
                response = requests.post(
                    f"{self.config.ragflow_base_url}/datasets/{dataset_id}/chunks",
                    headers={**headers, 'Content-Type': 'application/json'},
                    json={'document_ids': all_file_ids}
                )
                print("âœ… æ–‡æ¡£è§£æå®Œæˆ")
                return True
            else:
                self.log_error("æ²¡æœ‰æˆåŠŸä¸Šä¼ çš„æ–‡ä»¶")
                return False

        except Exception as e:
            self.log_error(f"ä¸Šä¼ æ–‡æ¡£å¼‚å¸¸: {e}")
            return False





    def log_download_errors(self, failed_instances: List[str], failed_files: Dict[str, List[str]]):
        """è®°å½•ä¸‹è½½é”™è¯¯"""
        if failed_instances:
            print("\n=== ä¸‹è½½é”™è¯¯æ€»ç»“ ===")
            print(f"âŒ ä»¥ä¸‹å®ä¾‹ä¸‹è½½å¤±è´¥:")
            for instance in failed_instances:
                print(f"  - {instance}")
                if instance in failed_files:
                    print("    å¤±è´¥æ–‡ä»¶:")
                    for file in failed_files[instance]:
                        print(f"      - {file}")

    def log_dataset_errors(self, failed_datasets: List[str], error_details: Dict[str, str]):
        """è®°å½•Datasetå¤„ç†é”™è¯¯"""
        if failed_datasets:
            print("\n=== Datasetå¤„ç†é”™è¯¯æ€»ç»“ ===")
            print(f"âŒ ä»¥ä¸‹Datasetå¤„ç†å¤±è´¥:")
            for dataset in failed_datasets:
                print(f"  - {dataset}")
                if dataset in error_details:
                    print(f"    é”™è¯¯åŸå› : {error_details[dataset]}")


async def main():
    """ä¸»å‡½æ•°"""
    manager = UpdateAIDataManager()

    print("=== AIæ•°æ®æ›´æ–°è„šæœ¬ ===")
    print("æœ¬è„šæœ¬å°†å®Œæ•´æ‰§è¡Œï¼šé¡µé¢ä¸‹è½½ -> Datasetæ›´æ–°")

    try:
        # 1. è¯­è¨€å›ºå®šä¸ºä¸­æ–‡ï¼ˆè‹±æ–‡ sitemap æš‚æœªå¯ç”¨ï¼‰
        manager.language = 'zh'
        manager.config_file = "../../docuo.config.zh.json"
        manager.sitemap_url = CHINESE_SITEMAP_URL
        manager.base_url = CHINESE_BASE_URL
        manager.faq_dataset_name = "FAQ-ZH"
        print("âœ… å·²é€‰æ‹©ä¸­æ–‡")

        # 2. åŠ è½½é…ç½®æ–‡ä»¶
        print(f"\nğŸ“– åŠ è½½é…ç½®æ–‡ä»¶: {manager.config_file}")
        config_data = manager.load_config_file()

        # 3. æå–groups
        groups = manager.get_groups_from_config(config_data)
        if not groups:
            manager.log_error("é…ç½®æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„groups")
            return

        print(f"âœ… æ‰¾åˆ° {len(groups)} ä¸ªäº§å“ç»„")

        # 4. è§£æå‘½ä»¤è¡Œå‚æ•°ï¼›æ”¯æŒ --all å…¨é‡æ¨¡å¼
        parser = argparse.ArgumentParser(add_help=False)
        parser.add_argument("--all", dest="all_mode", action="store_true", help="å…¨é‡æ¨¡å¼ï¼šéå†æ‰€æœ‰äº§å“çš„æ‰€æœ‰å®ä¾‹")
        args, _ = parser.parse_known_args()

        if args.all_mode:
            print("\n=== å…¨é‡æ¨¡å¼: éå†æ‰€æœ‰äº§å“çš„æ‰€æœ‰å®ä¾‹ ===")
            all_instances = []
            for grp in groups.values():
                all_instances.extend(grp.get('instances', []))

            total = len(all_instances)
            print(f"ğŸ“¦ å…±å‘ç° {total} ä¸ªå®ä¾‹ï¼Œå°†é€ä¸ªä¸‹è½½å¹¶æ›´æ–°å¯¹åº”çŸ¥è¯†åº“ï¼ˆå®Œæˆååˆ é™¤æœ¬åœ°æ–‡ä»¶ï¼‰")

            success_cnt, fail_cnt, skipped_cnt = 0, 0, 0
            failure_details = {}
            success_groups = set()
            # åœ¨å…¨é‡æ¨¡å¼ä¸‹è·³è¿‡å†—ä½™äº§å“
            skip_ids = {"real_time_voice_zh", "live_streaming_zh"}

            for idx, instance in enumerate(all_instances, 1):
                instance_id = instance.get('id')
                if instance_id in skip_ids:
                    print(f"â­ï¸ è·³è¿‡å†—ä½™äº§å“: {instance_id}")
                    skipped_cnt += 1
                    continue
                group_id_each = instance.get('navigationInfo', {}).get('group', {}).get('id')
                print(f"\n[{idx}/{total}] å®ä¾‹ {instance_id} å¼€å§‹")

                ok = await manager.download_instance_files(instance, group_id_each)
                if not ok:
                    reason = "é¡µé¢ä¸‹è½½å¤±è´¥"
                    print(f"âŒ {reason}ï¼Œè·³è¿‡ Dataset å¤„ç†: {instance_id}")
                    failure_details[instance_id] = reason
                    fail_cnt += 1
                    continue

                # æ”¶é›†æ–‡ä»¶
                target_dir = manager.data_dir / group_id_each / instance_id
                md_files = list(target_dir.glob("*.md"))
                if not md_files:
                    reason = "æœªæ‰¾åˆ°markdownæ–‡ä»¶"
                    print(f"âŒ {reason}ï¼Œè·³è¿‡: {instance_id}")
                    failure_details[instance_id] = reason
                    fail_cnt += 1
                    # æ¸…ç†ç©ºç›®å½•
                    shutil.rmtree(target_dir, ignore_errors=True)
                    continue

                file_paths = [str(f) for f in md_files]

                # åˆ›å»º/è·å– dataset
                dataset_id = manager.create_or_get_dataset(instance_id)
                if not dataset_id:
                    reason = "æ— æ³•åˆ›å»ºæˆ–è·å–dataset"
                    print(f"âŒ {reason}: {instance_id}")
                    failure_details[instance_id] = reason
                    fail_cnt += 1
                    shutil.rmtree(target_dir, ignore_errors=True)
                    continue

                # å…¨é‡æ¨¡å¼ï¼šæ¸…ç©ºå¹¶é‡æ–°ä¸Šä¼ 
                upload_success = manager.upload_and_parse_documents(dataset_id, file_paths, clear_existing=True)
                if not upload_success:
                    reason = "æ–‡æ¡£ä¸Šä¼ å¤±è´¥"
                    print(f"âŒ {reason}: {instance_id}")
                    failure_details[instance_id] = reason
                    fail_cnt += 1
                    shutil.rmtree(target_dir, ignore_errors=True)
                    continue

                print(f"âœ… å®ä¾‹å®Œæˆ: {instance_id}ï¼Œå¼€å§‹æ¸…ç†æœ¬åœ°æ–‡ä»¶")
                shutil.rmtree(target_dir, ignore_errors=True)
                # è‹¥ group ç›®å½•ä¸ºç©ºï¼Œåˆ™ä¸€å¹¶åˆ é™¤
                group_dir = manager.data_dir / group_id_each
                try:
                    if group_dir.exists() and not any(group_dir.iterdir()):
                        shutil.rmtree(group_dir, ignore_errors=True)
                except Exception:
                    pass

                success_cnt += 1
                if group_id_each:
                    success_groups.add(group_id_each)

            # å…¨é‡æ¨¡å¼å®Œæˆåï¼Œå¦‚æœ‰ webhook åˆ™å‘é€é£ä¹¦é€šçŸ¥
            try:
                webhook = os.getenv('RUN_COMPLETED_NOTE_FEISHU_WEBHOOK', '').strip()
                if webhook:
                    total_attempted = total - skipped_cnt
                    products_updated = len(success_groups)
                    failed_list = ", ".join(list(failure_details.keys())[:20])
                    text = (
                        f"AIæ•°æ®å…¨é‡æ›´æ–°å®Œæˆ\n"
                        f"- äº§å“æ•°(æˆåŠŸ): {products_updated}\n"
                        f"- å®ä¾‹ï¼šæˆåŠŸ {success_cnt} / å¤±è´¥ {fail_cnt} / è·³è¿‡ {skipped_cnt} / æ€»è®¡(å°è¯•) {total_attempted}\n"
                        f"- å¤±è´¥å®ä¾‹({len(failure_details)}): {failed_list}{' ç­‰' if len(failure_details) > 20 else ''}\n"
                    )
                    payload = {"msg_type": "text", "content": {"text": text}}
                    try:
                        resp = requests.post(webhook, json=payload, timeout=15)
                        print(f"ğŸ“£ é£ä¹¦é€šçŸ¥å·²å‘é€: status={resp.status_code}, body={resp.text[:200]}")
                    except Exception as ne:
                        print(f"âš ï¸ é£ä¹¦é€šçŸ¥å‘é€å¤±è´¥: {ne}")
                else:
                    print("â„¹ï¸ æœªé…ç½® RUN_COMPLETED_NOTE_FEISHU_WEBHOOKï¼Œè·³è¿‡é£ä¹¦é€šçŸ¥")
            except Exception as e:
                print(f"âš ï¸ æ±‡æ€»ä¸é€šçŸ¥é˜¶æ®µå‘ç”Ÿå¼‚å¸¸: {e}")

            print(f"\nğŸ‰ å…¨é‡æ¨¡å¼å®Œæˆï¼šæˆåŠŸ {success_cnt}ï¼Œå¤±è´¥ {fail_cnt}ï¼Œè·³è¿‡ {skipped_cnt}")
            return

        # é all æ¨¡å¼ï¼šé»˜è®¤è¿›å…¥â€œæŒ‡å®šå®ä¾‹æ›´æ–°â€ï¼ˆä¸å†è¯¢é—®ä¸‹è½½æ¨¡å¼ï¼‰
        download_mode = "select_instances"
        selected_instances = manager.select_groups_and_instances(groups)
        if not selected_instances:
            manager.log_error("æ²¡æœ‰é€‰æ‹©ä»»ä½•å®ä¾‹")
            return

        group_id = selected_instances[0].get('navigationInfo', {}).get('group', {}).get('id')
        instance_files = {}

        print(f"\nğŸš€ å¼€å§‹å¤„ç† {len(selected_instances)} ä¸ªå®ä¾‹...")

        # 5. ä¸‹è½½é¡µé¢æ–‡ä»¶
        print(f"\n=== ç¬¬1æ­¥: ä¸‹è½½é¡µé¢æ–‡ä»¶ ===")

        failed_instances = []
        failed_files = {}

        if download_mode == "git_changes":
            # æ ¹æ®gitå˜æ›´ä¸‹è½½
            print(f"ğŸ“Š å°†æ ¹æ®gitå˜æ›´ä¸‹è½½ {len(selected_instances)} ä¸ªå®ä¾‹çš„å˜æ›´æ–‡ä»¶")
            for instance in selected_instances:
                instance_id = instance.get('id')
                success = await manager.download_git_changed_files([instance], {instance_id: instance_files.get(instance_id, [])})
                if not success:
                    failed_instances.append(instance_id)
                    failed_files[instance_id] = instance_files.get(instance_id, [])

        elif download_mode == "select_instances":
            # é€‰æ‹©å®ä¾‹ä¸‹è½½
            print(f"ğŸ“Š å°†ä¸‹è½½ {len(selected_instances)} ä¸ªå®ä¾‹çš„æ‰€æœ‰æ–‡ä»¶")
            for instance in selected_instances:
                instance_id = instance.get('id')
                success = await manager.download_instance_files(instance, group_id)
                if not success:
                    failed_instances.append(instance_id)
                    failed_files[instance_id] = []  # è¿™é‡Œæ— æ³•è·å–å…·ä½“å¤±è´¥çš„æ–‡ä»¶åˆ—è¡¨

        if failed_instances:
            manager.log_download_errors(failed_instances, failed_files)
            print("âŒ ç”±äºä¸‹è½½å¤±è´¥ï¼Œç»ˆæ­¢æ‰§è¡Œæµç¨‹")
            return

        # 6. é»˜è®¤å¤„ç†datasetsï¼ˆä¸å†äº¤äº’ï¼‰
        process_datasets = True

        # 7. åˆ›å»ºæˆ–æ›´æ–°datasets
        failed_datasets = []
        dataset_errors = {}

        if process_datasets:
            print(f"\n=== ç¬¬2æ­¥: åˆ›å»ºæˆ–æ›´æ–°Datasets ===")

            for instance in selected_instances:
                instance_id = instance.get('id')

                # è·å–æ–‡ä»¶è·¯å¾„
                target_dir = manager.data_dir / group_id / instance_id
                if not target_dir.exists():
                    print(f"âŒ å®ä¾‹ {instance_id} ç›®å½•ä¸å­˜åœ¨: {target_dir}")
                    failed_datasets.append(instance_id)
                    dataset_errors[instance_id] = f"ç›®å½•ä¸å­˜åœ¨: {target_dir}"
                    continue

                md_files = list(target_dir.glob("*.md"))
                if not md_files:
                    print(f"âŒ å®ä¾‹ {instance_id} æ²¡æœ‰æ‰¾åˆ°markdownæ–‡ä»¶")
                    failed_datasets.append(instance_id)
                    dataset_errors[instance_id] = "æ²¡æœ‰æ‰¾åˆ°markdownæ–‡ä»¶"
                    continue

                file_paths = [str(f) for f in md_files]
                print(f"ğŸ“š å¤„ç†å®ä¾‹ {instance_id}ï¼Œæ‰¾åˆ° {len(md_files)} ä¸ªæ–‡ä»¶")

                # åˆ›å»ºæˆ–è·å–dataset
                dataset_id = manager.create_or_get_dataset(instance_id)
                if not dataset_id:
                    print(f"âŒ æ— æ³•åˆ›å»ºæˆ–è·å–dataset: {instance_id}")
                    failed_datasets.append(instance_id)
                    dataset_errors[instance_id] = "æ— æ³•åˆ›å»ºæˆ–è·å–dataset"
                    continue

                # æ ¹æ®ä¸‹è½½æ¨¡å¼å¤„ç†æ–‡æ¡£
                if download_mode == "git_changes" and instance_id in instance_files:
                    # gitå˜æ›´æ¨¡å¼ï¼šå…ˆåˆ é™¤å¯¹åº”çš„æ—§æ–‡æ¡£ï¼Œå†ä¸Šä¼ æ–°æ–‡æ¡£
                    changed_files = instance_files[instance_id]
                    print(f"ğŸ”„ gitå˜æ›´æ¨¡å¼ï¼šå¤„ç† {len(changed_files)} ä¸ªå˜æ›´æ–‡ä»¶")

                    # è·å–éœ€è¦åˆ é™¤çš„æ–‡æ¡£IDï¼Œä¼ å…¥å®ä¾‹ä¿¡æ¯ç”¨äºæ–‡ä»¶åè½¬æ¢
                    document_ids_to_delete = manager.get_documents_by_filename(dataset_id, changed_files, instance)
                    if document_ids_to_delete:
                        print(f"ğŸ—‘ï¸  åˆ é™¤ {len(document_ids_to_delete)} ä¸ªæ—§æ–‡æ¡£")
                        delete_success = manager.delete_specific_documents(dataset_id, document_ids_to_delete)
                        if not delete_success:
                            print(f"âš ï¸  åˆ é™¤æ—§æ–‡æ¡£å¤±è´¥ï¼Œä½†ç»§ç»­ä¸Šä¼ æ–°æ–‡æ¡£")

                    # ä¸Šä¼ æ–°æ–‡æ¡£ï¼ˆä¸æ¸…ç©ºæ•´ä¸ªdatasetï¼‰
                    upload_success = manager.upload_and_parse_documents(dataset_id, file_paths, clear_existing=False)
                else:
                    # ä¼ ç»Ÿæ¨¡å¼ï¼šæ¸…ç©ºå¹¶é‡æ–°ä¸Šä¼ æ‰€æœ‰æ–‡æ¡£
                    upload_success = manager.upload_and_parse_documents(dataset_id, file_paths, clear_existing=True)

                if not upload_success:
                    print(f"âŒ æ–‡æ¡£ä¸Šä¼ å¤±è´¥: {instance_id}")
                    failed_datasets.append(instance_id)
                    dataset_errors[instance_id] = "æ–‡æ¡£ä¸Šä¼ å¤±è´¥"
                    continue

                print(f"âœ… Datasetå¤„ç†æˆåŠŸ: {instance_id}")

            if failed_datasets:
                manager.log_dataset_errors(failed_datasets, dataset_errors)
                print("âŒ ç”±äºDatasetåˆ›å»º/æ›´æ–°å¤±è´¥ï¼Œç»ˆæ­¢æ‰§è¡Œæµç¨‹")
                return
        else:
            print(f"\n=== ç¬¬2æ­¥: è·³è¿‡Datasetå¤„ç† ===")
            print(f"â­ï¸  è·³è¿‡Datasetå¤„ç†æ­¥éª¤")


        print(f"\nğŸ‰ æ‰€æœ‰æ­¥éª¤æ‰§è¡Œå®Œæˆ!")
        print(f"âœ… æˆåŠŸå¤„ç†äº† {len(selected_instances)} ä¸ªå®ä¾‹")

        print(f"ğŸ“ æ–‡ä»¶ä¿å­˜åœ¨: {manager.data_dir / group_id}")

    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
    except Exception as e:
        print(f"\nâŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())
